<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>秋招面试总结分享 | 学习笔记记录感悟</title><meta name="author" content="Jack Zhu"><meta name="copyright" content="Jack Zhu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="秋招结束后的面试总结分享。"><link rel="shortcut icon" href="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/art_favicon.webp"><link rel="canonical" href="http://example.com/2024/11/30/%E7%A7%8B%E6%8B%9B%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?2a8dc97b4173a5cf5b6a570b541c0233";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '秋招面试总结分享',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-29 16:49:50'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"><link rel="alternate" href="/atom.xml" title="学习笔记记录感悟" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/PicGo/mingzi.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/790dbf2659a8aff75e42f73ecf06fab9.png')"><nav id="nav"><span id="blog-info"><a href="/" title="学习笔记记录感悟"><span class="site-name">学习笔记记录感悟</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">秋招面试总结分享</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-30T00:27:37.000Z" title="发表于 2024-11-30 00:27:37">2024-11-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-29T16:49:50.525Z" title="更新于 2024-11-29 16:49:50">2024-11-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/">面试总结</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>48分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="秋招面试总结分享"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>秋招基本也结束了，这里对整个过程进行复盘和总结，也是提前备战明年的春招。</p>
<h1 id="面试前的内容"><a href="#面试前的内容" class="headerlink" title="面试前的内容"></a>面试前的内容</h1><p>对于算法和开发的选择，我还是选择了算法，他们都说算法要有顶会，但是其实这个因素影响没有那么大的，我觉得不一定需要顶会，但是论文还是要有的，毕竟算法岗。没有顶会的情况下最重要的就是实习了，实习一定要把握住，这个可比顶会好拿到多了。我看往前几届的师兄师姐都去的开发岗，基本都是Java，但是这块我又不擅长，突击几个月还不一定有本科生学的好，干脆all in 算法了。毕竟算法还是比开发高很多的，还是可以冲下的。</p>
<p>对于算法的准备，主要从以下几个方面展开，大致有力扣，简历，实习，秋招等。</p>
<h2 id="力扣"><a href="#力扣" class="headerlink" title="力扣"></a>力扣</h2><p>力扣这是最基本的准备，毕竟除了人才计划，其他无论算法开发岗位都有力扣的考察，所以这个是一定要准备的。力扣的内容基本刷几遍hot100就差不多了，基本面试的手撕都是hot100这个难度，基本从前一年的12月就可以开始每天一道题了，开始的时候会很慢，后面慢慢熟练了就快了。之所以要开始这么早是因为实习是第二年三月就开始了，提前三个月准备刷题还是有必要的。很多人刷300题，基本肯定是够用了，有的人甚至刷了600题，其实不是特别有必要，力扣题主要起到的是一个门槛的作用，笔试不过直接pass，过的情况下主要看面试了。</p>
<h2 id="简历"><a href="#简历" class="headerlink" title="简历"></a>简历</h2><p>简历是非常重要的，决定了简历筛选后面能不能进笔试和面试，我简历凑合吧，投了90家，进面大概30家，正常水平吧。简历一般是先教育经历，之后最好是要有实习经历，然后是论文和科研部分，后面是项目可以放几个，后面是比赛获奖那些，主体就这些吧，具体根据不同人，优势不同，做的好的放前面一些。根据篇幅，后面可以加一些个人技能，比如编程语言和计算机工具比如git什么的掌握情况，最后篇幅不够可以放自我评价，这个不是必须，可有可无。</p>
<p>对于简历是一页还是两页，我看不同HR喜好并不同，有的认为一页简单明了，有的觉得一页太单薄。总之校招一般不超过两页（博士除外），实际来看如果确实有内容建议充实两页，排版不要过于空旷，适当紧凑一些。</p>
<h1 id="面试内容"><a href="#面试内容" class="headerlink" title="面试内容"></a>面试内容</h1><p>一般一面或者二面技术面，自我介绍之后，如果有比较有含金量的实习或者论文会先讲这部分，之后会问项目之类的，之后有的会问一些八股，这个八股有的是算法八股比如transformer、CNN、RNN卷积等什么的，这块需要背下。有的会问到计算机基础比如进程线程还有python基础比如装饰器，线程锁啥的，这块只是有时候会问到。</p>
<p>然后就是面试中的手撕了，这个不是必须，有的面试会有手撕，这个因岗位和公司不同，手撕大部分是力扣，当然也有例外，比如淘天，投实习的时候手撕是手写多头注意力，交叉熵等题目，还有k-means的，这个不提前准备的话想快速写出来也不太容易。比如华子正式面试二面就出了个数字图像处理的手撕，很难，就蚌埠住了，这种题不多见。</p>
<p>技术面之后一般主管面，这块可能的问题很多，从个人信息到家里情况，到抗压测试，了解新信息的途径，其他还有比如人工智能对各行各业的影响，大模型在不同行业的应用，甚至包括盈利模式啥的等产品问题，不同的侧重点也不同，需要随机应变。这个部分不可小视，相关问题要整理下，大致想想如何回答。有的公司比如华子是技术面了可以捞，但是主管面挂了就不行。</p>
<h2 id="实习"><a href="#实习" class="headerlink" title="实习"></a>实习</h2><p>实习的重要程度可以说是第一了，毕竟一般如果组内条件不具备的话，是发不出来顶会的，尤其对于硕士，所以实习就成为了性价比最高而且最可以获取到的了。</p>
<p>准备实习第一步是要提前刷力扣题，然后准备简历润色，之后到三月下实习基本就开始了，四五月是最主要的时间，六月基本就不多了，建议尽早投递，一般越早越容易拿到offer，前期看哪些先开，最开始的可以不投特别大的厂，因为没有相关经验，如果面的很差的话确实会影响面评，影响后面，不过大部分好像影响不大的，可以先从小厂开始，虽然很多时候大厂先开。在面试的过程中不断复盘，总结经验，不断提高，投递尽量不要拖太后，后面很多没有hc，即使准备的好也没用了。</p>
<p>我前面投递实习的时候技术面其实不会特别难，不过也有例外，比如淘天的，还有腾讯AILab这种，不过多面还是有好处的，对于秋招面试也是积累经验的过程。</p>
<p>如果实在没有实习经历，也是可以把一些横向的项目加进去，包括校企合作，实践活动，这种有的也是算实习的。</p>
<p>实习看能不能转正，如果能转正就要好好准备，不能的话就趁早想想后路，实习的经历如何整理成体系，面试的时候问答到相关的问题如何回答，把工作点整理好，面试的时候就不慌。</p>
<h2 id="秋招"><a href="#秋招" class="headerlink" title="秋招"></a>秋招</h2><p>如果有实习的话一般八九月就要回去准备秋招了，除非对自己非常自信能实习转正。对于有实习的有经历，但是秋招的准备会少一些时间，因为实习很多时候也挺忙的，个别实习好像能直接在公司刷力扣，不过这种极少，看部门和主管。没有实习的人秋招七八月的时候有比较完整的时间准备秋招的内容比如力扣八股，把项目好好复盘。</p>
<p>建议还是尽早投递，确实是越早越容易，所以早点开始准备秋招的时候就可以占有不少优势，前后期难度确实不同，基本上八月份面试的都容易进。然后整理一个表格，不同公司的啥时候开始投递，有些相关的群要关注下，有的学长学姐会进行整理，自己这个表格记录好公司、投递时间、岗位、base地，笔试面试情况和时间啥的，方便自己查看。</p>
<h1 id="算法方向"><a href="#算法方向" class="headerlink" title="算法方向"></a>算法方向</h1><p>对于算法方向，主要从以下几个方向展开。</p>
<h2 id="视觉方向"><a href="#视觉方向" class="headerlink" title="视觉方向"></a>视觉方向</h2><p>这个方向也是前面做的人最多的方向，虽然很多人现在做的还是视觉的项目或者科研，但是确实不建议用这个来找工作，除非你能发CCF-C及以上的论文吧，虽然C也很勉强。现在视觉的岗位太少了，所以必须是做的比较深，有一定的研究才行，不然相关的算法都很成熟了，不太好用来找工作。</p>
<p>以下是两个半可以做的方向，说是两个半是因为第三个方向部署推理这块的hc远远不如前面两个的。</p>
<h2 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h2><p>这个方向是目前算法最好找工作的方向了，大语言模型和AIGC这块的需求很多。很多人都说自己没做过，可问题是22年11月底ChatGPT出来之前，有几个人是做大模型的？现在做大模型的人99.9%的都是大模型出来之后开始学的，这块的上手门槛没有那么高的。很多人准备一两个月做一个大致像样的项目，用来参加比如书生浦语，或者阿里这种的大模型比赛，差不多获奖就可以拿来简历用了，只要被问到的时候能讲清楚就还是可以的。现在各行各业都在用大模型做一遍，仍然是个可以做的风口，虽然风口过去并不知道能做成啥样，但是这几年自己能赚到就可以了。、</p>
<p>学习这个并不需要一上来直接看论文，看看相关的项目，和一些相对比较容易的开源课程，先上手了解整个体系，然后尝试做个玩具微调下，之后再去认真研究，是个比较好的学习方法。</p>
<h2 id="搜广推"><a href="#搜广推" class="headerlink" title="搜广推"></a>搜广推</h2><p>这个方向作为互联网的基础，有非常稳定的基础，但是对于应届生来说学校期间往往不做这块的内容，不过要是突击两个月还是可以做下的，不算特别热，但是hc还可以，互联网都招。</p>
<h2 id="部署推理"><a href="#部署推理" class="headerlink" title="部署推理"></a>部署推理</h2><p>这个方向涉及模型的量化、剪枝、蒸馏、推理、部署、推理框架、推理引擎，还要AI Infra这些，这些方向其实还是可以做的，尤其现在大模型落地对这块的需求还是不少的，即使不是大模型的部署，小模型的部署也是有需求的。不过这块就要深挖下了，很多时候涉及C++，难度就上来了，不过这块的hc数就远比不上前面了，而且相对不太好转方向，有的开的价还可以，有的不如算法，但是还是比开发高的，也算是一个可以尝试的方向。tensorrt、ncnn、tvm、onnnx，还有vllm、turbomind、triton这些都是可以学习的。</p>
<h2 id="面试内容总结"><a href="#面试内容总结" class="headerlink" title="面试内容总结"></a>面试内容总结</h2><blockquote>
<h1 id="简历学习内容"><a href="#简历学习内容" class="headerlink" title="简历学习内容"></a>简历学习内容</h1><h2 id="大模型-1"><a href="#大模型-1" class="headerlink" title="大模型"></a>大模型</h2><h3 id="实习相关"><a href="#实习相关" class="headerlink" title="实习相关"></a>实习相关</h3><h4 id="实验评价指标"><a href="#实验评价指标" class="headerlink" title="实验评价指标"></a>实验评价指标</h4><ul>
<li>正则匹配如何做的</li>
<li>类别非常不均的情况下 <code>acc</code> 指标是否足够，不足使用了什么</li>
<li>语义理解的评价指标</li>
<li>具体指标</li>
<li>为什么这样用</li>
<li>这一指标达到多少，什么意义</li>
<li><code>badcase</code> 有多少，是什么原因，如何处理的</li>
</ul>
<h4 id="如何根据实验结果调整-prompt"><a href="#如何根据实验结果调整-prompt" class="headerlink" title="如何根据实验结果调整 prompt"></a>如何根据实验结果调整 <code>prompt</code></h4><h4 id="数据构造"><a href="#数据构造" class="headerlink" title="数据构造"></a>数据构造</h4><ul>
<li>数据量</li>
<li>构造方法</li>
<li>实际上下文长度是多少</li>
<li>数据截断是什么问题</li>
</ul>
<h4 id="数据如何标注的"><a href="#数据如何标注的" class="headerlink" title="数据如何标注的"></a>数据如何标注的</h4><ul>
<li>真值从哪里来</li>
<li>标注的数据有什么问题</li>
</ul>
<h4 id="采用大模型而不是之前的机器学习方法的意义、优点"><a href="#采用大模型而不是之前的机器学习方法的意义、优点" class="headerlink" title="采用大模型而不是之前的机器学习方法的意义、优点"></a>采用大模型而不是之前的机器学习方法的意义、优点</h4><h4 id="改进和上线所需措施"><a href="#改进和上线所需措施" class="headerlink" title="改进和上线所需措施"></a>改进和上线所需措施</h4><ul>
<li>模型是否满足上线需要</li>
<li>如果不满足是因为什么，效果或速度还是其他</li>
<li>上线需要解决其他什么问题，后续需要做什么</li>
</ul>
<h3 id="相关理论"><a href="#相关理论" class="headerlink" title="相关理论"></a>相关理论</h3><ul>
<li>主流的大模型参数、量级、结构、最大上下文长度，如何选取<ul>
<li>Llama 系列</li>
<li>千问</li>
<li>盘古智子</li>
<li>chatglm</li>
<li>internlm</li>
<li>其他</li>
<li>openai o1</li>
</ul>
</li>
<li>大模型评测方法</li>
<li>CoT 和实习中的应用</li>
<li>微调方法<ul>
<li>LoRA、QLoRA 等多种方法<ul>
<li>多头 LoRA</li>
<li>LoRA</li>
<li>QLoRA</li>
</ul>
</li>
<li>训练了哪些层</li>
<li>设置了哪些超参数，如 <code>r</code> 等</li>
<li>如何根据实际情况调整这些参数</li>
</ul>
</li>
<li>分布式训练方法<ul>
<li>deepspeed</li>
<li>zero</li>
</ul>
</li>
</ul>
<h3 id="RAG-相关"><a href="#RAG-相关" class="headerlink" title="RAG 相关"></a>RAG 相关</h3><ul>
<li>原理</li>
<li>解决什么问题，为什么用 RAG</li>
<li>RAG 的向量表征方法都有哪些，怎么做的</li>
</ul>
<h3 id="视觉"><a href="#视觉" class="headerlink" title="视觉"></a>视觉</h3><h4 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h4><ul>
<li>模型<ul>
<li>YOLOv1~v11</li>
<li>各代演进和区别</li>
<li>总体发展理论</li>
<li>DETR 等无 <code>nms</code> 的模型</li>
</ul>
</li>
<li>模型架构<ul>
<li>主干网络的变化和区别</li>
<li>金字塔融合方式的多种</li>
<li>中间模块和处理方法</li>
</ul>
</li>
<li>图像增强<ul>
<li>一般的几种</li>
<li>mosaic 等多种方法</li>
</ul>
</li>
<li>量化<ul>
<li>量化方法</li>
<li>参数量</li>
<li>速度精度</li>
</ul>
</li>
<li>剪枝<ul>
<li>方法</li>
<li>对网络什么部分进行剪枝</li>
<li>参数量变化</li>
<li>速度变化</li>
</ul>
</li>
<li>CPP 部分主要包括哪些<ul>
<li>具体这部分如何用的</li>
</ul>
</li>
</ul>
<h4 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h4><ul>
<li>人脸识别和检测部分</li>
<li>构建索引和检索理论</li>
<li>向量表征</li>
<li>相似度检索</li>
<li>活体识别</li>
</ul>
<h3 id="算法八股"><a href="#算法八股" class="headerlink" title="算法八股"></a>算法八股</h3><h4 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h4><ul>
<li>Transformer 模型讲解</li>
<li>LayerNorm 等多种 <code>norm</code> 方式如 BatchNorm</li>
<li>Encoder 和 Decoder 相关</li>
<li>RNN、LSTM、Transformer 变化</li>
<li>大模型中激活函数等多种常用的结构<ul>
<li>GQA</li>
<li>MQA</li>
<li>RMSNorm</li>
<li>SwiGLU</li>
<li>RoPE</li>
</ul>
</li>
<li>激活函数<ul>
<li>ReLU</li>
<li>Leaky ReLU</li>
<li>GELU</li>
<li>tanh</li>
<li>sigmoid</li>
<li>swish</li>
<li>ELU</li>
</ul>
</li>
<li>特征工程包括什么，如何做<ul>
<li>常见的数据预处理方法有哪些</li>
</ul>
</li>
<li>分布式数据处理原理（如 Hadoop）</li>
<li>计算机理论相关（如堆和栈的区别）</li>
</ul>
<h4 id="手撕"><a href="#手撕" class="headerlink" title="手撕"></a>手撕</h4><ul>
<li>Transformer</li>
<li>注意力</li>
<li>LayerNorm</li>
<li>KMeans</li>
<li>NMS</li>
<li>IOU 计算</li>
</ul>
</blockquote>
<h1 id="最后补个学习的内容：算法工程师面试常考手撕题"><a href="#最后补个学习的内容：算法工程师面试常考手撕题" class="headerlink" title="最后补个学习的内容：算法工程师面试常考手撕题"></a>最后补个学习的内容：算法工程师面试常考手撕题</h1><p>引用链接<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw">https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw</a></p>
<ul>
<li>算法工程师面试常考手撕题<ul>
<li>注意力（Attention）篇<ul>
<li>手撕单头注意力机制（ScaledDotProductAttention）函数</li>
<li>手撕多头注意力（MultiHeadAttention）</li>
<li>手撕自注意力机制函数（SelfAttention）</li>
<li>GPT2 解码中的KV Cache</li>
<li>手撕 MQA 算法</li>
</ul>
</li>
<li>基础机器学习算法篇<ul>
<li>手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）</li>
<li>手撕 k-means 算法</li>
</ul>
</li>
<li>手撕 Layer Normalization 算法</li>
<li>手撕 Batch Normalization 算法</li>
<li>解码算法篇<ul>
<li>手撕 贪心搜索 （greedy search）</li>
<li>手撕 集束搜索 beamsearch 算法</li>
<li>手撕 温度参数采样（Temperature Sampling）算法</li>
<li>手撕 Top-K Sampling算法</li>
<li>手撕 Top-P (Nucleus) Sampling 算法</li>
</ul>
</li>
<li>神经网络篇<ul>
<li>手撕反向传播(backward propagation，BP)法</li>
<li>手撕 卷积神经网络(CNN)法</li>
<li>手撕 循环神经网络(RNN)法</li>
<li>手撕 LSTM法</li>
<li>手撕 二维卷积 算法</li>
</ul>
</li>
<li>位置编码篇<ul>
<li>手撕 绝对位置编码 算法</li>
<li>手撕 可学习位置编码 算法</li>
<li>手撕 相对位置编码 算法</li>
<li>手撕 rope 算法</li>
</ul>
</li>
<li>面试题汇总</li>
<li>致谢</li>
</ul>
</li>
</ul>
<h2 id="注意力（Attention）篇"><a href="#注意力（Attention）篇" class="headerlink" title="注意力（Attention）篇"></a><strong>注意力（Attention）篇</strong></h2><h3 id="手撕单头注意力机制（ScaledDotProductAttention）函数"><a href="#手撕单头注意力机制（ScaledDotProductAttention）函数" class="headerlink" title="手撕单头注意力机制（ScaledDotProductAttention）函数"></a><strong>手撕单头注意力机制（ScaledDotProductAttention）函数</strong></h3><p>输入是query和 key-value，注意力机制首先计算query与每个key的关联性（compatibility），每个关联性作为每个value的权重（weight），各个权重与value的乘积相加得到输出。<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444477-8e426dd3-37ed-473f-ab5f-7d445ed7592b.jpeg#clientId=u8884b877-175c-4&from=paste&id=u147acd2d&originHeight=75&originWidth=390&originalType=url&ratio=1&rotation=0&showTitle=false&size=4542&status=done&style=none&taskId=u5130429b-212c-4651-83d6-557bf8c86d9&title=" alt="image.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class ScaledDotProductAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Scaled Dot-Product Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, scale):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.softmax = nn.Softmax(dim=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, q, k, v, mask=None):</span><br><span class="line">        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul</span><br><span class="line">        u = u / self.scale # 2.Scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if mask is not None:</span><br><span class="line">            u = u.masked_fill(mask, -np.inf) # 3.Mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn = self.softmax(u) # 4.Softmax</span><br><span class="line">        output = torch.bmm(attn, v) # 5.Output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_q, n_k, n_v = 2, 4, 4</span><br><span class="line">    d_q, d_k, d_v = 128, 128, 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    q = torch.randn(batch, n_q, d_q)</span><br><span class="line">    k = torch.randn(batch, n_k, d_k)</span><br><span class="line">    v = torch.randn(batch, n_v, d_v)</span><br><span class="line">    mask = torch.zeros(batch, n_q, n_k).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))</span><br><span class="line">    attn, output = attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn)</span><br><span class="line">    print(output)</span><br></pre></td></tr></table></figure>

<h3 id="手撕多头注意力（MultiHeadAttention）"><a href="#手撕多头注意力（MultiHeadAttention）" class="headerlink" title="手撕多头注意力（MultiHeadAttention）"></a><strong>手撕多头注意力（MultiHeadAttention）</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">class MultiHeadAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Multi-Head Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_q = nn.Linear(d_k_, n_head * d_k)</span><br><span class="line">        self.fc_k = nn.Linear(d_k_, n_head * d_k)</span><br><span class="line">        self.fc_v = nn.Linear(d_v_, n_head * d_v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_o = nn.Linear(n_head * d_v, d_o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, q, k, v, mask=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        batch, n_q, d_q_ = q.size()</span><br><span class="line">        batch, n_k, d_k_ = k.size()</span><br><span class="line">        batch, n_v, d_v_ = v.size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        q = self.fc_q(q) # 1.单头变多头</span><br><span class="line">        k = self.fc_k(k)</span><br><span class="line">        v = self.fc_v(v)</span><br><span class="line">        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)</span><br><span class="line">        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)</span><br><span class="line">        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if mask is not None:</span><br><span class="line">            mask = mask.repeat(n_head, 1, 1)</span><br><span class="line">        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat</span><br><span class="line">        output = self.fc_o(output) # 4.仿射变换得到最终输出</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_q, n_k, n_v = 2, 4, 4</span><br><span class="line">    d_q_, d_k_, d_v_ = 128, 128, 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    q = torch.randn(batch, n_q, d_q_)</span><br><span class="line">    k = torch.randn(batch, n_k, d_k_)</span><br><span class="line">    v = torch.randn(batch, n_v, d_v_)    </span><br><span class="line">    mask = torch.zeros(batch, n_q, n_k).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)</span><br><span class="line">    attn, output = mha(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn.size())</span><br><span class="line">    print(output.size())</span><br></pre></td></tr></table></figure>

<h3 id="手撕自注意力机制函数（SelfAttention）"><a href="#手撕自注意力机制函数（SelfAttention）" class="headerlink" title="手撕自注意力机制函数（SelfAttention）"></a><strong>手撕自注意力机制函数（SelfAttention）</strong></h3><p>Self-Attention。和Attention类似，他们都是一种注意力机制。不同的是Attention是source对target，输入的source和输出的target内容不同。例如英译中，输入英文，输出中文。而Self-Attention是source对source，是source内部元素之间或者target内部元素之间发生的Attention机制，也可以理解为Target&#x3D;Source这种特殊情况下的注意力机制。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class SelfAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Self-Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, n_head, d_k, d_v, d_x, d_o):</span><br><span class="line">        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))</span><br><span class="line">        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))</span><br><span class="line">        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.init_parameters()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def init_parameters(self):</span><br><span class="line">        for param in self.parameters():</span><br><span class="line">            stdv = 1. / np.power(param.size(-1), 0.5)</span><br><span class="line">            param.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, x, mask=None):</span><br><span class="line">        q = torch.matmul(x, self.wq)   </span><br><span class="line">        k = torch.matmul(x, self.wk)</span><br><span class="line">        v = torch.matmul(x, self.wv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn, output = self.mha(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_x = 4</span><br><span class="line">    d_x = 80</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x = torch.randn(batch, n_x, d_x)</span><br><span class="line">    mask = torch.zeros(batch, n_x, n_x).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)</span><br><span class="line">    attn, output = selfattn(x, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn.size())</span><br><span class="line">    print(output.size())</span><br></pre></td></tr></table></figure>

<h3 id="GPT2-解码中的KV-Cache"><a href="#GPT2-解码中的KV-Cache" class="headerlink" title="GPT2 解码中的KV Cache"></a><strong>GPT2 解码中的KV Cache</strong></h3><p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。<br>也就是，解码的时候，先根据当前输入  ，生成下一个  ，然后把新生成的  拼接在  后面，获得新的输入  ，再用  生成  ，依此迭代，直到生成结束。<br>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。<br>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。<br>也就是说中间有很多我们用不到的计算，这样就造成了浪费。<br>而且随着生成的结果越来越多，输入的长度也越来越长，上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写作文，那可能就有800个step。这个情况下，step0被算了800次，step1被算了799次…这样浪费的计算资源确实不容忽视。<br>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？<br>答案就是KV Cache，利用一个缓存，把需要重复利用的中间计算结果存下来，减少重复计算。<br>而 k 和 v 就是我要缓存的对象。<br>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要老实地计算一遍。然后把 k 、 v 值缓存起来。<br>则有<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444470-9305c89b-aa36-4998-b623-9c3b1c0caf41.jpeg#clientId=u8884b877-175c-4&from=paste&id=u1cb9436b&originHeight=196&originWidth=467&originalType=url&ratio=1&rotation=0&showTitle=false&size=6239&status=done&style=none&taskId=ud1d4d6ec-170b-458b-a212-499e5614e66&title=" alt="image.jpg"><br>kv cache的下标l表示模型层数。<br>在进行第二次预测，也就是预测第5个字的时候，在第l层的时候，由于前面我们缓存了每层的ku 值，那本层就只需要算新的 o3，而不用算 o0、o1、o2。<br>因为第l层的 o0、o1、o2本来会经过FNN层之后进到 l十1 层，再经过新的投影变换，成为 l + 1 层的 k、υ 值，但是l十 1 层的 k、υ值我们已经缓存过了!<br>然后我们把本次新增算出来的 k、υ 值也存入缓存。<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444594-19c3638e-08f5-4a3c-b392-afed682a17a1.jpeg#clientId=u8884b877-175c-4&from=paste&id=udd8910f7&originHeight=185&originWidth=482&originalType=url&ratio=1&rotation=0&showTitle=false&size=8952&status=done&style=none&taskId=u082ebbba-f43b-4beb-9b72-69ac67eb0ef&title=" alt="image.jpg"><br>这样就节省了attention和FFN的很多重复计算。<br>transformers中，生成的时候传入use_cache&#x3D;True就会开启KV Cache。<br>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">Class GPT2Attention(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    def forward(</span><br><span class="line">        self,</span><br><span class="line">        hidden_states: Optional[Tuple[torch.FloatTensor]],</span><br><span class="line">        layer_past: Optional[Tuple[torch.Tensor]] = None,</span><br><span class="line">        attention_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        head_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        encoder_hidden_states: Optional[torch.Tensor] = None,</span><br><span class="line">        encoder_attention_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        use_cache: Optional[bool] = False,</span><br><span class="line">        output_attentions: Optional[bool] = False,</span><br><span class="line">    ) -&gt; Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:</span><br><span class="line">        if encoder_hidden_states is not None:</span><br><span class="line">            if not hasattr(self, &quot;q_attn&quot;):</span><br><span class="line">                raise ValueError(</span><br><span class="line">                    &quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span><br><span class="line">                    &quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            query = self.q_attn(hidden_states)</span><br><span class="line">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        else:</span><br><span class="line">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class="line">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class="line">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        # 过去所存的值</span><br><span class="line">        if layer_past is not None:</span><br><span class="line">            past_key, past_value = layer_past</span><br><span class="line">            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入</span><br><span class="line">            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if use_cache is True:</span><br><span class="line">            present = (key, value)  # 输出用于保存</span><br><span class="line">        else:</span><br><span class="line">            present = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.reorder_and_upcast_attn:</span><br><span class="line">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class="line">        else:</span><br><span class="line">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class="line">        attn_output = self.c_proj(attn_output)</span><br><span class="line">        attn_output = self.resid_dropout(attn_output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        outputs = (attn_output, present)</span><br><span class="line">        if output_attentions:</span><br><span class="line">            outputs += (attn_weights,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return outputs  # a, present, (attentions)</span><br></pre></td></tr></table></figure>

<p>总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）</p>
<h3 id="手撕-MQA-算法"><a href="#手撕-MQA-算法" class="headerlink" title="手撕 MQA 算法"></a><strong>手撕 MQA 算法</strong></h3><p>MQA 让所有的头之间 共享 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">class MultiQueryAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Multi-Query self attention.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Using torch or triton attention implemetation enables user to also use</span><br><span class="line">    additive bias.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        d_model: int,</span><br><span class="line">        n_heads: int,</span><br><span class="line">        attn_impl: str = &#x27;triton&#x27;,</span><br><span class="line">        clip_qkv: Optional[float] = None,</span><br><span class="line">        qk_ln: bool = False,</span><br><span class="line">        softmax_scale: Optional[float] = None,</span><br><span class="line">        attn_pdrop: float = 0.0,</span><br><span class="line">        low_precision_layernorm: bool = False,</span><br><span class="line">        verbose: int = 0,</span><br><span class="line">        device: Optional[str] = None,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attn_impl = attn_impl</span><br><span class="line">        self.clip_qkv = clip_qkv</span><br><span class="line">        self.qk_ln = qk_ln</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = d_model // n_heads</span><br><span class="line">        self.softmax_scale = softmax_scale</span><br><span class="line">        if self.softmax_scale is None:</span><br><span class="line">            self.softmax_scale = 1 / math.sqrt(self.head_dim)</span><br><span class="line">        self.attn_dropout_p = attn_pdrop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.Wqkv = nn.Linear(</span><br><span class="line">            d_model,</span><br><span class="line">            d_model + 2 * self.head_dim,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        fuse_splits = (d_model, d_model + self.head_dim)</span><br><span class="line">        self.Wqkv._fused = (0, fuse_splits)  # type: ignore</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attn_fn = scaled_multihead_dot_product_attention</span><br><span class="line">        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)</span><br><span class="line">        self.out_proj._is_residual = True  # type: ignore</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(</span><br><span class="line">        self,</span><br><span class="line">        x,</span><br><span class="line">        past_key_value=None,</span><br><span class="line">        attn_bias=None,</span><br><span class="line">        attention_mask=None,</span><br><span class="line">        is_causal=True,</span><br><span class="line">        needs_weights=False,</span><br><span class="line">    ):</span><br><span class="line">        qkv = self.Wqkv(x)                                      # (1, 512, 960)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.clip_qkv:</span><br><span class="line">            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        query, key, value = qkv.split(                                  # query -&gt; (1, 512, 768)</span><br><span class="line">            [self.d_model, self.head_dim, self.head_dim],               # key   -&gt; (1, 512, 96)</span><br><span class="line">            dim=2                                                       # value -&gt; (1, 512, 96)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        key_padding_mask = attention_mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.qk_ln:</span><br><span class="line">            # Applying layernorm to qk</span><br><span class="line">            dtype = query.dtype</span><br><span class="line">            query = self.q_ln(query).to(dtype)</span><br><span class="line">            key = self.k_ln(key).to(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context, attn_weights, past_key_value = self.attn_fn(</span><br><span class="line">            query,</span><br><span class="line">            key,</span><br><span class="line">            value,</span><br><span class="line">            self.n_heads,</span><br><span class="line">            past_key_value=past_key_value,</span><br><span class="line">            softmax_scale=self.softmax_scale,</span><br><span class="line">            attn_bias=attn_bias,</span><br><span class="line">            key_padding_mask=key_padding_mask,</span><br><span class="line">            is_causal=is_causal,</span><br><span class="line">            dropout_p=self.attn_dropout_p,</span><br><span class="line">            training=self.training,</span><br><span class="line">            needs_weights=needs_weights,</span><br><span class="line">            multiquery=True,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return self.out_proj(context), attn_weights, past_key_value</span><br></pre></td></tr></table></figure>

<h2 id="基础机器学习算法篇"><a href="#基础机器学习算法篇" class="headerlink" title="基础机器学习算法篇"></a><strong>基础机器学习算法篇</strong></h2><h3 id="手撕-numpy写线性回归的随机梯度下降（stochastic-gradient-descent，SGD）"><a href="#手撕-numpy写线性回归的随机梯度下降（stochastic-gradient-descent，SGD）" class="headerlink" title="手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）"></a><strong>手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）</strong></h3><p>在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，对于最优化问题，凸问题，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"># 数据加载</span><br><span class="line">from sklearn.datasets import fetch_california_housing</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, Y = fetch_california_housing(return_X_y=True)</span><br><span class="line">X.shape, Y.shape  # (20640, 8), (20640, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据预处理</span><br><span class="line">ones = np.ones(shape=(X.shape[0], 1))</span><br><span class="line">X = np.hstack([X, ones])</span><br><span class="line">validate_size = 0.2</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validate_size, shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># batch 函数</span><br><span class="line">def get_batch(batchsize: int, X: np.ndarray, Y: np.ndarray):</span><br><span class="line">    assert 0 == X.shape[0]%batchsize, f&#x27;&#123;X.shape[0]&#125;%&#123;batchsize&#125; != 0&#x27;</span><br><span class="line">    batchnum = X.shape[0]//batchsize</span><br><span class="line">    X_new = X.reshape((batchnum, batchsize, X.shape[1]))</span><br><span class="line">    Y_new = Y.reshape((batchnum, batchsize, ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for i in range(batchnum):</span><br><span class="line">        yield X_new[i, :, :], Y_new[i, :]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 损失函数</span><br><span class="line">def mse(X: np.ndarray, Y: np.ndarray, W: np.ndarray):</span><br><span class="line">    return 0.5 * np.mean(np.square(X@W-Y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def diff_mse(X: np.ndarray, Y: np.ndarray, W: np.ndarray):</span><br><span class="line">    return X.T@(X@W-Y) / X.shape[0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 模型训练</span><br><span class="line">lr = 0.001          # 学习率</span><br><span class="line">num_epochs = 1000   # 训练周期</span><br><span class="line">batch_size = 64     # |每个batch包含的样本数</span><br><span class="line">validate_every = 4  # 多少个周期进行一次检验</span><br><span class="line">def train(num_epochs: int, batch_size: int, validate_every: int, W0: np.ndarray, X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray):</span><br><span class="line">    loop = tqdm(range(num_epochs))</span><br><span class="line">    loss_train = []</span><br><span class="line">    loss_validate = []</span><br><span class="line">    W = W0</span><br><span class="line">    # 遍历epoch</span><br><span class="line">    for epoch in loop:</span><br><span class="line">        loss_train_epoch = 0</span><br><span class="line">        # 遍历batch</span><br><span class="line">        for x_batch, y_batch in get_batch(64, X_train, Y_train):</span><br><span class="line">            loss_batch = mse(X=x_batch, Y=y_batch, W=W)</span><br><span class="line">            loss_train_epoch += loss_batch*x_batch.shape[0]/X_train.shape[0]</span><br><span class="line">            grad = diff_mse(X=x_batch, Y=y_batch, W=W)</span><br><span class="line">            W = W - lr*grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        loss_train.append(loss_train_epoch)</span><br><span class="line">        loop.set_description(f&#x27;Epoch: &#123;epoch&#125;, loss: &#123;loss_train_epoch&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if 0 == epoch%validate_every:</span><br><span class="line">            loss_validate_epoch = mse(X=X_test, Y=Y_test, W=W)</span><br><span class="line">            loss_validate.append(loss_validate_epoch)</span><br><span class="line">            print(&#x27;============Validate=============&#x27;)</span><br><span class="line">            print(f&#x27;Epoch: &#123;epoch&#125;, train loss: &#123;loss_train_epoch&#125;, val loss: &#123;loss_validate_epoch&#125;&#x27;)</span><br><span class="line">            print(&#x27;================================&#x27;)</span><br><span class="line">    plot_loss(np.array(loss_train), np.array(loss_validate), validate_every)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 程序运行</span><br><span class="line">W0 = np.random.random(size=(X.shape[1], ))  # 初始权重</span><br><span class="line">train(num_epochs=num_epochs, batch_size=batch_size, validate_every=validate_every, W0=W0, X_train=X_train, Y_train=Y_train, X_test=X_test, Y_test=Y_test)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444701-894badfa-d3a2-4f1b-ac27-42ec384fcb6e.jpeg#clientId=u8884b877-175c-4&from=paste&id=ub19aa012&originHeight=423&originWidth=670&originalType=url&ratio=1&rotation=0&showTitle=false&size=45178&status=done&style=none&taskId=u4c4ffcc6-6980-4366-84f1-ac4104983cd&title=" alt="image.jpg"></p>
<h3 id="手撕-k-means-算法"><a href="#手撕-k-means-算法" class="headerlink" title="手撕 k-means 算法"></a><strong>手撕 k-means 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def kmeans(data, k, thresh=1, max_iterations=100):</span><br><span class="line">  # 随机初始化k个中心点</span><br><span class="line">  centers = data[np.random.choice(data.shape[0], k, replace=False)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for _ in range(max_iterations):</span><br><span class="line">    # 计算每个样本到各个中心点的距离</span><br><span class="line">    distances = np.linalg.norm(data[:, None] - centers, axis=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 根据距离最近的中心点将样本分配到对应的簇</span><br><span class="line">    labels = np.argmin(distances, axis=1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 更新中心点为每个簇的平均值</span><br><span class="line">    new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 判断中心点是否收敛，多种收敛条件可选</span><br><span class="line">    # 条件1：中心点不再改变</span><br><span class="line">    if np.all(centers == new_centers):</span><br><span class="line">      break</span><br><span class="line">    # 条件2：中心点的阈值小于某个阈值</span><br><span class="line">    # center_change = np.linalg.norm(new_centers - centers)</span><br><span class="line">    # if center_change &lt; thresh:</span><br><span class="line">    #     break</span><br><span class="line">    centers = new_centers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  return labels, centers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 生成一些随机数据作为示例输入</span><br><span class="line">data = np.random.rand(100, 2)  # 100个样本，每个样本有两个特征</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 手动实现K均值算法</span><br><span class="line">k = 3  # 聚类数为3</span><br><span class="line">labels, centers = kmeans(data, k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 打印簇标签和聚类中心点</span><br><span class="line">print(&quot;簇标签:&quot;, labels)</span><br><span class="line">print(&quot;聚类中心点:&quot;, centers)</span><br></pre></td></tr></table></figure>

<h2 id="手撕-Layer-Normalization-算法"><a href="#手撕-Layer-Normalization-算法" class="headerlink" title="手撕 Layer Normalization 算法"></a><strong>手撕 Layer Normalization 算法</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"> </span><br><span class="line">class LN(nn.Module):</span><br><span class="line">    # 初始化</span><br><span class="line">    def __init__(self, normalized_shape,  # 在哪个维度上做LN</span><br><span class="line">                 eps:float = 1e-5, # 防止分母为0</span><br><span class="line">                 elementwise_affine:bool = True):  # 是否使用可学习的缩放因子和偏移因子</span><br><span class="line">        super(LN, self).__init__()</span><br><span class="line">        # 需要对哪个维度的特征做LN, torch.size查看维度</span><br><span class="line">        self.normalized_shape = normalized_shape  # [c,w*h]</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.elementwise_affine = elementwise_affine</span><br><span class="line">        # 构造可训练的缩放因子和偏置</span><br><span class="line">        if self.elementwise_affine:  </span><br><span class="line">            self.gain = nn.Parameter(torch.ones(normalized_shape))  # [c,w*h]</span><br><span class="line">            self.bias = nn.Parameter(torch.zeros(normalized_shape))  # [c,w*h]</span><br><span class="line"> </span><br><span class="line">    # 前向传播</span><br><span class="line">    def forward(self, x: torch.Tensor): # [b,c,w*h]</span><br><span class="line">        # 需要做LN的维度和输入特征图对应维度的shape相同</span><br><span class="line">        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]  # [-2:]</span><br><span class="line">        # 需要做LN的维度索引</span><br><span class="line">        dims = [-(i+1) for i in range(len(self.normalized_shape))]  # [b,c,w*h]维度上取[-1,-2]维度，即[c,w*h]</span><br><span class="line">        # 计算特征图对应维度的均值和方差</span><br><span class="line">        mean = x.mean(dim=dims, keepdims=True)  # [b,1,1]</span><br><span class="line">        mean_x2 = (x**2).mean(dim=dims, keepdims=True)  # [b,1,1]</span><br><span class="line">        var = mean_x2 - mean**2  # [b,c,1,1]</span><br><span class="line">        x_norm = (x-mean) / torch.sqrt(var+self.eps)  # [b,c,w*h]</span><br><span class="line">        # 线性变换</span><br><span class="line">        if self.elementwise_affine:</span><br><span class="line">            x_norm = self.gain * x_norm + self.bias  # [b,c,w*h]</span><br><span class="line">        return x_norm</span><br><span class="line"> </span><br><span class="line"># ------------------------------- #</span><br><span class="line"># 验证</span><br><span class="line"># ------------------------------- #</span><br><span class="line"> </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"> </span><br><span class="line">    x = torch.linspace(0, 23, 24, dtype=torch.float32)  # 构造输入层</span><br><span class="line">    x = x.reshape([2,3,2*2])  # [b,c,w*h]</span><br><span class="line">    # 实例化</span><br><span class="line">    ln = LN(x.shape[1:])</span><br><span class="line">    # 前向传播</span><br><span class="line">    x = ln(x)</span><br><span class="line">    print(x.shape)</span><br></pre></td></tr></table></figure>

<h3 id="手撕-Batch-Normalization-算法"><a href="#手撕-Batch-Normalization-算法" class="headerlink" title="手撕 Batch Normalization 算法"></a><strong>手撕 Batch Normalization 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class MyBN:</span><br><span class="line">    def __init__(self, momentum=0.01, eps=1e-5, feat_dim=2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        初始化参数值</span><br><span class="line">        :param momentum: 动量，用于计算每个batch均值和方差的滑动均值</span><br><span class="line">        :param eps: 防止分母为0</span><br><span class="line">        :param feat_dim: 特征维度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 均值和方差的滑动均值</span><br><span class="line">        self._running_mean = np.zeros(shape=(feat_dim, ))</span><br><span class="line">        self._running_var = np.ones((shape=(feat_dim, ))</span><br><span class="line">        # 更新self._running_xxx时的动量</span><br><span class="line">        self._momentum = momentum</span><br><span class="line">        # 防止分母计算为0</span><br><span class="line">        self._eps = eps</span><br><span class="line">        # 对应Batch Norm中需要更新的beta和gamma，采用pytorch文档中的初始化值</span><br><span class="line">        self._beta = np.zeros(shape=(feat_dim, ))</span><br><span class="line">        self._gamma = np.ones(shape=(feat_dim, ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def batch_norm(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        BN向传播</span><br><span class="line">        :param x: 数据</span><br><span class="line">        :return: BN输出</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if self.training:</span><br><span class="line">            x_mean = x.mean(axis=0)</span><br><span class="line">            x_var = x.var(axis=0)</span><br><span class="line">            # 对应running_mean的更新公式</span><br><span class="line">            self._running_mean = (1-self._momentum)*x_mean + self._momentum*self._running_mean</span><br><span class="line">            self._running_var = (1-self._momentum)*x_var + self._momentum*self._running_var</span><br><span class="line">            # 对应论文中计算BN的公式</span><br><span class="line">            x_hat = (x-x_mean)/np.sqrt(x_var+self._eps)</span><br><span class="line">        else:</span><br><span class="line">            x_hat = (x-self._running_mean)/np.sqrt(self._running_var+self._eps)</span><br><span class="line">        return self._gamma*x_hat + self._beta</span><br></pre></td></tr></table></figure>

<h2 id="解码算法篇"><a href="#解码算法篇" class="headerlink" title="解码算法篇"></a><strong>解码算法篇</strong></h2><h3 id="手撕-贪心搜索-（greedy-search）"><a href="#手撕-贪心搜索-（greedy-search）" class="headerlink" title="手撕 贪心搜索 （greedy search）"></a><strong>手撕 贪心搜索 （greedy search）</strong></h3><p>贪心搜索（greedy search）在每个时间步 t 都选取当前概率分布中概率最大的词，即<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444607-da855304-da72-4fa6-8f9b-fd9cf3440b94.jpeg#clientId=u8884b877-175c-4&from=paste&id=ud89f8e75&originHeight=59&originWidth=273&originalType=url&ratio=1&rotation=0&showTitle=false&size=2641&status=done&style=none&taskId=ub3f3a342-b73d-485d-8964-e14e29f56aa&title=" alt="image.jpg"><br>直到 yt 为或达到预设最大长度时停止生成。<br>贪心搜索本质上是局部最优策略，但并不能保证最终结果一定是全局最优的。由于贪心搜索在解码的任意时刻只保留一条候选序列，所以在搜索效率上，贪心搜索的复杂度显著低于穷举搜索。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def greedy_decoding(input_ids, max_tokens=300):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">            next_token = torch.argmax(next_token_logits, dim=-1)</span><br><span class="line"> if next_token == tokenizer.eos_token_id:</span><br><span class="line"> break</span><br><span class="line">            input_ids = torch.cat([input_ids, rearrange(next_token, &#x27;c -&gt; 1 c&#x27;)], dim=-1)</span><br><span class="line">        generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure>

<h3 id="手撕-集束搜索-beamsearch-算法"><a href="#手撕-集束搜索-beamsearch-算法" class="headerlink" title="手撕 集束搜索 beamsearch 算法"></a><strong>手撕 集束搜索 beamsearch 算法</strong></h3><p>在NLP翻译或对话任务中，在句子解码阶段，经常用到一种搜索算法beam search。这个算法有时候在大厂面试中，甚至可能会被要求手写实现。这里就从beam search的原理出发，最后手写实现一个beam search。</p>
<ul>
<li>思路：beam search在贪心搜索上进一步扩大了搜索范围，贪心搜索每下一步只考虑当前最优的top-1结果，beam search考虑最优的top-k个结果。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def beam_search(LM_prob,beam_size = 3):</span><br><span class="line">    batch,seqlen,vocab_size = LM_prob.shape</span><br><span class="line">    #对LM_prob取对数</span><br><span class="line">    log_LM_prob = LM_prob.log()</span><br><span class="line">    #先选择第0个位置的最大beam_size个token，log_emb_prob与indices的shape为(batch,beam)</span><br><span class="line">    log_beam_prob, indices = log_LM_prob[:,0,:].topk(beam_size,sorted = True)</span><br><span class="line">    indices = indices.unsqueeze(-1)</span><br><span class="line">    #对每个长度进行beam search</span><br><span class="line">    for i in range(1,seqlen):</span><br><span class="line">        #log_beam_prob (batch,beam,vocab_size),每个beam的可能产生的概率</span><br><span class="line">        log_beam_prob = log_beam_prob.unsqueeze(-1) + log_LM_prob[:,i,:].unsqueeze(1).repeat(1,beam_size,1)</span><br><span class="line">        #选择当前步概率最高的token</span><br><span class="line">        log_beam_prob, index = log_beam_prob.view(batch,-1).topk(beam_size,sorted = True)</span><br><span class="line">        #下面的计算：beam_id选出新beam来源于之前的哪个beam;index代表真实的token id</span><br><span class="line">        #beam_id,index (batch,beam)</span><br><span class="line">        beam_id = index//vocab_size</span><br><span class="line">        index = index%vocab_size</span><br><span class="line">        mid = torch.Tensor([])</span><br><span class="line">        #对batch内每个样本循环，选出beam的同时拼接上新生成的token id</span><br><span class="line">        for j,bid,idx in zip(range(batch),beam_id,index):</span><br><span class="line">            x = torch.cat([indices[j][bid],idx.unsqueeze(-1)],-1)</span><br><span class="line">            mid = torch.cat([mid,x.unsqueeze(0)],0)</span><br><span class="line">        indices = mid</span><br><span class="line">    return indices,log_beam_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    # 建立一个语言模型 LM_prob (batch,seqlen,vocab_size)</span><br><span class="line">    LM_prob = F.softmax(torch.randn([32,20,1000]),dim = -1)</span><br><span class="line">    #最终返回每个候选，以及每个候选的log_prob，shape为(batch,beam_size,seqlen)</span><br><span class="line">    indices,log_prob = beam_search(LM_prob,beam_size = 3)</span><br><span class="line">    print(indices)</span><br></pre></td></tr></table></figure>

<h3 id="手撕-温度参数采样（Temperature-Sampling）算法"><a href="#手撕-温度参数采样（Temperature-Sampling）算法" class="headerlink" title="手撕 温度参数采样（Temperature Sampling）算法"></a><strong>手撕 温度参数采样（Temperature Sampling）算法</strong></h3><p>温度参数采样（Temperature Sampling）常用于基于概率的生成模型，如语言模型。它通过引入一个称为“温度”（Temperature）的参数来调整模型输出的概率分布，从而控制生成文本的多样性。<br>在温度参数采样中，模型在每个时间步生成词语时，会计算出词语的条件概率分布。然后模型将这个条件概率分布中的每个词语的概率值除以温度参数，对结果进行归一化处理，获得新的归一化概率分布。较高的温度值会使概率分布更平滑，从而增加生成文本的多样性。低概率的词语也有较高的可能性被选择；而较低的温度值则会使概率分布更集中，更倾向于选择高概率的词语，因此生成的文本更加确定性。最后模型根据这个新的归一化概率分布进行随机采样，选择生成的词语。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def temperature_sampling(logits, temperature=1.0):</span><br><span class="line">    logits = logits / temperature</span><br><span class="line">    probabilities = F.softmax(logits, dim=-1)</span><br><span class="line">    sampled_token = torch.multinomial(probabilities, 1)</span><br><span class="line"> return sampled_token.item()</span><br></pre></td></tr></table></figure>

<h3 id="手撕-Top-K-Sampling算法"><a href="#手撕-Top-K-Sampling算法" class="headerlink" title="手撕 Top-K Sampling算法"></a><strong>手撕 Top-K Sampling算法</strong></h3><p>Top-K 采样（在每个时间步选择条件概率排名前 K 的词语，然后在这 K 个词语中进行随机采样。这种方法既能保持一定的生成质量，又能增加文本的多样性，并且可以通过限制候选词语的数量来控制生成文本的多样性。<br>这个过程使得生成的文本在保持一定的生成质量的同时，也具有一定的多样性，因为在候选词语中仍然存在一定的竞争性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def top_k_sampling(input_ids, max_tokens=100, top_k=50, temperature=1.0):</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)</span><br><span class="line">            top_k_probs = F.softmax(top_k_logits / temperature, dim=-1)</span><br><span class="line">            next_token_index = torch.multinomial(top_k_probs, num_samples=1)</span><br><span class="line">            next_token = top_k_indices.gather(-1, next_token_index)</span><br><span class="line">            input_ids = torch.cat([input_ids, next_token], dim=-1)</span><br><span class="line">    generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure>

<h3 id="手撕-Top-P-Nucleus-Sampling-算法"><a href="#手撕-Top-P-Nucleus-Sampling-算法" class="headerlink" title="手撕 Top-P (Nucleus) Sampling 算法"></a><strong>手撕 Top-P (Nucleus) Sampling 算法</strong></h3><p>Nucleus Sampling（核采样），也被称为Top-p Sampling旨在在保持生成文本质量的同时增加多样性。这种方法可以视作是Top-K Sampling的一种变体，它在每个时间步根据模型输出的概率分布选择概率累积超过给定阈值p的词语集合，然后在这个词语集合中进行随机采样。这种方法会动态调整候选词语的数量，以保持一定的文本多样性。<br>在Nucleus Sampling中，模型在每个时间步生成词语时，首先按照概率从高到低对词汇表中的所有词语进行排序，然后模型计算累积概率，并找到累积概率超过给定阈值p的最小词语子集，这个子集就是所谓的“核”（nucleus）。模型在这个核中进行随机采样，根据词语的概率分布来选择最终输出的词语。这样做可以保证所选词语的总概率超过了阈值p，同时也保持了一定的多样性。<br>参数p是Nucleus Sampling中的重要参数，它决定了所选词语的概率总和。p的值会被设置在(0,1]之间，表示词语总概率的一个下界。<br>Nucleus Sampling 能够保持一定的生成质量，因为它在一定程度上考虑了概率分布。通过选择概率总和超过给定阈值p的词语子集进行随机采样，Nucleus Sampling 能够增加生成文本的多样性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def top_p_sampling(input_ids, max_tokens=100, top_p=0.95):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line">                outputs = model(input_ids)</span><br><span class="line">                next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)</span><br><span class="line">                sorted_probabilities = F.softmax(sorted_logits, dim=-1) </span><br><span class="line">                cumulative_probs = torch.cumsum(sorted_probabilities, dim=-1)</span><br><span class="line">                sorted_indices_to_remove = cumulative_probs &gt; top_p</span><br><span class="line">                sorted_indices_to_remove[..., 0] = False </span><br><span class="line">                indices_to_remove = sorted_indices[sorted_indices_to_remove]</span><br><span class="line">                next_token_logits.scatter_(-1, indices_to_remove[None, :], float(&#x27;-inf&#x27;))</span><br><span class="line">                probs = F.softmax(next_token_logits, dim=-1)</span><br><span class="line">                next_token = torch.multinomial(probs, num_samples=1)</span><br><span class="line">                input_ids = torch.cat([input_ids, next_token], dim=-1)</span><br><span class="line">        generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure>

<h2 id="神经网络篇"><a href="#神经网络篇" class="headerlink" title="神经网络篇"></a><strong>神经网络篇</strong></h2><h3 id="手撕反向传播-backward-propagation，BP-法"><a href="#手撕反向传播-backward-propagation，BP-法" class="headerlink" title="手撕反向传播(backward propagation，BP)法"></a><strong>手撕反向传播(backward propagation，BP)法</strong></h3><p>BP算法就是反向传播，要输入的数据经过一个前向传播会得到一个输出，但是由于权重的原因，所以其输出会和你想要的输出有差距，这个时候就需要进行反向传播，利用梯度下降，对所有的权重进行更新，这样的话在进行前向传播就会发现其输出和你想要的输出越来越接近了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"># 生成权重以及偏执项layers_dim代表每层的神经元个数，</span><br><span class="line">#比如[2,3,1]代表一个三成的网络，输入为2层，中间为3层输出为1层</span><br><span class="line">def init_parameters(layers_dim):</span><br><span class="line">    </span><br><span class="line">    L = len(layers_dim)</span><br><span class="line">    parameters =&#123;&#125;</span><br><span class="line">    for i in range(1,L):</span><br><span class="line">        parameters[&quot;w&quot;+str(i)] = np.random.random([layers_dim[i],layers_dim[i-1]])</span><br><span class="line">        parameters[&quot;b&quot;+str(i)] = np.zeros((layers_dim[i],1))</span><br><span class="line">    return parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1.0/(1.0+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># sigmoid的导函数</span><br><span class="line">def sigmoid_prime(z):</span><br><span class="line">        return sigmoid(z) * (1-sigmoid(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 前向传播，需要用到一个输入x以及所有的权重以及偏执项，都在parameters这个字典里面存储</span><br><span class="line"># 最后返回会返回一个caches里面包含的 是各层的a和z，a[layers]就是最终的输出</span><br><span class="line">def forward(x,parameters):</span><br><span class="line">    a = []</span><br><span class="line">    z = []</span><br><span class="line">    caches = &#123;&#125;</span><br><span class="line">    a.append(x)</span><br><span class="line">    z.append(x)</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    # 前面都要用sigmoid</span><br><span class="line">    for i in range(1,layers):</span><br><span class="line">        z_temp =parameters[&quot;w&quot;+str(i)].dot(x) + parameters[&quot;b&quot;+str(i)]</span><br><span class="line">        z.append(z_temp)</span><br><span class="line">        a.append(sigmoid(z_temp))</span><br><span class="line">    # 最后一层不用sigmoid</span><br><span class="line">    z_temp = parameters[&quot;w&quot;+str(layers)].dot(a[layers-1]) + parameters[&quot;b&quot;+str(layers)]</span><br><span class="line">    z.append(z_temp)</span><br><span class="line">    a.append(z_temp)</span><br><span class="line">    </span><br><span class="line">    caches[&quot;z&quot;] = z</span><br><span class="line">    caches[&quot;a&quot;] = a    </span><br><span class="line">    return  caches,a[layers]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 反向传播，parameters里面存储的是所有的各层的权重以及偏执，caches里面存储各层的a和z</span><br><span class="line"># al是经过反向传播后最后一层的输出，y代表真实值</span><br><span class="line"># 返回的grades代表着误差对所有的w以及b的导数</span><br><span class="line">def backward(parameters,caches,al,y):</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    grades = &#123;&#125;</span><br><span class="line">    m = y.shape[1]</span><br><span class="line">    # 假设最后一层不经历激活函数</span><br><span class="line">    # 就是按照上面的图片中的公式写的</span><br><span class="line">    grades[&quot;dz&quot;+str(layers)] = al - y</span><br><span class="line">    grades[&quot;dw&quot;+str(layers)] = grades[&quot;dz&quot;+str(layers)].dot(caches[&quot;a&quot;][layers-1].T) /m</span><br><span class="line">    grades[&quot;db&quot;+str(layers)] = np.sum(grades[&quot;dz&quot;+str(layers)],axis = 1,keepdims = True) /m</span><br><span class="line">    # 前面全部都是sigmoid激活</span><br><span class="line">    for i in reversed(range(1,layers)):</span><br><span class="line">        grades[&quot;dz&quot;+str(i)] = parameters[&quot;w&quot;+str(i+1)].T.dot(grades[&quot;dz&quot;+str(i+1)]) * sigmoid_prime(caches[&quot;z&quot;][i])</span><br><span class="line">        grades[&quot;dw&quot;+str(i)] = grades[&quot;dz&quot;+str(i)].dot(caches[&quot;a&quot;][i-1].T)/m</span><br><span class="line">        grades[&quot;db&quot;+str(i)] = np.sum(grades[&quot;dz&quot;+str(i)],axis = 1,keepdims = True) /m</span><br><span class="line">    return grades   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 就是把其所有的权重以及偏执都更新一下</span><br><span class="line">def update_grades(parameters,grades,learning_rate):</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    for i in range(1,layers+1):</span><br><span class="line">        parameters[&quot;w&quot;+str(i)] -= learning_rate * grades[&quot;dw&quot;+str(i)]</span><br><span class="line">        parameters[&quot;b&quot;+str(i)] -= learning_rate * grades[&quot;db&quot;+str(i)]</span><br><span class="line">    return parameters</span><br><span class="line"># 计算误差值</span><br><span class="line">def compute_loss(al,y):</span><br><span class="line">    return np.mean(np.square(al-y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载数据集</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = np.arange(0.0,1.0,0.01)</span><br><span class="line">    y =20* np.sin(2*np.pi*x)</span><br><span class="line">    # 数据可视化</span><br><span class="line">    plt.scatter(x,y)</span><br><span class="line">    return x,y</span><br><span class="line">#进行测试</span><br><span class="line">x,y = load_data()</span><br><span class="line">x = x.reshape(1,100)</span><br><span class="line">y = y.reshape(1,100)</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">parameters = init_parameters([1,25,1])</span><br><span class="line">al = 0</span><br><span class="line">for i in range(4000):</span><br><span class="line">    caches,al = forward(x, parameters)</span><br><span class="line">    grades = backward(parameters, caches, al, y)</span><br><span class="line">    parameters = update_grades(parameters, grades, learning_rate= 0.3)</span><br><span class="line">    if i %100 ==0:</span><br><span class="line">        print(compute_loss(al, y))</span><br><span class="line">plt.scatter(x,al)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="手撕-卷积神经网络-CNN-法"><a href="#手撕-卷积神经网络-CNN-法" class="headerlink" title="手撕 卷积神经网络(CNN)法"></a><strong>手撕 卷积神经网络(CNN)法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F #使用functional中的ReLu激活函数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#CNN模型</span><br><span class="line">class CNNNet(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CNNNet, self).__init__()</span><br><span class="line">        #两个卷积层</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)  #1为in_channels 10为out_channels</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)</span><br><span class="line">        #池化层</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(2)  #2为分组大小2*2</span><br><span class="line">        #全连接层 320 = 20 * 4 * 4</span><br><span class="line">        self.fc = torch.nn.Linear(320, 10)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        #先从x数据维度中得到batch_size</span><br><span class="line">        batch_size = x.size(0)</span><br><span class="line">        #卷积层-&gt;池化层-&gt;激活函数</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -1)  #将数据展开，为输入全连接层做准备</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        return x</span><br><span class="line">model = CNNNet()</span><br></pre></td></tr></table></figure>

<h3 id="手撕-循环神经网络-RNN-法"><a href="#手撕-循环神经网络-RNN-法" class="headerlink" title="手撕 循环神经网络(RNN)法"></a><strong>手撕 循环神经网络(RNN)法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># encoding:utf-8</span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义RNN模型(可以类别下方RNN简单测试代码理解)</span><br><span class="line">class Rnn(nn.Module):</span><br><span class="line">    def __init__(self, input_size):</span><br><span class="line">        super(Rnn, self).__init__()</span><br><span class="line">        # 定义RNN网络</span><br><span class="line">        ## hidden_size是自己设置的，貌似取值都是32,64,128这样来取值</span><br><span class="line">        ## num_layers是隐藏层数量，超过2层那就是深度循环神经网络了</span><br><span class="line">        self.rnn = nn.RNN(</span><br><span class="line">                input_size=input_size,</span><br><span class="line">                hidden_size=32,</span><br><span class="line">                num_layers=1,</span><br><span class="line">                batch_first=True  # 输入形状为[批量大小, 数据序列长度, 特征维度]</span><br><span class="line">                )</span><br><span class="line">        # 定义全连接层</span><br><span class="line">        self.out = nn.Linear(32, 1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 定义前向传播函数</span><br><span class="line">    def forward(self, x, h_0):</span><br><span class="line">        r_out, h_n = self.rnn(x, h_0)</span><br><span class="line">        # print(&quot;数据输出结果；隐藏层数据结果&quot;, r_out, h_n)</span><br><span class="line">        # print(&quot;r_out.size()， h_n.size()&quot;, r_out.size(), h_n.size())</span><br><span class="line">        outs = []</span><br><span class="line">        # r_out.size=[1,10,32]即将一个长度为10的序列的每个元素都映射到隐藏层上</span><br><span class="line">        for time in range(r_out.size(1)):  </span><br><span class="line">            # print(&quot;映射&quot;, r_out[:, time, :])</span><br><span class="line">            # 依次抽取序列中每个单词,将之通过全连接层并输出.r_out[:, 0, :].size()=[1,32] -&gt; [1,1]</span><br><span class="line">            outs.append(self.out(r_out[:, time, :])) </span><br><span class="line">            # print(&quot;outs&quot;, outs)</span><br><span class="line">        # stack函数在dim=1上叠加:10*[1,1] -&gt; [1,10,1] 同时h_n已经被更新</span><br><span class="line">        return torch.stack(outs, dim=1), h_n </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TIME_STEP = 10</span><br><span class="line">INPUT_SIZE = 1</span><br><span class="line">LR = 0.02</span><br><span class="line">model = Rnn(INPUT_SIZE)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>

<h3 id="手撕-LSTM法"><a href="#手撕-LSTM法" class="headerlink" title="手撕 LSTM法"></a><strong>手撕 LSTM法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Define LSTM Neural Networks</span><br><span class="line">class LstmRNN(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">        Parameters：</span><br><span class="line">        - input_size: feature size</span><br><span class="line">        - hidden_size: number of hidden units</span><br><span class="line">        - output_size: number of output</span><br><span class="line">        - num_layers: layers of LSTM to stack</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, input_size, hidden_size=1, output_size=1, num_layers=1):</span><br><span class="line">        super().__init__()</span><br><span class="line"> </span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers) # utilize the LSTM model in torch.nn </span><br><span class="line">        self.forwardCalculation = nn.Linear(hidden_size, output_size)</span><br><span class="line"> </span><br><span class="line">    def forward(self, _x):</span><br><span class="line">        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)</span><br><span class="line">        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)</span><br><span class="line">        x = x.view(s*b, h)</span><br><span class="line">        x = self.forwardCalculation(x)</span><br><span class="line">        x = x.view(s, b, -1)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h3 id="手撕-二维卷积-算法"><a href="#手撕-二维卷积-算法" class="headerlink" title="手撕 二维卷积 算法"></a><strong>手撕 二维卷积 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np </span><br><span class="line">def conv2d(img, in_channels, out_channels ,kernels, bias, stride=1, padding=0):</span><br><span class="line">    N, C, H, W = img.shape </span><br><span class="line">    kh, kw = kernels.shape</span><br><span class="line">    p = padding</span><br><span class="line">    assert C == in_channels, &quot;kernels&#x27; input channels do not match with img&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if p:</span><br><span class="line">        img = np.pad(img, ((0,0),(0,0),(p,p),(p,p)), &#x27;constant&#x27;) # padding along with all axis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    out_h = (H + 2*padding - kh) // stride + 1</span><br><span class="line">    out_w = (W + 2*padding - kw) // stride + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    outputs = np.zeros([N, out_channels, out_h, out_w])</span><br><span class="line">    # print(img)</span><br><span class="line">    for n in range(N):</span><br><span class="line">        for out in range(out_channels):</span><br><span class="line">            for i in range(in_channels):</span><br><span class="line">                for h in range(out_h):</span><br><span class="line">                    for w in range(out_w):</span><br><span class="line">                        for x in range(kh):</span><br><span class="line">                            for y in range(kw):</span><br><span class="line">                                outputs[n][out][h][w] += img[n][i][h * stride + x][w * stride + y] * kernels[x][y]</span><br><span class="line">                if i == in_channels - 1:</span><br><span class="line">                    outputs[n][out][:][:] += bias[n][out]</span><br><span class="line">    return outputs</span><br></pre></td></tr></table></figure>

<h2 id="位置编码篇"><a href="#位置编码篇" class="headerlink" title="位置编码篇"></a><strong>位置编码篇</strong></h2><h3 id="手撕-绝对位置编码-算法"><a href="#手撕-绝对位置编码-算法" class="headerlink" title="手撕 绝对位置编码 算法"></a><strong>手撕 绝对位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class SinPositionEncoding(nn.Module):</span><br><span class="line">    def __init__(self, max_sequence_length, d_model, base=10000):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.max_sequence_length = max_sequence_length</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.base = base</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self):</span><br><span class="line">        pe = torch.zeros(self.max_sequence_length, self.d_model, dtype=torch.float)  # size(max_sequence_length, d_model)</span><br><span class="line">        exp_1 = torch.arange(self.d_model // 2, dtype=torch.float)  # 初始化一半维度，sin位置编码的维度被分为了两部分</span><br><span class="line">        exp_value = exp_1 / (self.d_model / 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        alpha = 1 / (self.base ** exp_value)  # size(dmodel/2)</span><br><span class="line">        out = torch.arange(self.max_sequence_length, dtype=torch.float)[:, None] @ alpha[None, :]  # size(max_sequence_length, d_model/2)</span><br><span class="line">        embedding_sin = torch.sin(out)</span><br><span class="line">        embedding_cos = torch.cos(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        pe[:, 0::2] = embedding_sin  # 奇数位置设置为sin</span><br><span class="line">        pe[:, 1::2] = embedding_cos  # 偶数位置设置为cos</span><br><span class="line">        return pe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SinPositionEncoding(d_model=4, max_sequence_length=10, base=10000).forward()</span><br></pre></td></tr></table></figure>

<h3 id="手撕-可学习位置编码-算法"><a href="#手撕-可学习位置编码-算法" class="headerlink" title="手撕 可学习位置编码 算法"></a><strong>手撕 可学习位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class TrainablePositionEncoding(nn.Module):</span><br><span class="line">    def __init__(self, max_sequence_length, d_model):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.max_sequence_length = max_sequence_length</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self):</span><br><span class="line">        pe = nn.Embedding(self.max_sequence_length, self.d_model)</span><br><span class="line">        nn.init.constant(pe.weight, 0.)</span><br><span class="line">        return pe</span><br></pre></td></tr></table></figure>

<h3 id="手撕-相对位置编码-算法"><a href="#手撕-相对位置编码-算法" class="headerlink" title="手撕 相对位置编码 算法"></a><strong>手撕 相对位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">class RelativePosition(nn.Module):</span><br><span class="line">    def __init__(self, num_units, max_relative_position):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_units = num_units</span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))</span><br><span class="line">        nn.init.xavier_uniform_(self.embeddings_table)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, length_q, length_k):</span><br><span class="line">        range_vec_q = torch.arange(length_q)</span><br><span class="line">        range_vec_k = torch.arange(length_k)</span><br><span class="line">        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]</span><br><span class="line">        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        final_mat = distance_mat_clipped + self.max_relative_position</span><br><span class="line">        final_mat = torch.LongTensor(final_mat).cuda()</span><br><span class="line">        embeddings = self.embeddings_table[final_mat].cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RelativeMultiHeadAttention(nn.Module):</span><br><span class="line">    def __init__(self, d_model, n_heads, dropout=0.1, batch_size=6):</span><br><span class="line">        &quot;Take in model size and number of heads.&quot;</span><br><span class="line">        super(RelativeMultiHeadAttention, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        assert d_model % n_heads == 0</span><br><span class="line">        self.head_dim = d_model // n_heads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.linears = _get_clones(nn.Linear(d_model, d_model), 4)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.relative_position_k = RelativePosition(self.head_dim, max_relative_position=16)</span><br><span class="line">        self.relative_position_v = RelativePosition(self.head_dim, max_relative_position=16)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value):</span><br><span class="line">        # embedding</span><br><span class="line">        # query, key, value = [batch_size, len, hid_dim]</span><br><span class="line">        query, key, value = [l(x).view(self.batch_size, -1, self.d_model) for l, x in</span><br><span class="line">                             zip(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        len_k = query.shape[1]</span><br><span class="line">        len_q = query.shape[1]</span><br><span class="line">        len_v = value.shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        # Self-Attention</span><br><span class="line">        # r_q1, r_k1 = [batch_size, len, n_heads, head_dim]</span><br><span class="line">        r_q1 = query.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        r_k1 = key.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, self.batch_size * self.n_heads, self.head_dim)</span><br><span class="line">        r_k2 = self.relative_position_k(len_q, len_k)</span><br><span class="line">        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)</span><br><span class="line">        attn2 = attn2.contiguous().view(self.batch_size, self.n_heads, len_q, len_k)</span><br><span class="line">        attn = (attn1 + attn2) / self.scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn = self.dropout(torch.softmax(attn, dim=-1))</span><br><span class="line">        # attn = [batch_size, n_heads, len, len]</span><br><span class="line">        r_v1 = value.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        weight1 = torch.matmul(attn, r_v1)</span><br><span class="line">        r_v2 = self.relative_position_v(len_q, len_v)</span><br><span class="line">        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, self.batch_size * self.n_heads, len_k)</span><br><span class="line">        weight2 = torch.matmul(weight2, r_v2)</span><br><span class="line">        weight2 = weight2.transpose(0, 1).contiguous().view(self.batch_size, self.n_heads, len_q, self.head_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = weight1 + weight2</span><br><span class="line">        # x = [batch size, n heads, query len, head dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        # x = [batch size, query len, n heads, head dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.view(self.batch_size * len_q, self.d_model)</span><br><span class="line">        # x = [batch size * query len, hid dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return self.linears[-1](x)</span><br></pre></td></tr></table></figure>

<h3 id="手撕-rope-算法"><a href="#手撕-rope-算法" class="headerlink" title="手撕 rope 算法"></a><strong>手撕 rope 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):</span><br><span class="line">    # (max_len, 1)</span><br><span class="line">    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)</span><br><span class="line">    # (output_dim//2)</span><br><span class="line">    ids = torch.arange(0, output_dim // 2, dtype=torch.float)  # 即公式里的i, i的范围是 [0,d/2]</span><br><span class="line">    theta = torch.pow(10000, -2 * ids / output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (max_len, output_dim//2)</span><br><span class="line">    embeddings = position * theta  # 即公式里的：pos / (10000^(2i/d))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (max_len, output_dim//2, 2)</span><br><span class="line">    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim//2, 2)</span><br><span class="line">    embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))  # 在bs维度重复，其他维度都是1不重复</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim)</span><br><span class="line">    # reshape后就是：偶数sin, 奇数cos了</span><br><span class="line">    embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))</span><br><span class="line">    embeddings = embeddings.to(device)</span><br><span class="line">    return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line">def RoPE(q, k):</span><br><span class="line">    # q,k: (bs, head, max_len, output_dim)</span><br><span class="line">    batch_size = q.shape[0]</span><br><span class="line">    nums_head = q.shape[1]</span><br><span class="line">    max_len = q.shape[2]</span><br><span class="line">    output_dim = q.shape[-1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim)</span><br><span class="line">    pos_emb = sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, q.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # cos_pos,sin_pos: (bs, head, max_len, output_dim)</span><br><span class="line">    # 看rope公式可知，相邻cos，sin之间是相同的，所以复制一遍。如(1,2,3)变成(1,1,2,2,3,3)</span><br><span class="line">    cos_pos = pos_emb[...,  1::2].repeat_interleave(2, dim=-1)  # 将奇数列信息抽取出来也就是cos 拿出来并复制</span><br><span class="line">    sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)  # 将偶数列信息抽取出来也就是sin 拿出来并复制</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # q,k: (bs, head, max_len, output_dim)</span><br><span class="line">    q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)</span><br><span class="line">    q2 = q2.reshape(q.shape)  # reshape后就是正负交替了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 更新qw, *对应位置相乘</span><br><span class="line">    q = q * cos_pos + q2 * sin_pos</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)</span><br><span class="line">    k2 = k2.reshape(k.shape)</span><br><span class="line">    # 更新kw, *对应位置相乘</span><br><span class="line">    k = k * cos_pos + k2 * sin_pos</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return q, k</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line">def attention(q, k, v, mask=None, dropout=None, use_RoPE=True):</span><br><span class="line">    # q.shape: (bs, head, seq_len, dk)</span><br><span class="line">    # k.shape: (bs, head, seq_len, dk)</span><br><span class="line">    # v.shape: (bs, head, seq_len, dk)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if use_RoPE:</span><br><span class="line">        q, k = RoPE(q, k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d_k = k.size()[-1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    att_logits = torch.matmul(q, k.transpose(-2, -1))  # (bs, head, seq_len, seq_len)</span><br><span class="line">    att_logits /= math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if mask is not None:</span><br><span class="line">        att_logits = att_logits.masked_fill(mask == 0, -1e9)  # mask掉为0的部分，设为无穷大</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    att_scores = F.softmax(att_logits, dim=-1)  # (bs, head, seq_len, seq_len)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if dropout is not None:</span><br><span class="line">        att_scores = dropout(att_scores)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, seq_len, seq_len) * (bs, head, seq_len, dk) = (bs, head, seq_len, dk)</span><br><span class="line">    return torch.matmul(att_scores, v), att_scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # (bs, head, seq_len, dk)</span><br><span class="line">    q = torch.randn((8, 12, 10, 32))</span><br><span class="line">    k = torch.randn((8, 12, 10, 32))</span><br><span class="line">    v = torch.randn((8, 12, 10, 32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    res, att_scores = attention(q, k, v, mask=None, dropout=None, use_RoPE=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, seq_len, dk),  (bs, head, seq_len, seq_len)</span><br></pre></td></tr></table></figure>

<pre><code>print(res.shape, att_scores.shape)
</code></pre>
<h2 id="面试题汇总"><a href="#面试题汇总" class="headerlink" title="面试题汇总"></a><strong>面试题汇总</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483957&idx=1&sn=abec4b75b9865b754f8a303c340c13a3&scene=21#wechat_redirect">大模型微调的经验与感想分享</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483942&idx=1&sn=a5ba1da8459df0b76e1ea70bfa4dc068&scene=21#wechat_redirect">百度-NLP算法工程师面经</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483919&idx=1&sn=c9a530ecce9e60af4fad4c06062ec9ce&scene=21#wechat_redirect">美团-大模型算法工程师面经</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483896&idx=1&sn=6b79f7eb585cc1d91a1f61010941477c&scene=21#wechat_redirect">小米-NLP算法工程师面试题</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483884&idx=1&sn=e1f4d13589606786f2d2467e11b4e2dc&scene=21#wechat_redirect">好未来-NLP算法工程师面经</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483862&idx=1&sn=0dc0ee080532d397b2b00bdd20c86260&scene=21#wechat_redirect">百度大模型算法工程师面经</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483853&idx=2&sn=f717767538329ce17325de72aa58ba1b&scene=21#wechat_redirect">昆仑天工大模型算法工程师</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483839&idx=1&sn=b66447f92f4dbfa8be7922f53aa8ba4b&scene=21#wechat_redirect">阿里大模型算法工程师一面</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483790&idx=1&sn=308fb18b66cc66b78f7e15822cdd6eff&scene=21#wechat_redirect">算法工程师面试常考手撕题</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483773&idx=1&sn=003c347fc05e1a3fa4328ac09dddb797&scene=21#wechat_redirect">搜狐大模型算法工程师</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483757&idx=1&sn=79394fd14e39948d1fc98aa09e031561&scene=21#wechat_redirect">字节大模型算法实习生</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483750&idx=1&sn=18d9c270e8d58a32dc4792fbc5f8f6e8&scene=21#wechat_redirect">理想汽车大模型算法实习生</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483745&idx=1&sn=ee37c895b25bf2a1f8387edf1d687e30&scene=21#wechat_redirect">百度大模型算法实习生面试题</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483731&idx=1&sn=08cb4b390e80f3ca4a1e0fa2dd5a3020&scene=21#wechat_redirect">腾讯大模型算法实习生面试题</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483723&idx=1&sn=baa9b82a7ac4f12e936ff8b58dcf8977&scene=21#wechat_redirect">阿里大模型算法工程师一面</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483713&idx=1&sn=c90af03630f92999eed214d5dc9f06a3&scene=21#wechat_redirect">某大厂大模型算法工程师面试题</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483697&idx=1&sn=82e8cbb46aa2a0a656ae6f76ed225b03&scene=21#wechat_redirect">说说百度大模型算法工程师二面经历</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483686&idx=1&sn=79b3d0eb8a034cf7fe8746cd5e362899&scene=21#wechat_redirect">阿里大模型算法工程师面试小结</a></li>
</ul>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a><strong>致谢</strong></h2><ul>
<li>LLMs 千面郎君 更新版 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/C6NdO_Ebj3DQx2AVAAgQRQ">https://mp.weixin.qq.com/s/C6NdO_Ebj3DQx2AVAAgQRQ</a></li>
<li>LLMs九层妖塔 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Eh0tY1zx2FqXQqIGa2dIBA">https://mp.weixin.qq.com/s/Eh0tY1zx2FqXQqIGa2dIBA</a></li>
<li>NLP 面无不过 <a target="_blank" rel="noopener" href="https://github.com/km1994/NLP-Interview-Notes">https://github.com/km1994/NLP-Interview-Notes</a></li>
</ul>
<blockquote>
<p>来自: <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw">算法工程师面试常考手撕题（更新）</a></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Jack Zhu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="http://jackzhu.top/">http://jackzhu.top/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="http://jackzhu.top/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">学习笔记记录感悟</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/">面试总结</a></div><div class="post_share"><div class="social-share" data-image="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/790dbf2659a8aff75e42f73ecf06fab9.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/11/06/%E7%A7%8B%E6%8B%9B%E9%87%91%E5%B1%B1%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招金山面试分享"><img class="cover" src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/2349135dde1b4a4828b209c87569c4a8.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">秋招金山面试分享</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/PicGo/mingzi.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jack Zhu</div><div class="author-info__description">过好每一天</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/CoderJackZhu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/CoderJackZhu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zhuyijie151@sina.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/zhu-yijie-51" target="_blank" title="知乎"><i class="fab fa-zhihu"></i></a><a class="social-icon" href="https://space.bilibili.com/355295657/" target="_blank" title="哔哩哔哩"><i class="fab fa-bilibili"></i></a><a class="social-icon" href="tencent://message/?uin=1363160029&amp;Site=qq&amp;Menu=yes" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E5%89%8D%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">1.</span> <span class="toc-text">面试前的内容</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%9B%E6%89%A3"><span class="toc-number">1.1.</span> <span class="toc-text">力扣</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8E%86"><span class="toc-number">1.2.</span> <span class="toc-text">简历</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E5%86%85%E5%AE%B9"><span class="toc-number">2.</span> <span class="toc-text">面试内容</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%B9%A0"><span class="toc-number">2.1.</span> <span class="toc-text">实习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A7%8B%E6%8B%9B"><span class="toc-number">2.2.</span> <span class="toc-text">秋招</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%96%B9%E5%90%91"><span class="toc-number">3.</span> <span class="toc-text">算法方向</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E6%96%B9%E5%90%91"><span class="toc-number">3.1.</span> <span class="toc-text">视觉方向</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.2.</span> <span class="toc-text">大模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E5%B9%BF%E6%8E%A8"><span class="toc-number">3.3.</span> <span class="toc-text">搜广推</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E6%8E%A8%E7%90%86"><span class="toc-number">3.4.</span> <span class="toc-text">部署推理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93"><span class="toc-number">3.5.</span> <span class="toc-text">面试内容总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E5%8E%86%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9"><span class="toc-number">4.</span> <span class="toc-text">简历学习内容</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">4.1.</span> <span class="toc-text">大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%B9%A0%E7%9B%B8%E5%85%B3"><span class="toc-number">4.1.1.</span> <span class="toc-text">实习相关</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">实验评价指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%A0%B9%E6%8D%AE%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%B0%83%E6%95%B4-prompt"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">如何根据实验结果调整 prompt</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0"><span class="toc-number">4.1.1.3.</span> <span class="toc-text">数据构造</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A6%82%E4%BD%95%E6%A0%87%E6%B3%A8%E7%9A%84"><span class="toc-number">4.1.1.4.</span> <span class="toc-text">数据如何标注的</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%80%8C%E4%B8%8D%E6%98%AF%E4%B9%8B%E5%89%8D%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E6%84%8F%E4%B9%89%E3%80%81%E4%BC%98%E7%82%B9"><span class="toc-number">4.1.1.5.</span> <span class="toc-text">采用大模型而不是之前的机器学习方法的意义、优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E5%92%8C%E4%B8%8A%E7%BA%BF%E6%89%80%E9%9C%80%E6%8E%AA%E6%96%BD"><span class="toc-number">4.1.1.6.</span> <span class="toc-text">改进和上线所需措施</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA"><span class="toc-number">4.1.2.</span> <span class="toc-text">相关理论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RAG-%E7%9B%B8%E5%85%B3"><span class="toc-number">4.1.3.</span> <span class="toc-text">RAG 相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E8%A7%89"><span class="toc-number">4.1.4.</span> <span class="toc-text">视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">目标检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">人脸识别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E5%85%AB%E8%82%A1"><span class="toc-number">4.1.5.</span> <span class="toc-text">算法八股</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%AE%BA"><span class="toc-number">4.1.5.1.</span> <span class="toc-text">理论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E6%92%95"><span class="toc-number">4.1.5.2.</span> <span class="toc-text">手撕</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E8%A1%A5%E4%B8%AA%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%9A%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%80%83%E6%89%8B%E6%92%95%E9%A2%98"><span class="toc-number">5.</span> <span class="toc-text">最后补个学习的内容：算法工程师面试常考手撕题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Attention%EF%BC%89%E7%AF%87"><span class="toc-number">5.1.</span> <span class="toc-text">注意力（Attention）篇</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95%E5%8D%95%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88ScaledDotProductAttention%EF%BC%89%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.1.</span> <span class="toc-text">手撕单头注意力机制（ScaledDotProductAttention）函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88MultiHeadAttention%EF%BC%89"><span class="toc-number">5.1.2.</span> <span class="toc-text">手撕多头注意力（MultiHeadAttention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%87%BD%E6%95%B0%EF%BC%88SelfAttention%EF%BC%89"><span class="toc-number">5.1.3.</span> <span class="toc-text">手撕自注意力机制函数（SelfAttention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT2-%E8%A7%A3%E7%A0%81%E4%B8%AD%E7%9A%84KV-Cache"><span class="toc-number">5.1.4.</span> <span class="toc-text">GPT2 解码中的KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-MQA-%E7%AE%97%E6%B3%95"><span class="toc-number">5.1.5.</span> <span class="toc-text">手撕 MQA 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AF%87"><span class="toc-number">5.2.</span> <span class="toc-text">基础机器学习算法篇</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-numpy%E5%86%99%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88stochastic-gradient-descent%EF%BC%8CSGD%EF%BC%89"><span class="toc-number">5.2.1.</span> <span class="toc-text">手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-k-means-%E7%AE%97%E6%B3%95"><span class="toc-number">5.2.2.</span> <span class="toc-text">手撕 k-means 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%8B%E6%92%95-Layer-Normalization-%E7%AE%97%E6%B3%95"><span class="toc-number">5.3.</span> <span class="toc-text">手撕 Layer Normalization 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-Batch-Normalization-%E7%AE%97%E6%B3%95"><span class="toc-number">5.3.1.</span> <span class="toc-text">手撕 Batch Normalization 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95%E7%AF%87"><span class="toc-number">5.4.</span> <span class="toc-text">解码算法篇</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E8%B4%AA%E5%BF%83%E6%90%9C%E7%B4%A2-%EF%BC%88greedy-search%EF%BC%89"><span class="toc-number">5.4.1.</span> <span class="toc-text">手撕 贪心搜索 （greedy search）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2-beamsearch-%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.2.</span> <span class="toc-text">手撕 集束搜索 beamsearch 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E6%B8%A9%E5%BA%A6%E5%8F%82%E6%95%B0%E9%87%87%E6%A0%B7%EF%BC%88Temperature-Sampling%EF%BC%89%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.3.</span> <span class="toc-text">手撕 温度参数采样（Temperature Sampling）算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-Top-K-Sampling%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.4.</span> <span class="toc-text">手撕 Top-K Sampling算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-Top-P-Nucleus-Sampling-%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.5.</span> <span class="toc-text">手撕 Top-P (Nucleus) Sampling 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AF%87"><span class="toc-number">5.5.</span> <span class="toc-text">神经网络篇</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-backward-propagation%EF%BC%8CBP-%E6%B3%95"><span class="toc-number">5.5.1.</span> <span class="toc-text">手撕反向传播(backward propagation，BP)法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN-%E6%B3%95"><span class="toc-number">5.5.2.</span> <span class="toc-text">手撕 卷积神经网络(CNN)法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-RNN-%E6%B3%95"><span class="toc-number">5.5.3.</span> <span class="toc-text">手撕 循环神经网络(RNN)法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-LSTM%E6%B3%95"><span class="toc-number">5.5.4.</span> <span class="toc-text">手撕 LSTM法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF-%E7%AE%97%E6%B3%95"><span class="toc-number">5.5.5.</span> <span class="toc-text">手撕 二维卷积 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%AF%87"><span class="toc-number">5.6.</span> <span class="toc-text">位置编码篇</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-%E7%AE%97%E6%B3%95"><span class="toc-number">5.6.1.</span> <span class="toc-text">手撕 绝对位置编码 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E5%8F%AF%E5%AD%A6%E4%B9%A0%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-%E7%AE%97%E6%B3%95"><span class="toc-number">5.6.2.</span> <span class="toc-text">手撕 可学习位置编码 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-%E7%AE%97%E6%B3%95"><span class="toc-number">5.6.3.</span> <span class="toc-text">手撕 相对位置编码 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95-rope-%E7%AE%97%E6%B3%95"><span class="toc-number">5.6.4.</span> <span class="toc-text">手撕 rope 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB"><span class="toc-number">5.7.</span> <span class="toc-text">面试题汇总</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-number">5.8.</span> <span class="toc-text">致谢</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/30/%E7%A7%8B%E6%8B%9B%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB/" title="秋招面试总结分享"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/790dbf2659a8aff75e42f73ecf06fab9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="秋招面试总结分享"/></a><div class="content"><a class="title" href="/2024/11/30/%E7%A7%8B%E6%8B%9B%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB/" title="秋招面试总结分享">秋招面试总结分享</a><time datetime="2024-11-30T00:27:37.000Z" title="发表于 2024-11-30 00:27:37">2024-11-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/06/%E7%A7%8B%E6%8B%9B%E9%87%91%E5%B1%B1%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招金山面试分享"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/2349135dde1b4a4828b209c87569c4a8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="秋招金山面试分享"/></a><div class="content"><a class="title" href="/2024/11/06/%E7%A7%8B%E6%8B%9B%E9%87%91%E5%B1%B1%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招金山面试分享">秋招金山面试分享</a><time datetime="2024-11-06T11:40:37.000Z" title="发表于 2024-11-06 11:40:37">2024-11-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/01/%E7%A7%8B%E6%8B%9B%E4%B8%AD%E9%87%91%E6%89%80%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招中金所面试分享"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/cda763e55b037970dd17031cffcd4e97.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="秋招中金所面试分享"/></a><div class="content"><a class="title" href="/2024/11/01/%E7%A7%8B%E6%8B%9B%E4%B8%AD%E9%87%91%E6%89%80%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招中金所面试分享">秋招中金所面试分享</a><time datetime="2024-11-01T14:39:37.000Z" title="发表于 2024-11-01 14:39:37">2024-11-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/01/%E7%A7%8B%E6%8B%9B%E8%94%9A%E6%9D%A5%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招蔚来面试分享"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/da31bfb39d7d0fb356ac618060ece6b7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="秋招蔚来面试分享"/></a><div class="content"><a class="title" href="/2024/11/01/%E7%A7%8B%E6%8B%9B%E8%94%9A%E6%9D%A5%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招蔚来面试分享">秋招蔚来面试分享</a><time datetime="2024-11-01T12:08:37.000Z" title="发表于 2024-11-01 12:08:37">2024-11-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/31/%E7%A7%8B%E6%8B%9B%E5%B9%BF%E8%81%94%E8%BE%BE%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招广联达"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/e236385b88c57c4a60ef5aaf72189cff.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="秋招广联达"/></a><div class="content"><a class="title" href="/2024/10/31/%E7%A7%8B%E6%8B%9B%E5%B9%BF%E8%81%94%E8%BE%BE%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/" title="秋招广联达">秋招广联达</a><time datetime="2024-10-31T00:04:37.000Z" title="发表于 2024-10-31 00:04:37">2024-10-31</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Jack Zhu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><br>
<a href="https://beian.miit.gov.cn/" target="_blank">陕ICP备2023003055号-1</a></div><div class="footer_custom_text"><span>本网站由 </span><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/logo5.png"align="absmiddle"width="59px" height="30px" /></a><span>提供加速服务</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'hGtO6K0x6GlrX1FhjTwfMaWM-gzGzoHsz',
      appKey: 'Nzeyk2Mmrhm9kBQtsXVYd4QA',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>