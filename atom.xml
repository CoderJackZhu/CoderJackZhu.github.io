<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>学习笔记记录感悟</title>
  
  <subtitle>不乱于心，不困于情，不念过往，不畏将来。</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-10-10T14:15:36.966Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Jack Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>秋招拼多多</title>
    <link href="http://example.com/2024/10/10/%E7%A7%8B%E6%8B%9B%E6%8B%BC%E5%A4%9A%E5%A4%9A%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/10/10/%E7%A7%8B%E6%8B%9B%E6%8B%BC%E5%A4%9A%E5%A4%9A%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-10-10T15:03:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<p>先说十一期间，十一当天下午阿里达摩院发消息要一会直接面试，我说下午有事，能否改成明天早晨，然后改完一会，面试官说要不你先笔试吧，我看笔试都已经10.12了，感觉不妙，也只能这样了，然后10.10直接被终止流程了，笔试被取消了。虽然达摩院我肯定去不了，但是这样一个笔试和面试的机会直接无也还是很难受，看来面试笔试尽量还是别推，不然一不小心直接无就难受了。</p><p>说到这不得不说科大讯飞了，九月中的线下面试，提前打电话说线下没有我这个岗的面试官了，然后说后面线上，结果过了三周了都没动静了，估计那边也不缺人了吧，应该也是直接g了。</p><h2 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h2><p>拼多多这次面试我面的也比较一般，过去自我介绍完然后开始问实习的内容，讲完之后开始问细节，大模型测评用的什么标准，具体是什么，如何衡量好坏，样本构造正负样本如何设置的，效果如何评价，然后我说主要用的精确率，极度不平衡的情况下结果是否有可参考性。对语义解释部分如何正则化提取，遇到无法提取的多吗，有多少，如何处理的。</p><p>然后是问的LoRA微调是否了解，原理是啥，如何做的，显存和速度是否有提升，如何体现，在推理过程中显存和速度是否有提升。</p><p>然后是八股，问我微调用的什么模型，我说盘古智子，然后让我讲下模型结构，这块我说都差不多，简单说了下，但是说的不详细。然后问我是否了解注意力机制，这块大致讲了讲，不是很细。</p><p>最后手撕，先给题目，然后说完思路再做，这次是依次给了两道题。</p><p>第一道是有1，2，5三种硬币若干，求组成价值为21的组合有多少种，这是个背包问题，用动态规划做，然后面试官问了问初始化1什么意义，两层循环是否可以调换。</p><p>第二道题是实现一个开根号的函数，保留n位有效数字，这个题目我写了，不过好像有一点小问题。面试官问了我是否确认这个能实现这个功能</p><p>最后问我是否了解拼多多的工作强度，是否可以接受，可以。然后反问，我问了问部门具体做啥，他们具体根据商家和客户需求找到可能有问题的，然后及时给出平台的措施，也是结合多模态的信息处理，涉及推荐。然后就结束了。</p><p>总体来说感觉面试中答的比较一般，好多问题都准备的不太好，感觉这次面试也是G了。</p>]]></content>
    
    
    <summary type="html">秋招正式批拼多多技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招金山面试分享</title>
    <link href="http://example.com/2024/09/30/%E7%A7%8B%E6%8B%9B%E9%87%91%E5%B1%B1%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/30/%E7%A7%8B%E6%8B%9B%E9%87%91%E5%B1%B1%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-30T12:20:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>今天面的金山，金山这个面试基本是我面试过程最难的几个了，本来面试计划时间半小时，结果面了一个小时，面试官非常深挖项目，问的非常细，而且非常的耐心，里面就问了两个八股，看来要准备的还真的需要很多。</p><p>首先是自我介绍，然后问了下我的实习，第一部分主要是针对实习的具体内容展开，我先讲完做了啥，</p><p>用了什么模型，参数是多少，量级是多少，为什么用千问，盘古智子，千问大模型版本，结构，最大上下文长度是多少，怎么评测，选取哪个大模型。大模型训练出来的结果如何评价，用的什么指标。</p><p>LoRA微调原理，训练哪些层，都哪些超参数，超参数怎么设置的。</p><p>前面这块我了解的不是特别深入，然后就开始讲Prompt这块关于具体任务的内容，数据集怎么构造的，为什么这么构建，标注数据有什么问题，数据量多少，这个项目最终的好坏如何评价。</p><p>后面上线需要解决什么问题，是效果达不到，还是速度达不到，还是其他问题，千问的模型是否可以上线，解决方案是什么，后续怎么做。</p><p>还问了大模型的分布式这块了解不，我问是不是deepspeed这种，是，这块说了两句，不是特别懂。</p><p>然后问我后面哪个项目感觉做的比较深入，然后我说了第一个项目做的深入而且有论文，然后他就开始问了我最后一个项目，这个项目做的比较浅。问到这个项目，量化和剪枝如何做的，参数量是多少，剪枝之后是多少，速度提升多少。量化如何做的，量化之后的参数量是多少，速度提升多少。其中的CPP需要写哪些部分。</p><p>最后是八股，问了下现在各个大模型的架构是啥，主流都是哪些模型，都是啥样的。除了Transformer之外还了解mamba和kan不。</p><p>反问问了他们主要是大模型还是传统，说都有。</p><p>被拷打的很惨，感觉这次面试是最难的，感觉肯定G了。</p>]]></content>
    
    
    <summary type="html">秋招正式批金山技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招滴滴面试分享</title>
    <link href="http://example.com/2024/09/27/%E7%A7%8B%E6%8B%9B%E6%BB%B4%E6%BB%B4%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/27/%E7%A7%8B%E6%8B%9B%E6%BB%B4%E6%BB%B4%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-27T20:20:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h2><p>这次面的滴滴，刚好赶上交大线下双选会，就在交大的楼里面了下，面完十分钟就过了。</p><p>过去自我介绍完之后，面试官就问了下我之前的实习主要做的内容，也问了不少细节，不过大体也答了上来。</p><p>然后问了特征工程是如何做的，具体这块我讲了下数据预处理和筛选相关这些，实际场景一般怎么做的。</p><p>问是否做过大规模的分布式的数据处理，我说实验室没有这个条件，这块估计问的可能是hadoop这种，理论估计还是要了解下。</p><p>然后问了一些八股，过拟合如何处理，需要如何调节。</p><p>LoRA微调是什么，需要调节什么参数，啥时候调节啥参数，为啥要这样调节。</p><p>最后是手撕一道算法题，这个是个场景题，题目还很长，已知n个员工每个员工的工资，钱币有100 50 20 10 5 1六种，每个员工的工资都是这六种钱币的组合，现在要找出这n个员工的工资的组合，使得这n个员工的工资的组合的钱币数量最少，然后手撕了出来。</p><p>面完三天写的复盘，面试的时候问的哪些已经忘得差不多了，看后面约二面的时间。</p>]]></content>
    
    
    <summary type="html">秋招正式批滴滴技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招理想面试分享</title>
    <link href="http://example.com/2024/09/25/%E7%A7%8B%E6%8B%9B%E7%90%86%E6%83%B3%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/25/%E7%A7%8B%E6%8B%9B%E7%90%86%E6%83%B3%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-25T19:00:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>开始投的机器学习，最后被调到LLM了，本来以为会问很多大模型八股，实际也没有那么多，还行，面试了38分钟。</p><p>过去首先自我介绍，然后开始问算法八股，第一个是介绍一下特征工程，我就先从传统特征工程来说的，然以在面试官的提示下也说了数据预处理相关的内容。问RNN、LSTM、Transformer的区别，这块是经典八股了，差不多答了。</p><p>然后是问大模型的实习具体做了啥，这块我讲了好多，感觉后面面试官都不是很耐烦了，毕竟这块东西还挺多。然后是经典的问题，既然机器学习方法能做，为什么用大模型做，这块大致答了下。然后面试官问了解CoT不，我说了解一点，面试官说这个任务也是可以用CoT来做的，然后我也简单回答了具体做的相关性。</p><p>最后是手撕，给了个经典题，word1到word2最少的操作，插入、删除、替换，然后手撕了出来。</p><p>最后反问，问部门做啥，不同岗位之间的区别和调整，面试官说虽然这个岗是LLM，不过还是有很多机器学校方法等的内容，然后就结束了。</p><p>总体答的大差不差，不过感觉有一点KPI面，不过也可能是我自己感觉的，毕竟面试官也是很忙的。</p>]]></content>
    
    
    <summary type="html">秋招正式批理想技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招蚂蚁面试分享</title>
    <link href="http://example.com/2024/09/24/%E7%A7%8B%E6%8B%9B%E8%9A%82%E8%9A%81%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/24/%E7%A7%8B%E6%8B%9B%E8%9A%82%E8%9A%81%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-24T20:40:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>昨天有人加我，说是网商银行的智能引擎部门，主要基于蚂蚁集团大规模数据结合大模型，提升数据的感知和认知能力，帮助千万小商家的信用成长。部门包括多个NLP、大模型、时序和图方向，技术自由度高，且有业务价值。</p><p>然后约了今天上午的一面，今天上午到点是个电话面试，之前还没电话面过。面试首先自我介绍，然后主要讲实习的工作，他们这边应该是大模型比较相关的，所以实习相对契合一点。然后就开始非常细致的讨论实习做的内容，包括很多细节，文件如何处理的，如何表示的，如何训练的，任务是怎么样的，系统的架构，用了哪些机器学习方法，改进使用大模型的意义都有哪些方面，还有如何微调的，用了哪些方面，具体参数如何调整和设置的。然后问到LoRA，问为什么用，原理是什么，需要调整什么，啥时候怎么调整。总之这个问的细致程度基本是其他面试二面的强度，深挖一个项目，还是非常深入的。</p><p>然后面试官说看你简历里面有了解RAG相关内容，然后开始问相关的问题，原理是什么，为什么用RAG，解决了什么问题，还有相关的一些问题，最后问到向量表征都有哪些方法，如何做的，这个确实不是很懂，没怎么研究过。</p><p>然后反问，问了下后面有几面怎么安排，然后就结束了。</p>]]></content>
    
    
    <summary type="html">秋招正式批蚂蚁技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招快手面试分享</title>
    <link href="http://example.com/2024/09/23/%E7%A7%8B%E6%8B%9B%E5%BF%AB%E6%89%8B%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/23/%E7%A7%8B%E6%8B%9B%E5%BF%AB%E6%89%8B%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-23T17:11:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次面试的岗位是机器学习算法，面了一个小时，这次面试也算有质量。</p><p>过去先简单自我介绍，面试官说你这专业算计算机是吧，那计算机相关的理论你应该会，我说我专业属于计算机，不过学的还是偏向电子信息，然后让讲一下堆和栈的区别，这块没怎么讲清楚，只是简单讲了下定义，这块应该从内存中的开辟使用管理啥的开始说的。毕竟这块也是算基础吧，还是要准备下的，前面这块的都没背。</p><p>然后面试官说有论文，是不是人工智能领域的，我说是的。让讲一下论文，然后我翻出来自己的论文，从前到后讲了一遍，不过前面部分我用的时间太多，讲的有点乱，后面讲的就挺快了，讲完面试官说讲的有点乱，听的不是很清楚。</p><p>然后说去实习了就讲下实习的内容，然后我大致讲了讲，面试官简单问了两个小问题，比如效果不好的时候，如何判断是哪里的问题导致的，怎么改正。</p><p>之后面试官说给个数学题你想下：一个圆上取三点，构成锐角三角形的概率是多少，这个我尬住了，想了半天没思路，然后面试官说构成直角三角形的概率是多少，然后再到锐角，最后还是没回答出来。这块答的感觉不太行，没有体现出遇到未知问题解决问题的能力，这个思考过程不够清晰。</p><p>然后让手撕一道题，找无序数组中第K大的数，这个是一个很经典的题了，我用python堆来做的，写出来自然很简单，写完面试官说你用库实现的太简单，要不换个方法，然后说要不你自己用数组实现一个堆，然后我写不出来，又尬住了半天，然后面试官说让用快排的方法写，写了出来了。</p><p>然后反问环节，问他们部门具体做啥，然后哪方面的，说是搜广推的，还有一点具体的方向，然后刚好一个小时，就结束了。</p><p>然后晚上看，果然已经挂了。</p>]]></content>
    
    
    <summary type="html">秋招正式批快手技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招合肥安迅精密技术面试分享</title>
    <link href="http://example.com/2024/09/23/%E7%A7%8B%E6%8B%9B%E5%90%88%E8%82%A5%E5%AE%89%E8%BF%85%E7%B2%BE%E5%AF%86%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/23/%E7%A7%8B%E6%8B%9B%E5%90%88%E8%82%A5%E5%AE%89%E8%BF%85%E7%B2%BE%E5%AF%86%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-23T14:15:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次面试的岗位是图像算法工程师，就面了15分钟7秒，特别短，面的不太行，应该也G了。</p><p>过去先自我介绍，介绍完之后面试官说看你简历里面有cpp，面试官说你会C++是吧，然后我说我只是在前面的一个项目中用到了一点，也是去年初了，后面几乎没怎么用到，所以也不是特别会。然后问CPP基础，第一个问题问值传递和引用有什么区别，第二个问题问虚函数定义是什么，这些都不会，毕竟cpp真很久不用了。</p><p>然后问其中一个项目，问是怎么做的，讲了讲，然后问做了什么创新点，当然是没有，把技术路线讲了讲，问这个项目的难点是什么。之后又问了一个项目，这个也问的不多，简单问问就够了。然后问训练模型都调节什么超参数，过拟合和欠拟合如何处理，注意力机制是什么。</p><p>然后反问，问他们这边是偏应用还是开发啥的，说开发的多一点，很多地方要用cpp，所以这块比较需要。</p><p>这个才是目前线上问的时间最短的，实习都没问，毕竟这边是图像算法。感觉不是很契合，第二天通知果然挂了。</p>]]></content>
    
    
    <summary type="html">秋招正式批合肥安迅精密技术面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招得物面试分享</title>
    <link href="http://example.com/2024/09/22/%E7%A7%8B%E6%8B%9B%E5%BE%97%E7%89%A9%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/22/%E7%A7%8B%E6%8B%9B%E5%BE%97%E7%89%A9%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-22T02:40:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次面试进去面试官就说时间紧，咱们尽快，晚上七点半的面试，看起来这公司还是要加班很多的。</p><p>进去先自我介绍，然后问了我最后面的一个项目，谈谈做的啥，然后问了下细节，数据处理，标注，处理啥的。然后问有啥项目感觉比较有内容的，然后我说了下第一个项目，讲讲背景和具体怎么做的，讲了一些，但是创新性不够，问这个项目的难点在哪里。后面又问了问实习做的啥，然后我就讲具体做的什么，这块也追问了一些细节，然后没讲完，面试官就说时间不多了，然后反问，我问进去是做啥，他说一面只是初筛，具体的方向和部分还是会具体分配，然后就结束了。</p><p>总的来说，得物这个一面是目前线上面试的里面时间最短的，没有之一，部分线下面试有不超过半小时的，这个面试都不到半小时，就问了下项目和实习，没有八股和手撕部分，像淘天这种，八股和手撕是主要，风格还很不一样，不过我八股和手撕都准备的不太好，所以这种风格也不能算太差吧，至少比手撕不出来好。</p><p>面完后面看果然已经挂了。</p>]]></content>
    
    
    <summary type="html">秋招正式批得物面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招比亚迪面试分享</title>
    <link href="http://example.com/2024/09/22/%E7%A7%8B%E6%8B%9B%E6%AF%94%E4%BA%9A%E8%BF%AA%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/22/%E7%A7%8B%E6%8B%9B%E6%AF%94%E4%BA%9A%E8%BF%AA%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-22T02:40:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次是线下面，就在南校区C楼面了，过去也不用叫号啥的，就是随便看人自己排队，这个面试就是人事面，相当于填简历的面试。</p><p>过去先看证件，给简历，然后看本硕学历证明，成绩单，四六级，还要本科的成绩单，我没打印，就给记录上没本科成绩单，然后问有没有挂科，当然没有；然后挨个问，问完记录 ，就是纯粹当面填简历信息，问论文，然后把英文论文翻译成中文录进去，问简历号，然后是竞赛有啥，然后记录专利是啥名字；然后挨个问项目是做的啥，记完名字一句话描述做的啥，然后录系统，挨个问完之后问编程语言，然后是研究方向是啥，也是简历上有的信息。</p><p>然后问了问实习，她好像都没看简历，简历上写的有实习，他还问我有没有，我给他指出来她才看到然后对着把信息填系统。</p><p>问完这些然后问家是哪里的，父母是什么工作，将来想去哪里工作，将来想做啥方面的，我说人工智能这块都比较通，这块都可以做。</p><p>后面还是说本科成绩单的事，所以只好加上微信，后面发过去，然后反问，问他们后面有几面，是不是分配，说一面是人事面，二面技术面，然后也是根据人匹配岗位啥的，然后就结束了。</p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/f0e470325b3d2276037b6d9878231f6.jpg" alt="f0e470325b3d2276037b6d9878231f6"></p><p>后面就发现挂了，进入人才库。</p><p>后面10.10被软开捞了，问cpp的也不太会。</p>]]></content>
    
    
    <summary type="html">秋招正式批比亚迪面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招淘天凉经</title>
    <link href="http://example.com/2024/09/19/%E7%A7%8B%E6%8B%9B%E6%B7%98%E5%A4%A9%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/19/%E7%A7%8B%E6%8B%9B%E6%B7%98%E5%A4%A9%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-19T13:14:20.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/15c68ad5ba57c4307aaf8b5475c4205.png" alt="15c68ad5ba57c4307aaf8b5475c4205"><br>9.19面试的，前面投实习的时候再加秋招前面简历挂了好多次了，这次面的时候就感觉大概率也g了，果然面完半小时就挂了。</p><p>面试过去首先面试官介绍了他们部门是做什么的，然后我做了自我介绍，然后面试官说给你三十分钟，让手撕三个力扣题，第一个是写一个查找字符串数组中的最长公共前缀；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longest_common_prefix</span>(<span class="params">strs</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取第一个字符串作为基准进行比较</span></span><br><span class="line">    prefix = strs[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历数组中剩下的字符串</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> strs[<span class="number">1</span>:]:</span><br><span class="line">        <span class="comment"># 逐渐缩短前缀，直到找到公共前缀</span></span><br><span class="line">        <span class="keyword">while</span> s[:<span class="built_in">len</span>(prefix)] != prefix:</span><br><span class="line">            prefix = prefix[:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> prefix:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prefix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line">strs = [<span class="string">&quot;flower&quot;</span>, <span class="string">&quot;flow&quot;</span>, <span class="string">&quot;flight&quot;</span>]</span><br><span class="line">result = longest_common_prefix(strs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Longest common prefix:&quot;</span>, result)</span><br></pre></td></tr></table></figure><p>时间复杂度：O(S)，其中 S 是数组中所有字符串的字符总数。在最坏情况下，算法需要逐个字符比较每个字符串的前缀。<br>空间复杂度：O(1)，除了输入数据外，额外使用的空间主要用于存储前缀。</p><p>关于这点我就说的很差，时间空间复杂度说的完全不对，我还以为是$O(mn)$， g</p><p>第二个是找出数组中所有可以使得数字和为target的组合，每个元素只能出现一次；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">combination_sum</span>(<span class="params">nums, target</span>):</span><br><span class="line">    <span class="comment"># 先对数组排序，这样可以更容易地进行剪枝操作</span></span><br><span class="line">    nums.sort()</span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义回溯函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">start, path, remaining_target</span>):</span><br><span class="line">        <span class="comment"># 当剩余的target为0时，说明我们找到了一个有效的组合</span></span><br><span class="line">        <span class="keyword">if</span> remaining_target == <span class="number">0</span>:</span><br><span class="line">            result.append(path[:])  <span class="comment"># 将当前路径加入结果中</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历从当前索引开始的每个数字</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># 如果当前数字大于剩余的target，则无需继续搜索</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; remaining_target:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 为了避免重复的组合，我们跳过相同的数字</span></span><br><span class="line">            <span class="keyword">if</span> i &gt; start <span class="keyword">and</span> nums[i] == nums[i - <span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 选择当前数字，继续递归搜索</span></span><br><span class="line">            path.append(nums[i])</span><br><span class="line">            <span class="comment"># 递归，i + 1表示每个元素只能使用一次</span></span><br><span class="line">            backtrack(i + <span class="number">1</span>, path, remaining_target - nums[i])</span><br><span class="line">            <span class="comment"># 回溯，撤销选择</span></span><br><span class="line">            path.pop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用回溯函数</span></span><br><span class="line">    backtrack(<span class="number">0</span>, [], target)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试示例</span></span><br><span class="line">nums = [<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>]</span><br><span class="line">target = <span class="number">8</span></span><br><span class="line">combinations = combination_sum(nums, target)</span><br><span class="line"><span class="built_in">print</span>(combinations)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>时间复杂度是指数级的 $O(2^n)$，属于递归经典的时间复杂度了，我这里说成了$O(N^2)$， 明显的直接g了， 这里的空间复杂度好像不太好算，应该是$O(k*2^n + n)$， k 是每个组合的平均长度， n是数组的长度。</p><p>第三题是一个链表数组，其中每个已经按照升序排列，合并成一个升序链表。写完后说时间和空间复杂度，这块没说清楚，对于第三题，问我为什么用堆，有什么优势，堆的几种基本操作的时间复杂度是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义链表节点的类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ListNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val=<span class="number">0</span>, <span class="built_in">next</span>=<span class="literal">None</span></span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="built_in">next</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_k_sorted_lists</span>(<span class="params">lists</span>):</span><br><span class="line">    <span class="comment"># 创建一个最小堆</span></span><br><span class="line">    heap = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将每个链表的头节点加入最小堆</span></span><br><span class="line">    <span class="keyword">for</span> i, lst <span class="keyword">in</span> <span class="built_in">enumerate</span>(lists):</span><br><span class="line">        <span class="keyword">if</span> lst:</span><br><span class="line">            heapq.heappush(heap, (lst.val, i, lst))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 哨兵节点，用于构建最终的合并链表</span></span><br><span class="line">    dummy = ListNode()</span><br><span class="line">    current = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 进行堆的操作，直到堆为空</span></span><br><span class="line">    <span class="keyword">while</span> heap:</span><br><span class="line">        <span class="comment"># 弹出堆中的最小值</span></span><br><span class="line">        val, i, node = heapq.heappop(heap)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将弹出的节点连接到最终结果链表中</span></span><br><span class="line">        current.<span class="built_in">next</span> = node</span><br><span class="line">        current = current.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果弹出的节点还有后续节点，将它的后续节点加入堆中</span></span><br><span class="line">        <span class="keyword">if</span> node.<span class="built_in">next</span>:</span><br><span class="line">            heapq.heappush(heap, (node.<span class="built_in">next</span>.val, i, node.<span class="built_in">next</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回合并后的链表</span></span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line"><span class="comment"># 构建几个升序链表</span></span><br><span class="line">list1 = ListNode(<span class="number">1</span>, ListNode(<span class="number">4</span>, ListNode(<span class="number">5</span>)))</span><br><span class="line">list2 = ListNode(<span class="number">1</span>, ListNode(<span class="number">3</span>, ListNode(<span class="number">4</span>)))</span><br><span class="line">list3 = ListNode(<span class="number">2</span>, ListNode(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">lists = [list1, list2, list3]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并链表</span></span><br><span class="line">merged_list = merge_k_sorted_lists(lists)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出合并后的链表</span></span><br><span class="line"><span class="keyword">while</span> merged_list:</span><br><span class="line">    <span class="built_in">print</span>(merged_list.val, end=<span class="string">&quot; -&gt; &quot;</span>)</span><br><span class="line">    merged_list = merged_list.<span class="built_in">next</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;None&quot;</span>)</span><br></pre></td></tr></table></figure><p>第三题这个不算特别难的，感觉比第二题还好一点，但是对于堆的相关概念说的不是很清楚，这个时间复杂度是$O(Nlogk)$，N是所有链表的节点总数，k是链表的个数，空间复杂度是$O(k)$，k是链表的个数。</p><p>然后让我讲下transformer结构，大致说了下，然后问transformer相比RNN有什么优势。这块实习的时候淘天就问过这个问题，当时说的模糊，思路不是很清晰，现在秋招了这块也没怎么看过，仍然说的不是很清楚，八股还是要背的，这种基础知识都说不好的肯定不行。</p><p>GPT总结了一些：</p><blockquote><ol><li>并行计算能力<br>RNN：RNN 是一种序列模型，必须按时间步依次处理输入数据。也就是说，第 t 时刻的输出依赖于第 t-1 时刻的计算结果，因此不能并行化处理序列中的元素，这导致训练时间较长。<br>Transformer：Transformer 利用自注意力机制（Self-Attention），使得每个输入位置的处理可以与其他位置的元素并行进行。这意味着 Transformer 能够在同一时间处理整个输入序列，从而大大加快了训练和推理速度。这种并行化特性使 Transformer 非常适合在 GPU 上运行，尤其对于长序列，训练速度相较 RNN 显著提升。</li><li>捕捉长距离依赖关系的能力<br>RNN：RNN 的主要缺点之一是难以有效捕捉长距离依赖关系。由于其每个时间步的状态依赖于上一个时间步的状态，随着序列的增长，长距离的依赖可能被遗忘（梯度消失问题）。虽然 LSTM 和 GRU 在一定程度上缓解了这个问题，但仍然存在限制。<br>Transformer：通过自注意力机制，Transformer 可以直接建模序列中任意两个位置之间的依赖关系，无论这两个位置相隔多远。这意味着 Transformer 在处理长序列时，能够高效且准确地捕捉长距离的依赖关系，而不会像 RNN 那样受到序列长度的限制。</li><li>消除递归结构，提升效率<br>RNN：RNN 的递归结构需要依次处理序列中的每个元素，这种顺序性导致模型无法并行执行，尤其在长序列任务中，RNN 的训练和推理效率较低。<br>Transformer：Transformer 通过完全消除递归结构，取而代之的是基于全局自注意力机制的并行计算，这大大提升了计算效率。每个位置的输出只需通过矩阵运算来计算，无需逐个时间步依赖前一个输出，极大地提升了效率。</li><li>更好的记忆和上下文捕捉<br>RNN：RNN 的记忆是通过隐状态（hidden state）来传递的，每一步的状态依赖于前一步的信息传递。这种方式在处理长序列时会因为过多的信息传递而产生信息损失，导致模型难以保留远距离的上下文信息。<br>Transformer：Transformer 使用自注意力机制，使得每个词或元素能够直接与其他所有元素建立联系。这种全局视角能够让模型在同一层次上捕捉到整个输入序列的全局信息，从而更好地保留上下文信息。</li><li>避免梯度消失问题<br>RNN：RNN 依赖于逐步传递的隐状态，训练时容易遇到梯度消失或梯度爆炸的问题，尤其是在处理长序列时，梯度的传播会变得非常困难。<br>Transformer：Transformer 没有依赖于序列传递的隐状态，而是通过自注意力机制和前馈神经网络来计算输出。这样就避免了在长序列上梯度消失或爆炸的问题，训练更为稳定，尤其在长文本或长序列任务中表现尤为显著。</li><li>更强的表示能力<br>RNN：RNN 只能通过顺序计算来处理序列信息，这种线性的计算方式在处理复杂序列依赖时可能有局限。<br>Transformer：通过多头自注意力机制（Multi-Head Attention），Transformer 可以从多个子空间同时处理信息，每个注意力头关注不同的信息模式，增强了模型的表示能力。多头自注意力机制允许模型同时关注输入序列中的不同部分，获取不同层次的特征。</li><li>适应性更强的输入表示<br>RNN：RNN 的输入表示是通过序列顺序传递的，缺乏对输入元素之间关系的灵活表示。<br>Transformer：Transformer 通过<strong>位置编码（Positional Encoding）</strong>来为序列中的每个位置引入位置信息。这种设计使得 Transformer 在处理文本等有序数据时，不会像 RNN 一样依赖于输入的顺序，还可以灵活适应各种输入长度和结构。</li><li>更容易并行扩展<br>RNN：RNN 的递归结构天然不适合并行处理，尤其在训练非常深层的 RNN 模型时，计算资源的利用效率较低。<br>Transformer：Transformer 的并行性和层级结构使其更容易扩展到大规模数据和模型，特别适合深层模型的并行计算。因为没有递归关系，Transformer 可以利用 GPU 进行大规模的并行计算，大幅提升训练效率和处理能力。</li><li>更适合预训练和迁移学习<br>RNN：尽管 RNN 可以在特定任务上进行预训练，但其能力有限，尤其在处理大规模数据和迁移学习时，效果不如 Transformer。<br>Transformer：Transformer 的架构非常适合大规模预训练模型，如 BERT、GPT 系列。这些模型通过在大规模数据上进行无监督预训练，能够很好地迁移到各种下游任务中，并且预训练的表示可以用于不同领域的任务，大大提高了模型的泛化能力和迁移学习能力。</li></ol></blockquote><p>之后是问简历上的实习经历，大致问了下，讲了背景，然后说具体是做了哪方面的工作，经典的一个问题就是，这个任务用大模型做有什么优势，这块我说我就是预研的一个探索，用大模型端到端的做这件事，实际上需要的数据什么的还是用的传统方法获得的，这块其实这样说还是不太好。另外涉及具体的工作，他们这边好像也是有做类似的东西，但是我也没讲的很出彩，然后中间问到了一个问题，我听的不是很明白，面试官就没什么兴趣了，我这块还没说完然后差不多就一个小时了，就让反问，然后就结束了。</p><p>面完就感觉不行了，过一会去看就挂了，淘天还是这么喜欢让手撕代码，这块还是要准备下。总之投实习的时候就淘天面的最差，现在秋招了，淘天还是面的最差的，问题还是有很多，面完要复盘总结经验。</p>]]></content>
    
    
    <summary type="html">秋招正式批淘天面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招中兴面试分享</title>
    <link href="http://example.com/2024/09/18/%E7%A7%8B%E6%8B%9B%E4%B8%AD%E5%85%B4%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/18/%E7%A7%8B%E6%8B%9B%E4%B8%AD%E5%85%B4%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-18T16:46:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次面试是秋招正式批的第一个线下面试，问的比较浅显，我查了下，一面应该是技术面，不过好像技术的东西问的不多。</p><p>过去首先检查学生证和身份证，然后给简历，自我介绍，然后就要看成绩单，问有没有论文，我说有论文，之后让介绍实习做了啥，难点是啥，大致说下我的工作。其他的项目都没怎么问到。</p><p>然后问我是想做哪方面的，是大模型研发微调还是应用，我说尽可能的偏向研发，然后问从哪个项目可以提现这个能力，我说前面做的项目应用的比较多，但是倾向还是偏向更技术一些的。</p><p>然后反问，然后我问了问部门是做啥的，他说看我意愿分配到适合的部门，面试二十多分钟就结束了，也没有力扣环节。</p><h1 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h1><p>一面完很快就发了二面的预约，9.18 上午的面试，这次人巨多， 等了好久，过去还是先检查学生证和身份证，然后给简历，自我介绍，这次涉及技术的更少了。问了爱好，实习的收获有哪些，然后我就详细讲了讲这块关于实习和在学校。</p><p>之后是问对中兴了解如何，如何看待加班，期望薪酬是多少，有没有其他的offer，我说了下，然后问我有没有问题，我问了问部门是做啥的，然后面试官说一面没问吗，确实没问，然后问意向地点去哪里，我说杭州或者北上广深这种一线城市，大致也就这些</p>]]></content>
    
    
    <summary type="html">秋招正式批中兴面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招美团机器学习面试分享</title>
    <link href="http://example.com/2024/09/12/%E7%A7%8B%E6%8B%9B%E7%BE%8E%E5%9B%A2%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/09/12/%E7%A7%8B%E6%8B%9B%E7%BE%8E%E5%9B%A2%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-09-12T00:14:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>这次面试是秋招正式批的第一个面试，八股什么的也都没有准备，就直接开始面试了。</p><p>首先介绍一下自己，介绍完之后问实习的经历，然后我就讲了讲业务背景是什么，自己在里面干了什么，但是面试官应该不是做这个的，所以也不是很感兴趣。</p><p>然后开始问我的第一个项目，这个发了论文的，问这个项目是背景和做了什么，问了下细节，把大致的流程讲了差不多，然后问这个项目的难点是什么，这个我倒是没有怎么总结，因为都没有说很难的地方。</p><p>然后面试官开始问八股了，YOLO相关的内容，按说我做的视觉项目比较多，这块应该比较懂，不过这块确实很久没看过了，连续问了三个问题回答的都一般，首先让我讲yolo的大致结构，我讲到提特征用darknet，然后讲为什么用darknet不用resnet，这块我就已经忘了，然后讲池化金字塔融合有哪些形式是什么样的，我就简单讲了下，也说的不是很清楚，然后后面的模块简单说了下，之后问发展过程的演变我也是只简单说了下。然后问了下yolo的图像增强方法都有哪些，我简单说了一两个，不太行，回去还是要准备下。</p><p>之后是问第二个项目是做什么的，然后我大致讲了下背景和具体怎么做的，然后这块也没有问太多，毕竟这个含金量也不是很高。</p><p>之后就直接让手撕了，这次考的题目是求最长的斐波那契数列子序列的长度，想了会也写了出来。</p><p>然后是反问环节，我问了问他们部门具体是做什么的，然后这块我也简单聊了聊3dgs和无人机相关那些东西，然后面试官说感谢你的参与，然后就结束了。</p><h1 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h1><p>9.18号下午三点的面试，这次面的挺延伸的。</p><p>过去首先自我介绍，然后让我挑一个认为做的深的项目讲一下，我就讲了下第一个项目，然后首先讲背景，这里就讲了好久，我说这个可以降温增效，面试官就要很清楚的了解，到底降低了什么成本，是在性能的改进，还是成本的改进，还是时间的改进，这个明确之后，才继续讲。然后是讲技术方案，这块也是细致的扣，技术如何这样做会不会有误差，如何保证，具体怎么做。然后是问认为这个项目的难点是什么，我说主要是把这个整个体系实现出来，然后大致讲讲如何看待这个事，是因为在学校和公司的差异。之后就是一个问题，关于测量的真实值如何获得，我简单说了下。然后是开放性问题，问给很多很多钱的情况下如何找到精准真实值，我说找很多专业人士，然后用好的仪器，然后面试官问还有啥，我说不知道了，然后面试官给了点提示，无人驾驶的，然后我说多模态融合激光雷达什么的，也算是答的差不多。后面也是开放的聊了一些。</p><p>之后是代码环境，要求实现一个容器，其中两个函数实现增加元素和删除元素，然后一开始懵逼，提示了下大致写了下思路，时间不够就把大致怎么实现的说了下，也算是差不多说清楚了，就是代码写的还不太完整。</p><p>总体来说，这种很延伸的还是很难的。</p>]]></content>
    
    
    <summary type="html">秋招正式批美团机器学习面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>秋招提前批百度机器学习/数据挖掘/自然语言处理工程师面试分享</title>
    <link href="http://example.com/2024/08/06/%E7%A7%8B%E6%8B%9B%E6%8F%90%E5%89%8D%E6%89%B9%E7%99%BE%E5%BA%A6%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/08/06/%E7%A7%8B%E6%8B%9B%E6%8F%90%E5%89%8D%E6%89%B9%E7%99%BE%E5%BA%A6%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-08-06T01:03:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h1><p>今天周一我六点多下班就回来面试，面试官首先让我自我介绍，然后我自我介绍完，然后是简单问一下我项目情况以及简历上各种内容的情况，然后展开论文讲解，然后我就把论文的背景和内容，然后创新点什么的讲解了一下，然后突出自己的优势，做的比较多学习新东西比较快。</p><p>然后问了问自己在华为实习，这段时间都做了什么？然后我也是简单讲了讲背景，然后讲自己在做的东西这块儿内容有些单薄，后面要丰富一下，这块儿我都没有怎么准备，所以说着坑坑巴巴的。</p><p>然后面试官问自己，在之前遇到什么困难，然后怎么解决的，然后我就换方向这个内容展开讲了讲，说之前遇到这些困难，当然现在解决了，现在做新东西学东西新东西都挺快。</p><p>然后面试官说让手撕代码，然后我说这块儿还没怎么开始准备秋招，然后给了一个题，对于一个长度为六的链表，然后如何反转456项，然后我就按照最简单粗暴的方法，然后他说长度不定，用通用的方法来做，然后我就用一般的方法做基本思路，还是说出来了，先找到一半的节点的头节点，然后面试官说，既然有思路，那就让本地IDE写了。我简单写了写，然后卡住了，翻转这块。想了一会儿，想出来了，这块儿也花了一会儿，还是不是很熟练。</p><p>然后面试官问还有什么想问的，我看面试官都有点不耐烦了，然后简单问了问他们部门儿是做什么的，然后因为我投的是语音部门，所以感觉还是不太match，最后说完面试官说感谢你的参与，全程面了一个小时，这样回答感觉大概率是G了，问题不大就第一场，后面还有机会慢慢准备先把实习做好。</p><h1 id="2024-8-19二面"><a href="#2024-8-19二面" class="headerlink" title="2024.8.19二面"></a>2024.8.19二面</h1><p>这次面试问的非常细，让自己选一个项目，然后深挖。首先讲了讲自己的项目大致背景、流程、以及主要用到的模型，然后提问这个数据是什么，要提特征，然后讲用了什么模型，模型输入输出是什么，架构是什么，我简单说了之后面试官让详细讲，我就讲了讲思路，但是整个模型的结构就忘了。之后问主体模型是啥，输入输出，架构是啥，为啥用这个模型，然后这个模型是单向还是双向的，为什么，然后单向双向网络有什么区别，感觉可以从模型设计的区别和代理任务的区别展开。然后问了问，这个项目你做了两年了，现在回过头来看，你会用什么来做，如何设计训练，输入输出是什么，预训练的话用什么做，对应的任务如何设置。这些还是挺难的，而且还不是那种能查出来的问题。</p><p>然后问了问未来发展如何想，是只做算法还是什么，然后我说因为本硕都学的这个，所以选这个感觉适合，后面做其他的学习能力也很强可以快速学好。</p><p>然后是简单的编程，写一个排序算法，然后我写了个插入排序。</p><p>最后是提问，我问了我部门做啥，具体什么场景。</p><h1 id="2024-9-2三面"><a href="#2024-9-2三面" class="headerlink" title="2024.9.2三面"></a>2024.9.2三面</h1><p>二面完本来约上周，但是上周都抽不出时间，只好这周，还是周一。这次面试主要还是聊聊天，首先做自我介绍，然后就问实习这段时间有什么收获，和学校有什么不同，我说一方面是技术上的，另一方面是从学生到职场转换这一过渡；在学校是一个人做项目，在企业分工合作做一个大的，然后问这两个哪个好，我说看根据具体的项目来定，大项目合作分工，小项目单人做。然后就是聊现在技术发展趋势和未来职业规划，我说跟着时代趋势走，同时积累自己的核心能力，然后面试官就在说时代趋势和语音这块发发展。<br>后面到反问环节，我问部门主要做什么，然后主要就是语音交互，相关的技术。最后我问了问部门升职和培养机制。</p><p>其他还问了有没有拿到其他offer， 投了哪些了，我就如实说，还没offer，也基本没开始投。</p><p>总的来说，比较偏向于聊天，但是具体要不要我真看不出来，希望能过吧，然后就说联系HR了。后面如果过的话应该就进池子了，不知道啥时候能泡出来。</p>]]></content>
    
    
    <summary type="html">秋招提前批百度机器学习/数据挖掘/自然语言处理工程师面试分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>腾讯AI Lab暑实面试分享</title>
    <link href="http://example.com/2024/05/28/%E8%85%BE%E8%AE%AFAI%20Lab%E6%9A%91%E5%AE%9E%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/05/28/%E8%85%BE%E8%AE%AFAI%20Lab%E6%9A%91%E5%AE%9E%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-05-28T10:59:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试前的内容"><a href="#面试前的内容" class="headerlink" title="面试前的内容"></a>面试前的内容</h2><p>本来投的计算机视觉，但是一直在简历筛选中，5.26我看腾讯实习都快结束了，视觉这块肯定做不了，改投了机器学习，然后昨天就接到了约面，今天上午面，腾讯的算法好像就叫技术研究-机器学习方向。<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240528110403.png" alt="20240528110403"></p><h2 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h2><p>面试首先让做自我介绍，之后让选一个项目深入讲解，我做的通用项目比较多，然后大致概括了下，之后选了一个项目，然后问这个项目做的啥，技术亮点是啥，这里问的也是很细，然后我说主要还是任务上应用到这个领域，然后问如何衡量效果，设计什么指标，效果如何，误差是多少；然后是问数据量多少，是否足够，如何判断数据量足够，质量如何，进行了什么预处理；讲具体用了mmseg这个框架，然后问具体用了什么模型，为什么选这个模型，这个模型结构是什么，模型的优缺点是什么。</p><p>然后让讲解一下YOLO系列，这块我没法详细的讲解，只能说个大体框架，然后每一代的改进简单提了一点，也没详细讲出来。</p><p>然后让讲transformer的结构，我大致讲了下，然后问qkv的物理意义，问为什么现在大模型都是decoder结构的，大致说了下和bert的区别，然后其中讲的比较宏观。</p><p>所以面试官问了SAM了解的如何，这块我还真不太了解，然后说用了下，感觉太慢，效果还行。然后问关于大模型理论这块的看法，然后我说现在视觉大模型还不太够，主要还是基于LLM，然后简单问了下LLM相关。</p><p>之后就是手撕，一个字符串求最大的无重复字符串长度，这个不算难，用的滑动窗口法，面试官说复杂度O(n)太高，有没有优化空间，不知道。</p><p>之后就是反问环节了，我问了问他们部门是做什么的，说是游戏和强化学习相关的，我说这块感觉是未来的方向。之后问我啥时候能去，我说六月多到九月多可以三个月，然后问老师放不放，我说没问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说感觉问题跳跃度非常高，从一个问题回答中间提到了其他技术，就继续追问，很多新技术我了解的并不深入，比如SAM这样。但是我在项目准备上略有欠缺，YOLO系列也没有系统的整理，这次面试官也只问了一个项目，让捡着一个详细说然后不断发散，前面的钉钉和淘天的视觉面就问的特别细，几乎每个项目，论文，竞赛等等都问了一遍。</p><p>一个问题就是我做的项目虽然多，但是大部分都是含金量不太够的项目，一深挖就会发现很工程，嗑盐创新点不够，不够深入，还是要有至少一个项目做的非常深，这样每次面试的时候只需要对这一个做的非常好的项目做准备就可以了，毕竟大部分面试也不会把每个项目都拷问一遍，贪多没用，简历写了一大堆也就那样。</p><p>这可是腾讯AI Lab，我也是太勇了，这种感觉几乎没有可能过，level太高了，这应该是我面试这么多以来最高的了。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>非常合理的面试挂了，毕竟面试开始前我就预估不通过率能有95%，这要是能过我只能说腾讯是真没人了，没有意外的非常合理。</p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240530145109.png" alt="20240530145109"></p>]]></content>
    
    
    <summary type="html">腾讯AI Lab暑实面试记录分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>阿里国际暑实面试分享</title>
    <link href="http://example.com/2024/05/23/%E9%98%BF%E9%87%8C%E5%9B%BD%E9%99%85%E6%9A%91%E5%AE%9E%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/05/23/%E9%98%BF%E9%87%8C%E5%9B%BD%E9%99%85%E6%9A%91%E5%AE%9E%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-05-23T13:43:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试前的内容"><a href="#面试前的内容" class="headerlink" title="面试前的内容"></a>面试前的内容</h2><p>本来阿里国际投的算法工程，结果简历筛选就没过，然后给调到了大模型应用开发，这块我没做过具体项目，感觉是要G了。</p><p>果然G了<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240528131347.png" alt="20240528131347"></p><h2 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h2><p>这块首先是做了两分钟的自我介绍，然后问哪个项目印象深刻，然后讲了下，之后问论文，然后简单讲下，然后说有没有网络模型的创新，我说这块没有；然后简单问下大模型知道多少，我简单说了一点理论和应用上的了解；关于项目和论文，也基本没有追问，没有仔细考察，可能毕竟和大模型差的也很多吧，然后就是反问环节，问了部门是做啥的，然后说主要是做大模型的应用开发，然后具体说是做应用，设计一些模型改进和调整什么的。</p><h2 id="手撕代码"><a href="#手撕代码" class="headerlink" title="手撕代码"></a>手撕代码</h2><p>力扣这块我做的确实还是不到位，大部分手撕都做不出来，这次出的是对一个$\m \times n$的矩阵，从每行中取出一个数然后相加，使得总和与一个target值的绝对差最小，绝对差是值两个数之差的绝对值，然后我就想半天也没想出来，然后就结束了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这次面试感觉还是挺差的，毕竟大模型这块我没做过，也没怎么了解，力扣这块也是要多刷题，总之这次感觉是有点KPI面，问的也比较粗略，感觉肯定是G了。</p>]]></content>
    
    
    <summary type="html">阿里国际技术的面试记录分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>钉钉和淘天机器学习捞暑期实习面试分享</title>
    <link href="http://example.com/2024/05/22/%E9%92%89%E9%92%89%E5%92%8C%E6%B7%98%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8D%9E%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/05/22/%E9%92%89%E9%92%89%E5%92%8C%E6%B7%98%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8D%9E%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-05-22T15:43:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试前的内容"><a href="#面试前的内容" class="headerlink" title="面试前的内容"></a>面试前的内容</h2><p>本来今天中午是打算面钉钉的，前面推了三次，终于约到了今天中午，结果提前淘天打电话让直接面试了，所以今天就先面了淘天</p><h2 id="淘天面试"><a href="#淘天面试" class="headerlink" title="淘天面试"></a>淘天面试</h2><p>在之前的时候淘天的视觉已经G了</p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240522153311.png" alt="20240522153311"></p><p>事实证明，视觉这块真不行，要求太高了，真做不了，不是简历挂就是面试挂，今天给调到了机器学习。</p><p>面试首先就是自我介绍，然后开始问项目，问哪个项目印象深刻，然后就重点讲了第三个项目，讲了之后问论文做了啥，大致都讲了下，然后是比赛，问做的过程中用了什么方法，前几名是用的什么方法，有什么差距，具体是哪里的差距；然后我用的什么方法，有什么亮点，我说前几名用的主要方法都差不太多，重点在于策略和具体的应用调整。反正这块主要就是拷问简历上的项目了，毕竟前面笔试啥的都弄了，算法八股倒是没问。</p><p>之后是反问环节，然后问他们部门是做什么的，然后说他们主要是做搜广推的，主要是搜索和广告，然后大致讲了做啥。</p><p>已经G了</p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240528131547.png" alt="20240528131547"></p><h2 id="钉钉面试"><a href="#钉钉面试" class="headerlink" title="钉钉面试"></a>钉钉面试</h2><p>刚面完淘天就是面钉钉，钉钉的这个笔试和面试挨着的，这个笔试不是力扣算法题，而是深度学习方面的题，首先手撕三选二：</p><ol><li>层归一化</li><li>多头注意力</li><li>kmeans</li></ol><p>层归一化我确实不太会理论，就没写，多头注意力是经典手撕了，凑合写了下，kmeans我是很熟悉的，毕竟大三第一次写这种算法就是kmeans， 手撕完之后让逐行讲解，凑合讲了下，不过细节还是有点漏，这块可以再看看。</p><p>之后是说怎么项目都是去年的，我说前面也有个，然后问了问论文，也问的详细，相比前人贡献是啥，我说这块前人工作很少，我主要是开辟了这样一个流程和处理方法，然后各种细节问了下，这块问的很细，具体来说，这个部分包括一个流程，其中视频部分具体用了什么模型，为什么用这个模型，这个模型架构是什么样的，具体是如何实现这个功能的，对比效果如何；检测和关键点部分用了什么模型，相应的模型架构是什么样的，卷积神经网络和transformer相比特点分别是什么。</p><p>然后问常见的激活函数都有哪些，然后都介绍一下，这块比较新的我都没怎么记，就只说了下之前的激活函数。</p><p>然后问最喜欢的网络模型是什么，为什么喜欢这个模型，讲一下结构，然后相比前人工作改进是什么，我大致讲了下，不过模型结构讲的不够细节。</p><p>之后是说看我简历上写了多模态大模型啥的，我说这块了解了些，但是发现算力什么的差很多，然后主要是做老师的项目，所以这块只是了解。</p><p>然后问职业规划，这块我没准备，也没经验，只能凑合说下，后面还是要准备下。</p><p>之后是说我学习能力强，各种项目都做，所以迁移到学习新东西也很快，然后问具体如何体现，这块我就不是很知道怎么回答了。</p><p>最后反问环节，我也是问了部门是做的什么，说部门主要三个方向，第一是风控，第二个是啥我给忘了，第三个就是大模型文生文，图生成文，语言和图文多模态啥的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这两场面完仍然是汗流浃背，总之还是比上次淘天的要好一点吧，至少手撕八股准备了下。具体这次问项目和论文的细节也是非常的细致，具体很多问题已经记不清了，总之简历上的东西还是要详细到问每个细节都能答得上来。</p>]]></content>
    
    
    <summary type="html">淘天技术的面试记录分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>淘天暑期实习计算机视觉面试凉经</title>
    <link href="http://example.com/2024/04/29/%E6%B7%98%E5%A4%A9%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/04/29/%E6%B7%98%E5%A4%A9%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-04-29T20:43:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="面试凉经"><a href="#面试凉经" class="headerlink" title="面试凉经"></a>面试凉经</h1><p>估计没有二面了，这次被拷打的狠狠的，估计是无了，第一次视觉面，没有准备相关内容,第二天去看，果然已经流程结束<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240430181423.png" alt="20240430181423"></p><h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>和前面一样先自我介绍。</p><h2 id="项目拷打"><a href="#项目拷打" class="headerlink" title="项目拷打"></a>项目拷打</h2><p>这次我投的计算机视觉算法，面试的比较对口，面试官就问的很细，面试首先是问了几个项目的内容，我前面做的任务主要是通用视觉目标检测、姿态估计、分割等这些的项目，这些项目都没什么深度，主要是用，然后问的很细，就很难受，被拷打出来了没做啥有深度的内容，主要是应用，这块就不太行了。前面的面试面试官不是研究视觉的，就只大致问问做项目过程中遇到的难点，困难如何解决的，这次要先把做的任务说清楚，不清楚就会继续不断追问，然后说清楚做了什么，涉及较难的部分还会重点追问。但是这部分我往往都是只应用，并没有结构上的创新，然后具体用的什么模型，如何训练，如何处理，等等都会追问，问的非常细，项目虽然是自己做的，但是很多东西也没怎么想到，就非常难受。这里面除了一个配准的专利面试官可能没做过没问，其他所有项目比赛都非常仔细的抠细节。</p><p>现在想想，前面做过的项目也没有系统的整理里面的亮点和创新点等等，而且做的时候也很多时候只是用现有的技术做了，但是没有去想过创新点，含金量就不太够，还是要把做过的东西包装一下。</p><h2 id="简单的数据处理脚本"><a href="#简单的数据处理脚本" class="headerlink" title="简单的数据处理脚本"></a>简单的数据处理脚本</h2><p>然后面试官说让做题，首先是一个简单的路径字符串列表变成文件树的任务，这个非常简单的，这种类似的见得多了，但是当时太紧张了，然后这个我就没写出来，实在是不应该，其实另外一个写不出来的问题在于，平时写代码都有Github Copilot辅助，基本都是只需要写个大致的思路，辅助编程就能自动补全了，具体某个函数名字和参数什么的真不怎么记住，这个也是一个问题，以后要多写多记住一些基础的东西。</p><p>具体题目是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">news/car/suv</span><br><span class="line">news/car/mpv</span><br><span class="line">news/sport/basketball</span><br><span class="line">news/sport/football</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tree</span>(<span class="params">str_list</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(str_list)</span><br><span class="line">    category_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        strs = str_list[i].split(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">        category, name = strs[<span class="number">1</span>], strs[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> category <span class="keyword">not</span> <span class="keyword">in</span> category_dict:</span><br><span class="line">            <span class="comment"># 这一步我都没想起来，实在是太离谱了</span></span><br><span class="line">            category_dict[category] = [name]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            category_dict[category].append(name)</span><br><span class="line">    <span class="keyword">return</span> category_dict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_tree</span>(<span class="params">category_dict, depth=<span class="number">1</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;news&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> category_dict.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span> * depth + key)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(value, <span class="built_in">dict</span>):</span><br><span class="line">            print_tree(value, depth + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> value:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span> * (depth + <span class="number">1</span>) + v)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    str_list = [<span class="string">&#x27;news/car/suv&#x27;</span>, <span class="string">&#x27;news/car/mpv&#x27;</span>, <span class="string">&#x27;news/sport/basketball&#x27;</span>, <span class="string">&#x27;news/sport/football&#x27;</span>]</span><br><span class="line">    category_dict = tree(str_list)</span><br><span class="line">    print_tree(category_dict)</span><br></pre></td></tr></table></figure><h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>让说一下logistic回归，我就说了下公式，然后说前些年用，因为方便求导，简单说了下，具体用来更新模型的参数没说。这相关的算法也是学过的，但是要清晰讲出来还是要准备的。</p><h2 id="手撕注意力"><a href="#手撕注意力" class="headerlink" title="手撕注意力"></a>手撕注意力</h2><p>然后让写一个attention的公式，然后也没写出来，就简单说了下原理，肯定是不行的，这种都是算法的八股，我应该提前准备下的。</p><h3 id="chatgpt答案"><a href="#chatgpt答案" class="headerlink" title="chatgpt答案"></a>chatgpt答案</h3><p>注意力机制是一种让模型能够在处理信息时自动地聚焦于最重要信息的技术。在深度学习中，注意力机制通常用于序列模型，特别是在自然语言处理领域。最常见的注意力机制是由 Vaswani 等人在 2017 年提出的“Scaled Dot-Product Attention”。</p><h4 id="注意力机制的数学公式"><a href="#注意力机制的数学公式" class="headerlink" title="注意力机制的数学公式"></a>注意力机制的数学公式</h4><p>基础的注意力机制可以表示为三个核心组件：查询（Query），键（Key），值（Value）。给定查询 ( Q )，键 ( K )，和值 ( V )，注意力权重计算通常包括以下步骤：</p><ol><li><p><strong>注意力得分（Score）计算</strong>：<br>[ \text{Score}(Q, K) &#x3D; QK^T ]</p></li><li><p><strong>缩放（Scaling）</strong>：<br>[ \text{Scaled Score} &#x3D; \frac{\text{Score}}{\sqrt{d_k}} ]<br>其中 ( d_k ) 是键向量的维度，这一步骤是为了避免计算中的梯度过小。</p></li><li><p><strong>Softmax 归一化</strong>：<br>[ \text{Attention Weights} &#x3D; \text{softmax}(\text{Scaled Score}) ]</p></li><li><p><strong>输出计算</strong>：<br>[ \text{Output} &#x3D; \text{Attention Weights} \cdot V ]</p></li></ol><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>在 Python 中，可以使用 PyTorch 这样的深度学习框架来实现注意力机制。下面是一个简单的 Scaled Dot-Product Attention 的实现示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 Scaled Dot-Product Attention。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    Q: 查询张量 [batch_size, num_heads, query_len, key_dim]</span></span><br><span class="line"><span class="string">    K: 键张量 [batch_size, num_heads, key_len, key_dim]</span></span><br><span class="line"><span class="string">    V: 值张量 [batch_size, num_heads, value_len, value_dim]</span></span><br><span class="line"><span class="string">    mask: 掩码张量，用于遮蔽不需要的值 [batch_size, 1, 1, key_len]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    output: 注意力机制的结果 [batch_size, num_heads, query_len, value_dim]</span></span><br><span class="line"><span class="string">    attention_weights: 注意力权重 [batch_size, num_heads, query_len, key_len]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    d_k = Q.size(-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 Q 和 K 的点积，然后除以 sqrt(d_k)</span></span><br><span class="line">    scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果有掩码，将掩码加到 scores 上</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用 Softmax 函数获取注意力权重</span></span><br><span class="line">    attention_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算最终的输出</span></span><br><span class="line">    output = torch.matmul(attention_weights, V)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output, attention_weights</span><br></pre></td></tr></table></figure><p>这段代码实现了 Scaled Dot-Product Attention，其中 <code>Q</code>, <code>K</code>, <code>V</code> 是输入的张量，可来自相同的或不同的输入源。<code>mask</code> 参数用于可选的掩码操作，通常用在处理变长序列时忽略填充项。注意力权重通过点积、缩放、Softmax 计算得出，最后使用这些权重对值 <code>V</code> 进行加权求和，得到每个查询的输出。</p><h4 id="手撕Attention"><a href="#手撕Attention" class="headerlink" title="手撕Attention"></a>手撕Attention</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        <span class="comment"># NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights</span></span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="MLP的代码"><a href="#MLP的代码" class="headerlink" title="MLP的代码"></a>MLP的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h2><p>然后问扩散模型知道不，然后我说没做过，这里也是改准备下的，毕竟这么著名的模型，我虽然大致了解原理还有DDPM什么的，但是如果认真的讲出来，不成体系，我也就没敢说，这块还是要准备下。</p><h3 id="chatgpt回答"><a href="#chatgpt回答" class="headerlink" title="chatgpt回答"></a>chatgpt回答</h3><p>扩散模型是一种新兴的深度学习方法，特别是在生成模型领域中表现出色。这种模型基于随机过程理论，特别是扩散过程，通过逐步增加随机性来转换数据，然后再逐步学习如何逆转这一过程，从而生成高质量的数据样本。这一方法在图像和声音生成中尤其流行。</p><h4 id="扩散模型的基本原理"><a href="#扩散模型的基本原理" class="headerlink" title="扩散模型的基本原理"></a>扩散模型的基本原理</h4><p>扩散模型包括两个主要阶段：<strong>正向过程</strong>（forward process）和<strong>反向过程</strong>（reverse process）。</p><ol><li><p><strong>正向过程（噪声添加过程）</strong>：<br>在正向过程中，模型逐步地将噪声引入到原始数据中。这通常通过多个时间步骤实现，每个时间步骤都会向数据添加更多的高斯噪声，直到数据完全转变为纯噪声。这个过程可以用以下等式表示：<br>[<br>x_{t} &#x3D; \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_t<br>]<br>其中，( x_t ) 是在时间 ( t ) 的数据，( \alpha_t ) 是时间步骤中噪声的比例，而 ( \epsilon_t ) 是从标准正态分布中采样的噪声。</p></li><li><p><strong>反向过程（噪声去除过程）</strong>：<br>反向过程旨在学习如何从纯噪声数据逐步恢复出原始数据。这是通过一个参数化的神经网络模型实现的，该模型试图预测给定噪声数据的前一时间步骤的状态。反向过程的目标是逐渐减少数据中的噪声，最终恢复出接近原始数据的状态。</p></li></ol><h4 id="训练扩散模型"><a href="#训练扩散模型" class="headerlink" title="训练扩散模型"></a>训练扩散模型</h4><p>扩散模型的训练通常依赖于变分下界（ELBO，Evidence Lower Bound）的优化。模型的目标是最小化重构数据和原始数据之间的差异，同时也优化生成数据的概率分布。训练过程需要大量的计算资源，因为它涉及到大量的正向和反向模拟步骤。</p><h2 id="FM算法"><a href="#FM算法" class="headerlink" title="FM算法"></a>FM算法</h2><p>然后问机器学习算法FM算法，然后我说不知道，这个我还真没听说过，查了下，是推荐的算法，因子分解机，真不懂，现学记住吧，chatgpt说：</p><h3 id="chatgpt回答-1"><a href="#chatgpt回答-1" class="headerlink" title="chatgpt回答"></a>chatgpt回答</h3><p>FM（Factorization Machines，因子分解机）是一种广泛用于推荐系统和预测任务的机器学习算法。它被设计用来处理具有极大稀疏性的数据集，特别是在用户-物品交互数据中常见的那种稀疏性。</p><p><em>基本原理</em>: 因子分解机模型主要是为了解决传统线性模型（如线性回归、逻辑回归）在处理大规模稀疏数据集时的不足。在传统的线性模型中，每个特征的权重是独立学习的，这使得模型难以捕捉特征间的交互作用。FM通过引入隐向量来为每个特征学习潜在的交互效应，这样可以有效地捕捉到特征之间的相互作用。<br><em>数学表示</em>: 给定一个包含$n$个特征的数据集，FM模型的预测$\hat{y}$由下式给出：<br>$$<br>\hat{y}(\mathbf{x}) &#x3D; w_0 + \sum_{i&#x3D;1}^{n}w_i x_i + \sum_{i&#x3D;1}^{n} \sum_{j&#x3D;i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j<br>$$<br>其中：</p><ul><li>$w_0$ 是模型的全局偏置。</li><li>$w_i$ 是第$i$个特征的权重。</li><li>$\mathbf{x}$ 是输入特征向量。</li><li>$\mathbf{v}_i$ 和 $\mathbf{v}_j$ 是特征$i$和$j$的隐向量，隐向量的维度$k$通常远小于特征数量$n$。</li><li>$\langle \mathbf{v}_i, \mathbf{v}_j \rangle$ 表示特征$i$和$j$的隐向量的点积，用于模拟这两个特征的交互效果。</li></ul><p><em>优势</em>:</p><ol><li><p><strong>高效性</strong>：虽然FM模型包含二阶特征交互项，但是它可以在$O(kn)$的时间复杂度内完成计算，其中$k$是隐向量的维度，$n$是特征的数量。这种高效性主要来源于隐向量的点积运算方式，使得算法适用于大规模的数据集。</p></li><li><p><strong>灵活性</strong>：FM模型不仅适用于任何实数特征，还可以很容易地扩展到高阶交互，如三阶或更高阶的交互。</p></li><li><p><strong>泛化能力</strong>：由于特征间的交互是通过低维隐向量学习的，FM可以很好地泛化到新的、未见过的特征组合，尤其是在数据稀疏的场景下。</p></li></ol><p><em>应用场景</em>：</p><ul><li><strong>推荐系统</strong>：FM广泛用于推荐系统，能有效处理用户和物品的特征交互。</li><li><strong>点击率预测</strong>：在线广告和搜索引擎优化中，FM可以用来预测用户点击广告或搜索结果的概率。</li><li><strong>排名和分类任务</strong>：通过适当的目标函数和优化方法，FM可以适用于各种排名和分类问题。</li></ul><p>因子分解机因其在处理稀疏数据和特征交互中的优势而被广泛研究和应用。</p><h2 id="手写交叉熵损失"><a href="#手写交叉熵损失" class="headerlink" title="手写交叉熵损失"></a>手写交叉熵损失</h2><p>然后让写CrossEntropy，我还是没写出来，然后简单说了下公式的思想，</p><h3 id="chatgpt回答-2"><a href="#chatgpt回答-2" class="headerlink" title="chatgpt回答"></a>chatgpt回答</h3><p>交叉熵损失（Cross-Entropy Loss）是用于衡量模型预测概率分布与真实标签概率分布之间的差异的一个常用损失函数。它在分类任务中特别常见，尤其是在处理二分类或多分类问题时。</p><h4 id="交叉熵损失的公式"><a href="#交叉熵损失的公式" class="headerlink" title="交叉熵损失的公式"></a>交叉熵损失的公式</h4><p>对于二分类问题，交叉熵损失可以表示为：</p><p>[ L &#x3D; -\frac{1}{N} \sum_{i&#x3D;1}^N [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] ]</p><p>其中：</p><ul><li>( N ) 是样本的数量。</li><li>( y_i ) 是第 ( i ) 个样本的真实标签，通常为 0 或 1。</li><li>( \hat{y}_i ) 是模型预测第 ( i ) 个样本为正类的概率。</li></ul><p>对于多分类问题，公式扩展为：</p><p>[ L &#x3D; -\frac{1}{N} \sum_{i&#x3D;1}^N \sum_{c&#x3D;1}^C y_{i,c} \log(\hat{y}_{i,c}) ]</p><p>其中：</p><ul><li>( C ) 是类别的总数。</li><li>( y_{i,c} ) 是一个指示变量，如果样本 ( i ) 属于类别 ( c )，则为 1，否则为 0。</li><li>( \hat{y}_{i,c} ) 是模型预测第 ( i ) 个样本属于类别 ( c ) 的概率。</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p>如果不使用库函数，可以手动实现交叉熵损失函数。这里我们将以 Python 为例展示如何实现多分类交叉熵损失函数。这将涉及到计算模型输出的对数概率，然后根据真实标签计算损失。</p><h4 id="二分类交叉熵损失"><a href="#二分类交叉熵损失" class="headerlink" title="二分类交叉熵损失"></a>二分类交叉熵损失</h4><p>首先，我们从较简单的二分类交叉熵损失开始：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_cross_entropy_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 防止 log(0) 导致数值问题，所以加上一个非常小的值 epsilon</span></span><br><span class="line">    epsilon = <span class="number">1e-12</span></span><br><span class="line">    y_pred = np.clip(y_pred, epsilon, <span class="number">1</span> - epsilon)</span><br><span class="line">    loss = -np.mean(y_true * np.log(y_pred) + (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0.1</span>, <span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Binary Cross Entropy Loss:&quot;</span>, binary_cross_entropy_loss(y_true, y_pred))</span><br></pre></td></tr></table></figure><h4 id="多分类交叉熵损失"><a href="#多分类交叉熵损失" class="headerlink" title="多分类交叉熵损失"></a>多分类交叉熵损失</h4><p>对于多分类问题，我们通常使用独热编码的方式来表示真实的标签，然后计算每个类别的交叉熵损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">categorical_cross_entropy_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 防止 log(0) 导致数值问题，所以加上一个非常小的值 epsilon</span></span><br><span class="line">    epsilon = <span class="number">1e-12</span></span><br><span class="line">    y_pred = np.clip(y_pred, epsilon, <span class="number">1.0</span> - epsilon)</span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    loss = -np.<span class="built_in">sum</span>(y_true * np.log(y_pred)) / y_true.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">y_true = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">y_pred = np.array([</span><br><span class="line">    [<span class="number">0.05</span>, <span class="number">0.9</span>, <span class="number">0.05</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.8</span>],</span><br><span class="line">    [<span class="number">0.7</span>, <span class="number">0.2</span>, <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>]</span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Categorical Cross Entropy Loss:&quot;</span>, categorical_cross_entropy_loss(y_true, y_pred))</span><br></pre></td></tr></table></figure><p>在这些示例中，<code>y_pred</code> 中的值必须是有效的概率分布，即所有预测概率的和应为1，且各概率值应在0到1之间。在实际应用中，模型的输出通常通过 softmax 函数来保证这一点。<code>np.clip</code> 用于防止数值问题，如计算对数时输入值为0。这些实现提供了对交叉熵损失计算的直接控制，避免了依赖外部库函数的需求。</p><h2 id="U-Net相关"><a href="#U-Net相关" class="headerlink" title="U-Net相关"></a>U-Net相关</h2><p>我说研究方向主要是医学影像分割分类，然后面试官说前面做的这些和这个也没什么关系，然后问医学影像出了什么成果，然后我也没，然后问U-Net，我说了下，然后问U-Net的原理，我说了下，然后后上下采样是如何做的，残差连接是如何做的，我说是拼接，这块要说原理我也是说的不太清楚，也没系统准备过。</p><p>然后问那个医学影像的竞赛，说用到nnUNet了，然后介绍下，也说不清。</p><p>然后问科研医学影像现在做到什么阶段了，我就说是分割分类任务的大致框架，具体仍然说不系统。</p><h2 id="反问"><a href="#反问" class="headerlink" title="反问"></a>反问</h2><p>然后就是反问，我问了实习那边任务是应用还是偏科研，然后面试官说不会以发论文为目的，是算法在具体任务重应用和优化，然后就结束了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总之这个面试是最差的一次，属于是项目没啥东西被拷打，后面问八股我也一点没准备，直接凉透，面完直接胳膊下面全是汗，湿透了，估计是凉了，不过也是一次经验，以后要多准备一些基础知识，吃一堑长一智，加油吧。</p><h1 id="最后补个学习的内容：算法工程师面试常考手撕题"><a href="#最后补个学习的内容：算法工程师面试常考手撕题" class="headerlink" title="最后补个学习的内容：算法工程师面试常考手撕题"></a>最后补个学习的内容：算法工程师面试常考手撕题</h1><p>引用链接<a href="https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw">https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw</a></p><ul><li>算法工程师面试常考手撕题<ul><li>注意力（Attention）篇<ul><li>手撕单头注意力机制（ScaledDotProductAttention）函数</li><li>手撕多头注意力（MultiHeadAttention）</li><li>手撕自注意力机制函数（SelfAttention）</li><li>GPT2 解码中的KV Cache</li><li>手撕 MQA 算法</li></ul></li><li>基础机器学习算法篇<ul><li>手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）</li><li>手撕 k-means 算法</li></ul></li><li>手撕 Layer Normalization 算法</li><li>手撕 Batch Normalization 算法</li><li>解码算法篇<ul><li>手撕 贪心搜索 （greedy search）</li><li>手撕 集束搜索 beamsearch 算法</li><li>手撕 温度参数采样（Temperature Sampling）算法</li><li>手撕 Top-K Sampling算法</li><li>手撕 Top-P (Nucleus) Sampling 算法</li></ul></li><li>神经网络篇<ul><li>手撕反向传播(backward propagation，BP)法</li><li>手撕 卷积神经网络(CNN)法</li><li>手撕 循环神经网络(RNN)法</li><li>手撕 LSTM法</li><li>手撕 二维卷积 算法</li></ul></li><li>位置编码篇<ul><li>手撕 绝对位置编码 算法</li><li>手撕 可学习位置编码 算法</li><li>手撕 相对位置编码 算法</li><li>手撕 rope 算法</li></ul></li><li>面试题汇总</li><li>致谢</li></ul></li></ul><h2 id="注意力（Attention）篇"><a href="#注意力（Attention）篇" class="headerlink" title="注意力（Attention）篇"></a><strong>注意力（Attention）篇</strong></h2><h3 id="手撕单头注意力机制（ScaledDotProductAttention）函数"><a href="#手撕单头注意力机制（ScaledDotProductAttention）函数" class="headerlink" title="手撕单头注意力机制（ScaledDotProductAttention）函数"></a><strong>手撕单头注意力机制（ScaledDotProductAttention）函数</strong></h3><p>输入是query和 key-value，注意力机制首先计算query与每个key的关联性（compatibility），每个关联性作为每个value的权重（weight），各个权重与value的乘积相加得到输出。<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444477-8e426dd3-37ed-473f-ab5f-7d445ed7592b.jpeg#clientId=u8884b877-175c-4&from=paste&id=u147acd2d&originHeight=75&originWidth=390&originalType=url&ratio=1&rotation=0&showTitle=false&size=4542&status=done&style=none&taskId=u5130429b-212c-4651-83d6-557bf8c86d9&title=" alt="image.jpg"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class ScaledDotProductAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Scaled Dot-Product Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, scale):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.softmax = nn.Softmax(dim=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, q, k, v, mask=None):</span><br><span class="line">        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul</span><br><span class="line">        u = u / self.scale # 2.Scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if mask is not None:</span><br><span class="line">            u = u.masked_fill(mask, -np.inf) # 3.Mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn = self.softmax(u) # 4.Softmax</span><br><span class="line">        output = torch.bmm(attn, v) # 5.Output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_q, n_k, n_v = 2, 4, 4</span><br><span class="line">    d_q, d_k, d_v = 128, 128, 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    q = torch.randn(batch, n_q, d_q)</span><br><span class="line">    k = torch.randn(batch, n_k, d_k)</span><br><span class="line">    v = torch.randn(batch, n_v, d_v)</span><br><span class="line">    mask = torch.zeros(batch, n_q, n_k).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))</span><br><span class="line">    attn, output = attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn)</span><br><span class="line">    print(output)</span><br></pre></td></tr></table></figure><h3 id="手撕多头注意力（MultiHeadAttention）"><a href="#手撕多头注意力（MultiHeadAttention）" class="headerlink" title="手撕多头注意力（MultiHeadAttention）"></a><strong>手撕多头注意力（MultiHeadAttention）</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">class MultiHeadAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Multi-Head Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_q = nn.Linear(d_k_, n_head * d_k)</span><br><span class="line">        self.fc_k = nn.Linear(d_k_, n_head * d_k)</span><br><span class="line">        self.fc_v = nn.Linear(d_v_, n_head * d_v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_o = nn.Linear(n_head * d_v, d_o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, q, k, v, mask=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        batch, n_q, d_q_ = q.size()</span><br><span class="line">        batch, n_k, d_k_ = k.size()</span><br><span class="line">        batch, n_v, d_v_ = v.size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        q = self.fc_q(q) # 1.单头变多头</span><br><span class="line">        k = self.fc_k(k)</span><br><span class="line">        v = self.fc_v(v)</span><br><span class="line">        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)</span><br><span class="line">        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)</span><br><span class="line">        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if mask is not None:</span><br><span class="line">            mask = mask.repeat(n_head, 1, 1)</span><br><span class="line">        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat</span><br><span class="line">        output = self.fc_o(output) # 4.仿射变换得到最终输出</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_q, n_k, n_v = 2, 4, 4</span><br><span class="line">    d_q_, d_k_, d_v_ = 128, 128, 64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    q = torch.randn(batch, n_q, d_q_)</span><br><span class="line">    k = torch.randn(batch, n_k, d_k_)</span><br><span class="line">    v = torch.randn(batch, n_v, d_v_)    </span><br><span class="line">    mask = torch.zeros(batch, n_q, n_k).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)</span><br><span class="line">    attn, output = mha(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn.size())</span><br><span class="line">    print(output.size())</span><br></pre></td></tr></table></figure><h3 id="手撕自注意力机制函数（SelfAttention）"><a href="#手撕自注意力机制函数（SelfAttention）" class="headerlink" title="手撕自注意力机制函数（SelfAttention）"></a><strong>手撕自注意力机制函数（SelfAttention）</strong></h3><p>Self-Attention。和Attention类似，他们都是一种注意力机制。不同的是Attention是source对target，输入的source和输出的target内容不同。例如英译中，输入英文，输出中文。而Self-Attention是source对source，是source内部元素之间或者target内部元素之间发生的Attention机制，也可以理解为Target&#x3D;Source这种特殊情况下的注意力机制。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class SelfAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Self-Attention &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(self, n_head, d_k, d_v, d_x, d_o):</span><br><span class="line">        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))</span><br><span class="line">        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))</span><br><span class="line">        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.init_parameters()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def init_parameters(self):</span><br><span class="line">        for param in self.parameters():</span><br><span class="line">            stdv = 1. / np.power(param.size(-1), 0.5)</span><br><span class="line">            param.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, x, mask=None):</span><br><span class="line">        q = torch.matmul(x, self.wq)   </span><br><span class="line">        k = torch.matmul(x, self.wk)</span><br><span class="line">        v = torch.matmul(x, self.wv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn, output = self.mha(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return attn, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    n_x = 4</span><br><span class="line">    d_x = 80</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x = torch.randn(batch, n_x, d_x)</span><br><span class="line">    mask = torch.zeros(batch, n_x, n_x).bool()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)</span><br><span class="line">    attn, output = selfattn(x, mask=mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(attn.size())</span><br><span class="line">    print(output.size())</span><br></pre></td></tr></table></figure><h3 id="GPT2-解码中的KV-Cache"><a href="#GPT2-解码中的KV-Cache" class="headerlink" title="GPT2 解码中的KV Cache"></a><strong>GPT2 解码中的KV Cache</strong></h3><p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。<br>也就是，解码的时候，先根据当前输入  ，生成下一个  ，然后把新生成的  拼接在  后面，获得新的输入  ，再用  生成  ，依此迭代，直到生成结束。<br>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。<br>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。<br>也就是说中间有很多我们用不到的计算，这样就造成了浪费。<br>而且随着生成的结果越来越多，输入的长度也越来越长，上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写作文，那可能就有800个step。这个情况下，step0被算了800次，step1被算了799次…这样浪费的计算资源确实不容忽视。<br>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？<br>答案就是KV Cache，利用一个缓存，把需要重复利用的中间计算结果存下来，减少重复计算。<br>而 k 和 v 就是我要缓存的对象。<br>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要老实地计算一遍。然后把 k 、 v 值缓存起来。<br>则有<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444470-9305c89b-aa36-4998-b623-9c3b1c0caf41.jpeg#clientId=u8884b877-175c-4&from=paste&id=u1cb9436b&originHeight=196&originWidth=467&originalType=url&ratio=1&rotation=0&showTitle=false&size=6239&status=done&style=none&taskId=ud1d4d6ec-170b-458b-a212-499e5614e66&title=" alt="image.jpg"><br>kv cache的下标l表示模型层数。<br>在进行第二次预测，也就是预测第5个字的时候，在第l层的时候，由于前面我们缓存了每层的ku 值，那本层就只需要算新的 o3，而不用算 o0、o1、o2。<br>因为第l层的 o0、o1、o2本来会经过FNN层之后进到 l十1 层，再经过新的投影变换，成为 l + 1 层的 k、υ 值，但是l十 1 层的 k、υ值我们已经缓存过了!<br>然后我们把本次新增算出来的 k、υ 值也存入缓存。<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444594-19c3638e-08f5-4a3c-b392-afed682a17a1.jpeg#clientId=u8884b877-175c-4&from=paste&id=udd8910f7&originHeight=185&originWidth=482&originalType=url&ratio=1&rotation=0&showTitle=false&size=8952&status=done&style=none&taskId=u082ebbba-f43b-4beb-9b72-69ac67eb0ef&title=" alt="image.jpg"><br>这样就节省了attention和FFN的很多重复计算。<br>transformers中，生成的时候传入use_cache&#x3D;True就会开启KV Cache。<br>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">Class GPT2Attention(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    def forward(</span><br><span class="line">        self,</span><br><span class="line">        hidden_states: Optional[Tuple[torch.FloatTensor]],</span><br><span class="line">        layer_past: Optional[Tuple[torch.Tensor]] = None,</span><br><span class="line">        attention_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        head_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        encoder_hidden_states: Optional[torch.Tensor] = None,</span><br><span class="line">        encoder_attention_mask: Optional[torch.FloatTensor] = None,</span><br><span class="line">        use_cache: Optional[bool] = False,</span><br><span class="line">        output_attentions: Optional[bool] = False,</span><br><span class="line">    ) -&gt; Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:</span><br><span class="line">        if encoder_hidden_states is not None:</span><br><span class="line">            if not hasattr(self, &quot;q_attn&quot;):</span><br><span class="line">                raise ValueError(</span><br><span class="line">                    &quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span><br><span class="line">                    &quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            query = self.q_attn(hidden_states)</span><br><span class="line">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        else:</span><br><span class="line">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class="line">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class="line">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        # 过去所存的值</span><br><span class="line">        if layer_past is not None:</span><br><span class="line">            past_key, past_value = layer_past</span><br><span class="line">            key = torch.cat((past_key, key), dim=-2)  # 把当前新的key加入</span><br><span class="line">            value = torch.cat((past_value, value), dim=-2)  # 把当前新的value加入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if use_cache is True:</span><br><span class="line">            present = (key, value)  # 输出用于保存</span><br><span class="line">        else:</span><br><span class="line">            present = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.reorder_and_upcast_attn:</span><br><span class="line">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class="line">        else:</span><br><span class="line">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class="line">        attn_output = self.c_proj(attn_output)</span><br><span class="line">        attn_output = self.resid_dropout(attn_output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        outputs = (attn_output, present)</span><br><span class="line">        if output_attentions:</span><br><span class="line">            outputs += (attn_weights,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return outputs  # a, present, (attentions)</span><br></pre></td></tr></table></figure><p>总的来说，KV Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask attention的存在，使得前面的token可以不用关注后面的token）</p><h3 id="手撕-MQA-算法"><a href="#手撕-MQA-算法" class="headerlink" title="手撕 MQA 算法"></a><strong>手撕 MQA 算法</strong></h3><p>MQA 让所有的头之间 共享 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">class MultiQueryAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Multi-Query self attention.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Using torch or triton attention implemetation enables user to also use</span><br><span class="line">    additive bias.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        d_model: int,</span><br><span class="line">        n_heads: int,</span><br><span class="line">        attn_impl: str = &#x27;triton&#x27;,</span><br><span class="line">        clip_qkv: Optional[float] = None,</span><br><span class="line">        qk_ln: bool = False,</span><br><span class="line">        softmax_scale: Optional[float] = None,</span><br><span class="line">        attn_pdrop: float = 0.0,</span><br><span class="line">        low_precision_layernorm: bool = False,</span><br><span class="line">        verbose: int = 0,</span><br><span class="line">        device: Optional[str] = None,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attn_impl = attn_impl</span><br><span class="line">        self.clip_qkv = clip_qkv</span><br><span class="line">        self.qk_ln = qk_ln</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = d_model // n_heads</span><br><span class="line">        self.softmax_scale = softmax_scale</span><br><span class="line">        if self.softmax_scale is None:</span><br><span class="line">            self.softmax_scale = 1 / math.sqrt(self.head_dim)</span><br><span class="line">        self.attn_dropout_p = attn_pdrop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.Wqkv = nn.Linear(</span><br><span class="line">            d_model,</span><br><span class="line">            d_model + 2 * self.head_dim,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        fuse_splits = (d_model, d_model + self.head_dim)</span><br><span class="line">        self.Wqkv._fused = (0, fuse_splits)  # type: ignore</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.attn_fn = scaled_multihead_dot_product_attention</span><br><span class="line">        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)</span><br><span class="line">        self.out_proj._is_residual = True  # type: ignore</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(</span><br><span class="line">        self,</span><br><span class="line">        x,</span><br><span class="line">        past_key_value=None,</span><br><span class="line">        attn_bias=None,</span><br><span class="line">        attention_mask=None,</span><br><span class="line">        is_causal=True,</span><br><span class="line">        needs_weights=False,</span><br><span class="line">    ):</span><br><span class="line">        qkv = self.Wqkv(x)                                      # (1, 512, 960)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.clip_qkv:</span><br><span class="line">            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        query, key, value = qkv.split(                                  # query -&gt; (1, 512, 768)</span><br><span class="line">            [self.d_model, self.head_dim, self.head_dim],               # key   -&gt; (1, 512, 96)</span><br><span class="line">            dim=2                                                       # value -&gt; (1, 512, 96)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        key_padding_mask = attention_mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if self.qk_ln:</span><br><span class="line">            # Applying layernorm to qk</span><br><span class="line">            dtype = query.dtype</span><br><span class="line">            query = self.q_ln(query).to(dtype)</span><br><span class="line">            key = self.k_ln(key).to(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context, attn_weights, past_key_value = self.attn_fn(</span><br><span class="line">            query,</span><br><span class="line">            key,</span><br><span class="line">            value,</span><br><span class="line">            self.n_heads,</span><br><span class="line">            past_key_value=past_key_value,</span><br><span class="line">            softmax_scale=self.softmax_scale,</span><br><span class="line">            attn_bias=attn_bias,</span><br><span class="line">            key_padding_mask=key_padding_mask,</span><br><span class="line">            is_causal=is_causal,</span><br><span class="line">            dropout_p=self.attn_dropout_p,</span><br><span class="line">            training=self.training,</span><br><span class="line">            needs_weights=needs_weights,</span><br><span class="line">            multiquery=True,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return self.out_proj(context), attn_weights, past_key_value</span><br></pre></td></tr></table></figure><h2 id="基础机器学习算法篇"><a href="#基础机器学习算法篇" class="headerlink" title="基础机器学习算法篇"></a><strong>基础机器学习算法篇</strong></h2><h3 id="手撕-numpy写线性回归的随机梯度下降（stochastic-gradient-descent，SGD）"><a href="#手撕-numpy写线性回归的随机梯度下降（stochastic-gradient-descent，SGD）" class="headerlink" title="手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）"></a><strong>手撕 numpy写线性回归的随机梯度下降（stochastic gradient descent，SGD）</strong></h3><p>在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，对于最优化问题，凸问题，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"># 数据加载</span><br><span class="line">from sklearn.datasets import fetch_california_housing</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, Y = fetch_california_housing(return_X_y=True)</span><br><span class="line">X.shape, Y.shape  # (20640, 8), (20640, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据预处理</span><br><span class="line">ones = np.ones(shape=(X.shape[0], 1))</span><br><span class="line">X = np.hstack([X, ones])</span><br><span class="line">validate_size = 0.2</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validate_size, shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># batch 函数</span><br><span class="line">def get_batch(batchsize: int, X: np.ndarray, Y: np.ndarray):</span><br><span class="line">    assert 0 == X.shape[0]%batchsize, f&#x27;&#123;X.shape[0]&#125;%&#123;batchsize&#125; != 0&#x27;</span><br><span class="line">    batchnum = X.shape[0]//batchsize</span><br><span class="line">    X_new = X.reshape((batchnum, batchsize, X.shape[1]))</span><br><span class="line">    Y_new = Y.reshape((batchnum, batchsize, ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for i in range(batchnum):</span><br><span class="line">        yield X_new[i, :, :], Y_new[i, :]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 损失函数</span><br><span class="line">def mse(X: np.ndarray, Y: np.ndarray, W: np.ndarray):</span><br><span class="line">    return 0.5 * np.mean(np.square(X@W-Y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def diff_mse(X: np.ndarray, Y: np.ndarray, W: np.ndarray):</span><br><span class="line">    return X.T@(X@W-Y) / X.shape[0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 模型训练</span><br><span class="line">lr = 0.001          # 学习率</span><br><span class="line">num_epochs = 1000   # 训练周期</span><br><span class="line">batch_size = 64     # |每个batch包含的样本数</span><br><span class="line">validate_every = 4  # 多少个周期进行一次检验</span><br><span class="line">def train(num_epochs: int, batch_size: int, validate_every: int, W0: np.ndarray, X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray):</span><br><span class="line">    loop = tqdm(range(num_epochs))</span><br><span class="line">    loss_train = []</span><br><span class="line">    loss_validate = []</span><br><span class="line">    W = W0</span><br><span class="line">    # 遍历epoch</span><br><span class="line">    for epoch in loop:</span><br><span class="line">        loss_train_epoch = 0</span><br><span class="line">        # 遍历batch</span><br><span class="line">        for x_batch, y_batch in get_batch(64, X_train, Y_train):</span><br><span class="line">            loss_batch = mse(X=x_batch, Y=y_batch, W=W)</span><br><span class="line">            loss_train_epoch += loss_batch*x_batch.shape[0]/X_train.shape[0]</span><br><span class="line">            grad = diff_mse(X=x_batch, Y=y_batch, W=W)</span><br><span class="line">            W = W - lr*grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        loss_train.append(loss_train_epoch)</span><br><span class="line">        loop.set_description(f&#x27;Epoch: &#123;epoch&#125;, loss: &#123;loss_train_epoch&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        if 0 == epoch%validate_every:</span><br><span class="line">            loss_validate_epoch = mse(X=X_test, Y=Y_test, W=W)</span><br><span class="line">            loss_validate.append(loss_validate_epoch)</span><br><span class="line">            print(&#x27;============Validate=============&#x27;)</span><br><span class="line">            print(f&#x27;Epoch: &#123;epoch&#125;, train loss: &#123;loss_train_epoch&#125;, val loss: &#123;loss_validate_epoch&#125;&#x27;)</span><br><span class="line">            print(&#x27;================================&#x27;)</span><br><span class="line">    plot_loss(np.array(loss_train), np.array(loss_validate), validate_every)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 程序运行</span><br><span class="line">W0 = np.random.random(size=(X.shape[1], ))  # 初始权重</span><br><span class="line">train(num_epochs=num_epochs, batch_size=batch_size, validate_every=validate_every, W0=W0, X_train=X_train, Y_train=Y_train, X_test=X_test, Y_test=Y_test)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444701-894badfa-d3a2-4f1b-ac27-42ec384fcb6e.jpeg#clientId=u8884b877-175c-4&from=paste&id=ub19aa012&originHeight=423&originWidth=670&originalType=url&ratio=1&rotation=0&showTitle=false&size=45178&status=done&style=none&taskId=u4c4ffcc6-6980-4366-84f1-ac4104983cd&title=" alt="image.jpg"></p><h3 id="手撕-k-means-算法"><a href="#手撕-k-means-算法" class="headerlink" title="手撕 k-means 算法"></a><strong>手撕 k-means 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def kmeans(data, k, thresh=1, max_iterations=100):</span><br><span class="line">  # 随机初始化k个中心点</span><br><span class="line">  centers = data[np.random.choice(data.shape[0], k, replace=False)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for _ in range(max_iterations):</span><br><span class="line">    # 计算每个样本到各个中心点的距离</span><br><span class="line">    distances = np.linalg.norm(data[:, None] - centers, axis=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 根据距离最近的中心点将样本分配到对应的簇</span><br><span class="line">    labels = np.argmin(distances, axis=1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 更新中心点为每个簇的平均值</span><br><span class="line">    new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 判断中心点是否收敛，多种收敛条件可选</span><br><span class="line">    # 条件1：中心点不再改变</span><br><span class="line">    if np.all(centers == new_centers):</span><br><span class="line">      break</span><br><span class="line">    # 条件2：中心点的阈值小于某个阈值</span><br><span class="line">    # center_change = np.linalg.norm(new_centers - centers)</span><br><span class="line">    # if center_change &lt; thresh:</span><br><span class="line">    #     break</span><br><span class="line">    centers = new_centers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  return labels, centers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 生成一些随机数据作为示例输入</span><br><span class="line">data = np.random.rand(100, 2)  # 100个样本，每个样本有两个特征</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 手动实现K均值算法</span><br><span class="line">k = 3  # 聚类数为3</span><br><span class="line">labels, centers = kmeans(data, k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 打印簇标签和聚类中心点</span><br><span class="line">print(&quot;簇标签:&quot;, labels)</span><br><span class="line">print(&quot;聚类中心点:&quot;, centers)</span><br></pre></td></tr></table></figure><h2 id="手撕-Layer-Normalization-算法"><a href="#手撕-Layer-Normalization-算法" class="headerlink" title="手撕 Layer Normalization 算法"></a><strong>手撕 Layer Normalization 算法</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"> </span><br><span class="line">class LN(nn.Module):</span><br><span class="line">    # 初始化</span><br><span class="line">    def __init__(self, normalized_shape,  # 在哪个维度上做LN</span><br><span class="line">                 eps:float = 1e-5, # 防止分母为0</span><br><span class="line">                 elementwise_affine:bool = True):  # 是否使用可学习的缩放因子和偏移因子</span><br><span class="line">        super(LN, self).__init__()</span><br><span class="line">        # 需要对哪个维度的特征做LN, torch.size查看维度</span><br><span class="line">        self.normalized_shape = normalized_shape  # [c,w*h]</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.elementwise_affine = elementwise_affine</span><br><span class="line">        # 构造可训练的缩放因子和偏置</span><br><span class="line">        if self.elementwise_affine:  </span><br><span class="line">            self.gain = nn.Parameter(torch.ones(normalized_shape))  # [c,w*h]</span><br><span class="line">            self.bias = nn.Parameter(torch.zeros(normalized_shape))  # [c,w*h]</span><br><span class="line"> </span><br><span class="line">    # 前向传播</span><br><span class="line">    def forward(self, x: torch.Tensor): # [b,c,w*h]</span><br><span class="line">        # 需要做LN的维度和输入特征图对应维度的shape相同</span><br><span class="line">        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]  # [-2:]</span><br><span class="line">        # 需要做LN的维度索引</span><br><span class="line">        dims = [-(i+1) for i in range(len(self.normalized_shape))]  # [b,c,w*h]维度上取[-1,-2]维度，即[c,w*h]</span><br><span class="line">        # 计算特征图对应维度的均值和方差</span><br><span class="line">        mean = x.mean(dim=dims, keepdims=True)  # [b,1,1]</span><br><span class="line">        mean_x2 = (x**2).mean(dim=dims, keepdims=True)  # [b,1,1]</span><br><span class="line">        var = mean_x2 - mean**2  # [b,c,1,1]</span><br><span class="line">        x_norm = (x-mean) / torch.sqrt(var+self.eps)  # [b,c,w*h]</span><br><span class="line">        # 线性变换</span><br><span class="line">        if self.elementwise_affine:</span><br><span class="line">            x_norm = self.gain * x_norm + self.bias  # [b,c,w*h]</span><br><span class="line">        return x_norm</span><br><span class="line"> </span><br><span class="line"># ------------------------------- #</span><br><span class="line"># 验证</span><br><span class="line"># ------------------------------- #</span><br><span class="line"> </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"> </span><br><span class="line">    x = torch.linspace(0, 23, 24, dtype=torch.float32)  # 构造输入层</span><br><span class="line">    x = x.reshape([2,3,2*2])  # [b,c,w*h]</span><br><span class="line">    # 实例化</span><br><span class="line">    ln = LN(x.shape[1:])</span><br><span class="line">    # 前向传播</span><br><span class="line">    x = ln(x)</span><br><span class="line">    print(x.shape)</span><br></pre></td></tr></table></figure><h3 id="手撕-Batch-Normalization-算法"><a href="#手撕-Batch-Normalization-算法" class="headerlink" title="手撕 Batch Normalization 算法"></a><strong>手撕 Batch Normalization 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class MyBN:</span><br><span class="line">    def __init__(self, momentum=0.01, eps=1e-5, feat_dim=2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        初始化参数值</span><br><span class="line">        :param momentum: 动量，用于计算每个batch均值和方差的滑动均值</span><br><span class="line">        :param eps: 防止分母为0</span><br><span class="line">        :param feat_dim: 特征维度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 均值和方差的滑动均值</span><br><span class="line">        self._running_mean = np.zeros(shape=(feat_dim, ))</span><br><span class="line">        self._running_var = np.ones((shape=(feat_dim, ))</span><br><span class="line">        # 更新self._running_xxx时的动量</span><br><span class="line">        self._momentum = momentum</span><br><span class="line">        # 防止分母计算为0</span><br><span class="line">        self._eps = eps</span><br><span class="line">        # 对应Batch Norm中需要更新的beta和gamma，采用pytorch文档中的初始化值</span><br><span class="line">        self._beta = np.zeros(shape=(feat_dim, ))</span><br><span class="line">        self._gamma = np.ones(shape=(feat_dim, ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def batch_norm(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        BN向传播</span><br><span class="line">        :param x: 数据</span><br><span class="line">        :return: BN输出</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if self.training:</span><br><span class="line">            x_mean = x.mean(axis=0)</span><br><span class="line">            x_var = x.var(axis=0)</span><br><span class="line">            # 对应running_mean的更新公式</span><br><span class="line">            self._running_mean = (1-self._momentum)*x_mean + self._momentum*self._running_mean</span><br><span class="line">            self._running_var = (1-self._momentum)*x_var + self._momentum*self._running_var</span><br><span class="line">            # 对应论文中计算BN的公式</span><br><span class="line">            x_hat = (x-x_mean)/np.sqrt(x_var+self._eps)</span><br><span class="line">        else:</span><br><span class="line">            x_hat = (x-self._running_mean)/np.sqrt(self._running_var+self._eps)</span><br><span class="line">        return self._gamma*x_hat + self._beta</span><br></pre></td></tr></table></figure><h2 id="解码算法篇"><a href="#解码算法篇" class="headerlink" title="解码算法篇"></a><strong>解码算法篇</strong></h2><h3 id="手撕-贪心搜索-（greedy-search）"><a href="#手撕-贪心搜索-（greedy-search）" class="headerlink" title="手撕 贪心搜索 （greedy search）"></a><strong>手撕 贪心搜索 （greedy search）</strong></h3><p>贪心搜索（greedy search）在每个时间步 t 都选取当前概率分布中概率最大的词，即<br><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/1574965/1714410444607-da855304-da72-4fa6-8f9b-fd9cf3440b94.jpeg#clientId=u8884b877-175c-4&from=paste&id=ud89f8e75&originHeight=59&originWidth=273&originalType=url&ratio=1&rotation=0&showTitle=false&size=2641&status=done&style=none&taskId=ub3f3a342-b73d-485d-8964-e14e29f56aa&title=" alt="image.jpg"><br>直到 yt 为或达到预设最大长度时停止生成。<br>贪心搜索本质上是局部最优策略，但并不能保证最终结果一定是全局最优的。由于贪心搜索在解码的任意时刻只保留一条候选序列，所以在搜索效率上，贪心搜索的复杂度显著低于穷举搜索。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def greedy_decoding(input_ids, max_tokens=300):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">            next_token = torch.argmax(next_token_logits, dim=-1)</span><br><span class="line"> if next_token == tokenizer.eos_token_id:</span><br><span class="line"> break</span><br><span class="line">            input_ids = torch.cat([input_ids, rearrange(next_token, &#x27;c -&gt; 1 c&#x27;)], dim=-1)</span><br><span class="line">        generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure><h3 id="手撕-集束搜索-beamsearch-算法"><a href="#手撕-集束搜索-beamsearch-算法" class="headerlink" title="手撕 集束搜索 beamsearch 算法"></a><strong>手撕 集束搜索 beamsearch 算法</strong></h3><p>在NLP翻译或对话任务中，在句子解码阶段，经常用到一种搜索算法beam search。这个算法有时候在大厂面试中，甚至可能会被要求手写实现。这里就从beam search的原理出发，最后手写实现一个beam search。</p><ul><li>思路：beam search在贪心搜索上进一步扩大了搜索范围，贪心搜索每下一步只考虑当前最优的top-1结果，beam search考虑最优的top-k个结果。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def beam_search(LM_prob,beam_size = 3):</span><br><span class="line">    batch,seqlen,vocab_size = LM_prob.shape</span><br><span class="line">    #对LM_prob取对数</span><br><span class="line">    log_LM_prob = LM_prob.log()</span><br><span class="line">    #先选择第0个位置的最大beam_size个token，log_emb_prob与indices的shape为(batch,beam)</span><br><span class="line">    log_beam_prob, indices = log_LM_prob[:,0,:].topk(beam_size,sorted = True)</span><br><span class="line">    indices = indices.unsqueeze(-1)</span><br><span class="line">    #对每个长度进行beam search</span><br><span class="line">    for i in range(1,seqlen):</span><br><span class="line">        #log_beam_prob (batch,beam,vocab_size),每个beam的可能产生的概率</span><br><span class="line">        log_beam_prob = log_beam_prob.unsqueeze(-1) + log_LM_prob[:,i,:].unsqueeze(1).repeat(1,beam_size,1)</span><br><span class="line">        #选择当前步概率最高的token</span><br><span class="line">        log_beam_prob, index = log_beam_prob.view(batch,-1).topk(beam_size,sorted = True)</span><br><span class="line">        #下面的计算：beam_id选出新beam来源于之前的哪个beam;index代表真实的token id</span><br><span class="line">        #beam_id,index (batch,beam)</span><br><span class="line">        beam_id = index//vocab_size</span><br><span class="line">        index = index%vocab_size</span><br><span class="line">        mid = torch.Tensor([])</span><br><span class="line">        #对batch内每个样本循环，选出beam的同时拼接上新生成的token id</span><br><span class="line">        for j,bid,idx in zip(range(batch),beam_id,index):</span><br><span class="line">            x = torch.cat([indices[j][bid],idx.unsqueeze(-1)],-1)</span><br><span class="line">            mid = torch.cat([mid,x.unsqueeze(0)],0)</span><br><span class="line">        indices = mid</span><br><span class="line">    return indices,log_beam_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    # 建立一个语言模型 LM_prob (batch,seqlen,vocab_size)</span><br><span class="line">    LM_prob = F.softmax(torch.randn([32,20,1000]),dim = -1)</span><br><span class="line">    #最终返回每个候选，以及每个候选的log_prob，shape为(batch,beam_size,seqlen)</span><br><span class="line">    indices,log_prob = beam_search(LM_prob,beam_size = 3)</span><br><span class="line">    print(indices)</span><br></pre></td></tr></table></figure><h3 id="手撕-温度参数采样（Temperature-Sampling）算法"><a href="#手撕-温度参数采样（Temperature-Sampling）算法" class="headerlink" title="手撕 温度参数采样（Temperature Sampling）算法"></a><strong>手撕 温度参数采样（Temperature Sampling）算法</strong></h3><p>温度参数采样（Temperature Sampling）常用于基于概率的生成模型，如语言模型。它通过引入一个称为“温度”（Temperature）的参数来调整模型输出的概率分布，从而控制生成文本的多样性。<br>在温度参数采样中，模型在每个时间步生成词语时，会计算出词语的条件概率分布。然后模型将这个条件概率分布中的每个词语的概率值除以温度参数，对结果进行归一化处理，获得新的归一化概率分布。较高的温度值会使概率分布更平滑，从而增加生成文本的多样性。低概率的词语也有较高的可能性被选择；而较低的温度值则会使概率分布更集中，更倾向于选择高概率的词语，因此生成的文本更加确定性。最后模型根据这个新的归一化概率分布进行随机采样，选择生成的词语。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def temperature_sampling(logits, temperature=1.0):</span><br><span class="line">    logits = logits / temperature</span><br><span class="line">    probabilities = F.softmax(logits, dim=-1)</span><br><span class="line">    sampled_token = torch.multinomial(probabilities, 1)</span><br><span class="line"> return sampled_token.item()</span><br></pre></td></tr></table></figure><h3 id="手撕-Top-K-Sampling算法"><a href="#手撕-Top-K-Sampling算法" class="headerlink" title="手撕 Top-K Sampling算法"></a><strong>手撕 Top-K Sampling算法</strong></h3><p>Top-K 采样（在每个时间步选择条件概率排名前 K 的词语，然后在这 K 个词语中进行随机采样。这种方法既能保持一定的生成质量，又能增加文本的多样性，并且可以通过限制候选词语的数量来控制生成文本的多样性。<br>这个过程使得生成的文本在保持一定的生成质量的同时，也具有一定的多样性，因为在候选词语中仍然存在一定的竞争性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def top_k_sampling(input_ids, max_tokens=100, top_k=50, temperature=1.0):</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)</span><br><span class="line">            top_k_probs = F.softmax(top_k_logits / temperature, dim=-1)</span><br><span class="line">            next_token_index = torch.multinomial(top_k_probs, num_samples=1)</span><br><span class="line">            next_token = top_k_indices.gather(-1, next_token_index)</span><br><span class="line">            input_ids = torch.cat([input_ids, next_token], dim=-1)</span><br><span class="line">    generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure><h3 id="手撕-Top-P-Nucleus-Sampling-算法"><a href="#手撕-Top-P-Nucleus-Sampling-算法" class="headerlink" title="手撕 Top-P (Nucleus) Sampling 算法"></a><strong>手撕 Top-P (Nucleus) Sampling 算法</strong></h3><p>Nucleus Sampling（核采样），也被称为Top-p Sampling旨在在保持生成文本质量的同时增加多样性。这种方法可以视作是Top-K Sampling的一种变体，它在每个时间步根据模型输出的概率分布选择概率累积超过给定阈值p的词语集合，然后在这个词语集合中进行随机采样。这种方法会动态调整候选词语的数量，以保持一定的文本多样性。<br>在Nucleus Sampling中，模型在每个时间步生成词语时，首先按照概率从高到低对词汇表中的所有词语进行排序，然后模型计算累积概率，并找到累积概率超过给定阈值p的最小词语子集，这个子集就是所谓的“核”（nucleus）。模型在这个核中进行随机采样，根据词语的概率分布来选择最终输出的词语。这样做可以保证所选词语的总概率超过了阈值p，同时也保持了一定的多样性。<br>参数p是Nucleus Sampling中的重要参数，它决定了所选词语的概率总和。p的值会被设置在(0,1]之间，表示词语总概率的一个下界。<br>Nucleus Sampling 能够保持一定的生成质量，因为它在一定程度上考虑了概率分布。通过选择概率总和超过给定阈值p的词语子集进行随机采样，Nucleus Sampling 能够增加生成文本的多样性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def top_p_sampling(input_ids, max_tokens=100, top_p=0.95):</span><br><span class="line"> with torch.inference_mode():</span><br><span class="line"> for _ in range(max_tokens):</span><br><span class="line">                outputs = model(input_ids)</span><br><span class="line">                next_token_logits = outputs.logits[:, -1, :]</span><br><span class="line">                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)</span><br><span class="line">                sorted_probabilities = F.softmax(sorted_logits, dim=-1) </span><br><span class="line">                cumulative_probs = torch.cumsum(sorted_probabilities, dim=-1)</span><br><span class="line">                sorted_indices_to_remove = cumulative_probs &gt; top_p</span><br><span class="line">                sorted_indices_to_remove[..., 0] = False </span><br><span class="line">                indices_to_remove = sorted_indices[sorted_indices_to_remove]</span><br><span class="line">                next_token_logits.scatter_(-1, indices_to_remove[None, :], float(&#x27;-inf&#x27;))</span><br><span class="line">                probs = F.softmax(next_token_logits, dim=-1)</span><br><span class="line">                next_token = torch.multinomial(probs, num_samples=1)</span><br><span class="line">                input_ids = torch.cat([input_ids, next_token], dim=-1)</span><br><span class="line">        generated_text = tokenizer.decode(input_ids[0])</span><br><span class="line"> return generated_text</span><br></pre></td></tr></table></figure><h2 id="神经网络篇"><a href="#神经网络篇" class="headerlink" title="神经网络篇"></a><strong>神经网络篇</strong></h2><h3 id="手撕反向传播-backward-propagation，BP-法"><a href="#手撕反向传播-backward-propagation，BP-法" class="headerlink" title="手撕反向传播(backward propagation，BP)法"></a><strong>手撕反向传播(backward propagation，BP)法</strong></h3><p>BP算法就是反向传播，要输入的数据经过一个前向传播会得到一个输出，但是由于权重的原因，所以其输出会和你想要的输出有差距，这个时候就需要进行反向传播，利用梯度下降，对所有的权重进行更新，这样的话在进行前向传播就会发现其输出和你想要的输出越来越接近了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"># 生成权重以及偏执项layers_dim代表每层的神经元个数，</span><br><span class="line">#比如[2,3,1]代表一个三成的网络，输入为2层，中间为3层输出为1层</span><br><span class="line">def init_parameters(layers_dim):</span><br><span class="line">    </span><br><span class="line">    L = len(layers_dim)</span><br><span class="line">    parameters =&#123;&#125;</span><br><span class="line">    for i in range(1,L):</span><br><span class="line">        parameters[&quot;w&quot;+str(i)] = np.random.random([layers_dim[i],layers_dim[i-1]])</span><br><span class="line">        parameters[&quot;b&quot;+str(i)] = np.zeros((layers_dim[i],1))</span><br><span class="line">    return parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1.0/(1.0+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># sigmoid的导函数</span><br><span class="line">def sigmoid_prime(z):</span><br><span class="line">        return sigmoid(z) * (1-sigmoid(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 前向传播，需要用到一个输入x以及所有的权重以及偏执项，都在parameters这个字典里面存储</span><br><span class="line"># 最后返回会返回一个caches里面包含的 是各层的a和z，a[layers]就是最终的输出</span><br><span class="line">def forward(x,parameters):</span><br><span class="line">    a = []</span><br><span class="line">    z = []</span><br><span class="line">    caches = &#123;&#125;</span><br><span class="line">    a.append(x)</span><br><span class="line">    z.append(x)</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    # 前面都要用sigmoid</span><br><span class="line">    for i in range(1,layers):</span><br><span class="line">        z_temp =parameters[&quot;w&quot;+str(i)].dot(x) + parameters[&quot;b&quot;+str(i)]</span><br><span class="line">        z.append(z_temp)</span><br><span class="line">        a.append(sigmoid(z_temp))</span><br><span class="line">    # 最后一层不用sigmoid</span><br><span class="line">    z_temp = parameters[&quot;w&quot;+str(layers)].dot(a[layers-1]) + parameters[&quot;b&quot;+str(layers)]</span><br><span class="line">    z.append(z_temp)</span><br><span class="line">    a.append(z_temp)</span><br><span class="line">    </span><br><span class="line">    caches[&quot;z&quot;] = z</span><br><span class="line">    caches[&quot;a&quot;] = a    </span><br><span class="line">    return  caches,a[layers]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 反向传播，parameters里面存储的是所有的各层的权重以及偏执，caches里面存储各层的a和z</span><br><span class="line"># al是经过反向传播后最后一层的输出，y代表真实值</span><br><span class="line"># 返回的grades代表着误差对所有的w以及b的导数</span><br><span class="line">def backward(parameters,caches,al,y):</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    grades = &#123;&#125;</span><br><span class="line">    m = y.shape[1]</span><br><span class="line">    # 假设最后一层不经历激活函数</span><br><span class="line">    # 就是按照上面的图片中的公式写的</span><br><span class="line">    grades[&quot;dz&quot;+str(layers)] = al - y</span><br><span class="line">    grades[&quot;dw&quot;+str(layers)] = grades[&quot;dz&quot;+str(layers)].dot(caches[&quot;a&quot;][layers-1].T) /m</span><br><span class="line">    grades[&quot;db&quot;+str(layers)] = np.sum(grades[&quot;dz&quot;+str(layers)],axis = 1,keepdims = True) /m</span><br><span class="line">    # 前面全部都是sigmoid激活</span><br><span class="line">    for i in reversed(range(1,layers)):</span><br><span class="line">        grades[&quot;dz&quot;+str(i)] = parameters[&quot;w&quot;+str(i+1)].T.dot(grades[&quot;dz&quot;+str(i+1)]) * sigmoid_prime(caches[&quot;z&quot;][i])</span><br><span class="line">        grades[&quot;dw&quot;+str(i)] = grades[&quot;dz&quot;+str(i)].dot(caches[&quot;a&quot;][i-1].T)/m</span><br><span class="line">        grades[&quot;db&quot;+str(i)] = np.sum(grades[&quot;dz&quot;+str(i)],axis = 1,keepdims = True) /m</span><br><span class="line">    return grades   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 就是把其所有的权重以及偏执都更新一下</span><br><span class="line">def update_grades(parameters,grades,learning_rate):</span><br><span class="line">    layers = len(parameters)//2</span><br><span class="line">    for i in range(1,layers+1):</span><br><span class="line">        parameters[&quot;w&quot;+str(i)] -= learning_rate * grades[&quot;dw&quot;+str(i)]</span><br><span class="line">        parameters[&quot;b&quot;+str(i)] -= learning_rate * grades[&quot;db&quot;+str(i)]</span><br><span class="line">    return parameters</span><br><span class="line"># 计算误差值</span><br><span class="line">def compute_loss(al,y):</span><br><span class="line">    return np.mean(np.square(al-y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载数据集</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = np.arange(0.0,1.0,0.01)</span><br><span class="line">    y =20* np.sin(2*np.pi*x)</span><br><span class="line">    # 数据可视化</span><br><span class="line">    plt.scatter(x,y)</span><br><span class="line">    return x,y</span><br><span class="line">#进行测试</span><br><span class="line">x,y = load_data()</span><br><span class="line">x = x.reshape(1,100)</span><br><span class="line">y = y.reshape(1,100)</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">parameters = init_parameters([1,25,1])</span><br><span class="line">al = 0</span><br><span class="line">for i in range(4000):</span><br><span class="line">    caches,al = forward(x, parameters)</span><br><span class="line">    grades = backward(parameters, caches, al, y)</span><br><span class="line">    parameters = update_grades(parameters, grades, learning_rate= 0.3)</span><br><span class="line">    if i %100 ==0:</span><br><span class="line">        print(compute_loss(al, y))</span><br><span class="line">plt.scatter(x,al)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="手撕-卷积神经网络-CNN-法"><a href="#手撕-卷积神经网络-CNN-法" class="headerlink" title="手撕 卷积神经网络(CNN)法"></a><strong>手撕 卷积神经网络(CNN)法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F #使用functional中的ReLu激活函数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#CNN模型</span><br><span class="line">class CNNNet(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CNNNet, self).__init__()</span><br><span class="line">        #两个卷积层</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)  #1为in_channels 10为out_channels</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)</span><br><span class="line">        #池化层</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(2)  #2为分组大小2*2</span><br><span class="line">        #全连接层 320 = 20 * 4 * 4</span><br><span class="line">        self.fc = torch.nn.Linear(320, 10)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        #先从x数据维度中得到batch_size</span><br><span class="line">        batch_size = x.size(0)</span><br><span class="line">        #卷积层-&gt;池化层-&gt;激活函数</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -1)  #将数据展开，为输入全连接层做准备</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        return x</span><br><span class="line">model = CNNNet()</span><br></pre></td></tr></table></figure><h3 id="手撕-循环神经网络-RNN-法"><a href="#手撕-循环神经网络-RNN-法" class="headerlink" title="手撕 循环神经网络(RNN)法"></a><strong>手撕 循环神经网络(RNN)法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># encoding:utf-8</span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义RNN模型(可以类别下方RNN简单测试代码理解)</span><br><span class="line">class Rnn(nn.Module):</span><br><span class="line">    def __init__(self, input_size):</span><br><span class="line">        super(Rnn, self).__init__()</span><br><span class="line">        # 定义RNN网络</span><br><span class="line">        ## hidden_size是自己设置的，貌似取值都是32,64,128这样来取值</span><br><span class="line">        ## num_layers是隐藏层数量，超过2层那就是深度循环神经网络了</span><br><span class="line">        self.rnn = nn.RNN(</span><br><span class="line">                input_size=input_size,</span><br><span class="line">                hidden_size=32,</span><br><span class="line">                num_layers=1,</span><br><span class="line">                batch_first=True  # 输入形状为[批量大小, 数据序列长度, 特征维度]</span><br><span class="line">                )</span><br><span class="line">        # 定义全连接层</span><br><span class="line">        self.out = nn.Linear(32, 1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 定义前向传播函数</span><br><span class="line">    def forward(self, x, h_0):</span><br><span class="line">        r_out, h_n = self.rnn(x, h_0)</span><br><span class="line">        # print(&quot;数据输出结果；隐藏层数据结果&quot;, r_out, h_n)</span><br><span class="line">        # print(&quot;r_out.size()， h_n.size()&quot;, r_out.size(), h_n.size())</span><br><span class="line">        outs = []</span><br><span class="line">        # r_out.size=[1,10,32]即将一个长度为10的序列的每个元素都映射到隐藏层上</span><br><span class="line">        for time in range(r_out.size(1)):  </span><br><span class="line">            # print(&quot;映射&quot;, r_out[:, time, :])</span><br><span class="line">            # 依次抽取序列中每个单词,将之通过全连接层并输出.r_out[:, 0, :].size()=[1,32] -&gt; [1,1]</span><br><span class="line">            outs.append(self.out(r_out[:, time, :])) </span><br><span class="line">            # print(&quot;outs&quot;, outs)</span><br><span class="line">        # stack函数在dim=1上叠加:10*[1,1] -&gt; [1,10,1] 同时h_n已经被更新</span><br><span class="line">        return torch.stack(outs, dim=1), h_n </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TIME_STEP = 10</span><br><span class="line">INPUT_SIZE = 1</span><br><span class="line">LR = 0.02</span><br><span class="line">model = Rnn(INPUT_SIZE)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><h3 id="手撕-LSTM法"><a href="#手撕-LSTM法" class="headerlink" title="手撕 LSTM法"></a><strong>手撕 LSTM法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Define LSTM Neural Networks</span><br><span class="line">class LstmRNN(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">        Parameters：</span><br><span class="line">        - input_size: feature size</span><br><span class="line">        - hidden_size: number of hidden units</span><br><span class="line">        - output_size: number of output</span><br><span class="line">        - num_layers: layers of LSTM to stack</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, input_size, hidden_size=1, output_size=1, num_layers=1):</span><br><span class="line">        super().__init__()</span><br><span class="line"> </span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers) # utilize the LSTM model in torch.nn </span><br><span class="line">        self.forwardCalculation = nn.Linear(hidden_size, output_size)</span><br><span class="line"> </span><br><span class="line">    def forward(self, _x):</span><br><span class="line">        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)</span><br><span class="line">        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)</span><br><span class="line">        x = x.view(s*b, h)</span><br><span class="line">        x = self.forwardCalculation(x)</span><br><span class="line">        x = x.view(s, b, -1)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="手撕-二维卷积-算法"><a href="#手撕-二维卷积-算法" class="headerlink" title="手撕 二维卷积 算法"></a><strong>手撕 二维卷积 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np </span><br><span class="line">def conv2d(img, in_channels, out_channels ,kernels, bias, stride=1, padding=0):</span><br><span class="line">    N, C, H, W = img.shape </span><br><span class="line">    kh, kw = kernels.shape</span><br><span class="line">    p = padding</span><br><span class="line">    assert C == in_channels, &quot;kernels&#x27; input channels do not match with img&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if p:</span><br><span class="line">        img = np.pad(img, ((0,0),(0,0),(p,p),(p,p)), &#x27;constant&#x27;) # padding along with all axis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    out_h = (H + 2*padding - kh) // stride + 1</span><br><span class="line">    out_w = (W + 2*padding - kw) // stride + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    outputs = np.zeros([N, out_channels, out_h, out_w])</span><br><span class="line">    # print(img)</span><br><span class="line">    for n in range(N):</span><br><span class="line">        for out in range(out_channels):</span><br><span class="line">            for i in range(in_channels):</span><br><span class="line">                for h in range(out_h):</span><br><span class="line">                    for w in range(out_w):</span><br><span class="line">                        for x in range(kh):</span><br><span class="line">                            for y in range(kw):</span><br><span class="line">                                outputs[n][out][h][w] += img[n][i][h * stride + x][w * stride + y] * kernels[x][y]</span><br><span class="line">                if i == in_channels - 1:</span><br><span class="line">                    outputs[n][out][:][:] += bias[n][out]</span><br><span class="line">    return outputs</span><br></pre></td></tr></table></figure><h2 id="位置编码篇"><a href="#位置编码篇" class="headerlink" title="位置编码篇"></a><strong>位置编码篇</strong></h2><h3 id="手撕-绝对位置编码-算法"><a href="#手撕-绝对位置编码-算法" class="headerlink" title="手撕 绝对位置编码 算法"></a><strong>手撕 绝对位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class SinPositionEncoding(nn.Module):</span><br><span class="line">    def __init__(self, max_sequence_length, d_model, base=10000):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.max_sequence_length = max_sequence_length</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.base = base</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self):</span><br><span class="line">        pe = torch.zeros(self.max_sequence_length, self.d_model, dtype=torch.float)  # size(max_sequence_length, d_model)</span><br><span class="line">        exp_1 = torch.arange(self.d_model // 2, dtype=torch.float)  # 初始化一半维度，sin位置编码的维度被分为了两部分</span><br><span class="line">        exp_value = exp_1 / (self.d_model / 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        alpha = 1 / (self.base ** exp_value)  # size(dmodel/2)</span><br><span class="line">        out = torch.arange(self.max_sequence_length, dtype=torch.float)[:, None] @ alpha[None, :]  # size(max_sequence_length, d_model/2)</span><br><span class="line">        embedding_sin = torch.sin(out)</span><br><span class="line">        embedding_cos = torch.cos(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        pe[:, 0::2] = embedding_sin  # 奇数位置设置为sin</span><br><span class="line">        pe[:, 1::2] = embedding_cos  # 偶数位置设置为cos</span><br><span class="line">        return pe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SinPositionEncoding(d_model=4, max_sequence_length=10, base=10000).forward()</span><br></pre></td></tr></table></figure><h3 id="手撕-可学习位置编码-算法"><a href="#手撕-可学习位置编码-算法" class="headerlink" title="手撕 可学习位置编码 算法"></a><strong>手撕 可学习位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class TrainablePositionEncoding(nn.Module):</span><br><span class="line">    def __init__(self, max_sequence_length, d_model):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.max_sequence_length = max_sequence_length</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self):</span><br><span class="line">        pe = nn.Embedding(self.max_sequence_length, self.d_model)</span><br><span class="line">        nn.init.constant(pe.weight, 0.)</span><br><span class="line">        return pe</span><br></pre></td></tr></table></figure><h3 id="手撕-相对位置编码-算法"><a href="#手撕-相对位置编码-算法" class="headerlink" title="手撕 相对位置编码 算法"></a><strong>手撕 相对位置编码 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">class RelativePosition(nn.Module):</span><br><span class="line">    def __init__(self, num_units, max_relative_position):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_units = num_units</span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))</span><br><span class="line">        nn.init.xavier_uniform_(self.embeddings_table)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, length_q, length_k):</span><br><span class="line">        range_vec_q = torch.arange(length_q)</span><br><span class="line">        range_vec_k = torch.arange(length_k)</span><br><span class="line">        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]</span><br><span class="line">        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        final_mat = distance_mat_clipped + self.max_relative_position</span><br><span class="line">        final_mat = torch.LongTensor(final_mat).cuda()</span><br><span class="line">        embeddings = self.embeddings_table[final_mat].cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RelativeMultiHeadAttention(nn.Module):</span><br><span class="line">    def __init__(self, d_model, n_heads, dropout=0.1, batch_size=6):</span><br><span class="line">        &quot;Take in model size and number of heads.&quot;</span><br><span class="line">        super(RelativeMultiHeadAttention, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        assert d_model % n_heads == 0</span><br><span class="line">        self.head_dim = d_model // n_heads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.linears = _get_clones(nn.Linear(d_model, d_model), 4)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.relative_position_k = RelativePosition(self.head_dim, max_relative_position=16)</span><br><span class="line">        self.relative_position_v = RelativePosition(self.head_dim, max_relative_position=16)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value):</span><br><span class="line">        # embedding</span><br><span class="line">        # query, key, value = [batch_size, len, hid_dim]</span><br><span class="line">        query, key, value = [l(x).view(self.batch_size, -1, self.d_model) for l, x in</span><br><span class="line">                             zip(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        len_k = query.shape[1]</span><br><span class="line">        len_q = query.shape[1]</span><br><span class="line">        len_v = value.shape[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        # Self-Attention</span><br><span class="line">        # r_q1, r_k1 = [batch_size, len, n_heads, head_dim]</span><br><span class="line">        r_q1 = query.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        r_k1 = key.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, self.batch_size * self.n_heads, self.head_dim)</span><br><span class="line">        r_k2 = self.relative_position_k(len_q, len_k)</span><br><span class="line">        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)</span><br><span class="line">        attn2 = attn2.contiguous().view(self.batch_size, self.n_heads, len_q, len_k)</span><br><span class="line">        attn = (attn1 + attn2) / self.scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        attn = self.dropout(torch.softmax(attn, dim=-1))</span><br><span class="line">        # attn = [batch_size, n_heads, len, len]</span><br><span class="line">        r_v1 = value.view(self.batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        weight1 = torch.matmul(attn, r_v1)</span><br><span class="line">        r_v2 = self.relative_position_v(len_q, len_v)</span><br><span class="line">        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, self.batch_size * self.n_heads, len_k)</span><br><span class="line">        weight2 = torch.matmul(weight2, r_v2)</span><br><span class="line">        weight2 = weight2.transpose(0, 1).contiguous().view(self.batch_size, self.n_heads, len_q, self.head_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = weight1 + weight2</span><br><span class="line">        # x = [batch size, n heads, query len, head dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        # x = [batch size, query len, n heads, head dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.view(self.batch_size * len_q, self.d_model)</span><br><span class="line">        # x = [batch size * query len, hid dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return self.linears[-1](x)</span><br></pre></td></tr></table></figure><h3 id="手撕-rope-算法"><a href="#手撕-rope-算法" class="headerlink" title="手撕 rope 算法"></a><strong>手撕 rope 算法</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):</span><br><span class="line">    # (max_len, 1)</span><br><span class="line">    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)</span><br><span class="line">    # (output_dim//2)</span><br><span class="line">    ids = torch.arange(0, output_dim // 2, dtype=torch.float)  # 即公式里的i, i的范围是 [0,d/2]</span><br><span class="line">    theta = torch.pow(10000, -2 * ids / output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (max_len, output_dim//2)</span><br><span class="line">    embeddings = position * theta  # 即公式里的：pos / (10000^(2i/d))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (max_len, output_dim//2, 2)</span><br><span class="line">    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim//2, 2)</span><br><span class="line">    embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))  # 在bs维度重复，其他维度都是1不重复</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim)</span><br><span class="line">    # reshape后就是：偶数sin, 奇数cos了</span><br><span class="line">    embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))</span><br><span class="line">    embeddings = embeddings.to(device)</span><br><span class="line">    return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line">def RoPE(q, k):</span><br><span class="line">    # q,k: (bs, head, max_len, output_dim)</span><br><span class="line">    batch_size = q.shape[0]</span><br><span class="line">    nums_head = q.shape[1]</span><br><span class="line">    max_len = q.shape[2]</span><br><span class="line">    output_dim = q.shape[-1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, max_len, output_dim)</span><br><span class="line">    pos_emb = sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, q.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # cos_pos,sin_pos: (bs, head, max_len, output_dim)</span><br><span class="line">    # 看rope公式可知，相邻cos，sin之间是相同的，所以复制一遍。如(1,2,3)变成(1,1,2,2,3,3)</span><br><span class="line">    cos_pos = pos_emb[...,  1::2].repeat_interleave(2, dim=-1)  # 将奇数列信息抽取出来也就是cos 拿出来并复制</span><br><span class="line">    sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)  # 将偶数列信息抽取出来也就是sin 拿出来并复制</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # q,k: (bs, head, max_len, output_dim)</span><br><span class="line">    q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)</span><br><span class="line">    q2 = q2.reshape(q.shape)  # reshape后就是正负交替了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 更新qw, *对应位置相乘</span><br><span class="line">    q = q * cos_pos + q2 * sin_pos</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)</span><br><span class="line">    k2 = k2.reshape(k.shape)</span><br><span class="line">    # 更新kw, *对应位置相乘</span><br><span class="line">    k = k * cos_pos + k2 * sin_pos</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return q, k</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># %%</span><br><span class="line">def attention(q, k, v, mask=None, dropout=None, use_RoPE=True):</span><br><span class="line">    # q.shape: (bs, head, seq_len, dk)</span><br><span class="line">    # k.shape: (bs, head, seq_len, dk)</span><br><span class="line">    # v.shape: (bs, head, seq_len, dk)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if use_RoPE:</span><br><span class="line">        q, k = RoPE(q, k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d_k = k.size()[-1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    att_logits = torch.matmul(q, k.transpose(-2, -1))  # (bs, head, seq_len, seq_len)</span><br><span class="line">    att_logits /= math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if mask is not None:</span><br><span class="line">        att_logits = att_logits.masked_fill(mask == 0, -1e9)  # mask掉为0的部分，设为无穷大</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    att_scores = F.softmax(att_logits, dim=-1)  # (bs, head, seq_len, seq_len)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if dropout is not None:</span><br><span class="line">        att_scores = dropout(att_scores)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, seq_len, seq_len) * (bs, head, seq_len, dk) = (bs, head, seq_len, dk)</span><br><span class="line">    return torch.matmul(att_scores, v), att_scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # (bs, head, seq_len, dk)</span><br><span class="line">    q = torch.randn((8, 12, 10, 32))</span><br><span class="line">    k = torch.randn((8, 12, 10, 32))</span><br><span class="line">    v = torch.randn((8, 12, 10, 32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    res, att_scores = attention(q, k, v, mask=None, dropout=None, use_RoPE=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # (bs, head, seq_len, dk),  (bs, head, seq_len, seq_len)</span><br></pre></td></tr></table></figure><pre><code>print(res.shape, att_scores.shape)</code></pre><h2 id="面试题汇总"><a href="#面试题汇总" class="headerlink" title="面试题汇总"></a><strong>面试题汇总</strong></h2><ul><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483957&idx=1&sn=abec4b75b9865b754f8a303c340c13a3&scene=21#wechat_redirect">大模型微调的经验与感想分享</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483942&idx=1&sn=a5ba1da8459df0b76e1ea70bfa4dc068&scene=21#wechat_redirect">百度-NLP算法工程师面经</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483919&idx=1&sn=c9a530ecce9e60af4fad4c06062ec9ce&scene=21#wechat_redirect">美团-大模型算法工程师面经</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483896&idx=1&sn=6b79f7eb585cc1d91a1f61010941477c&scene=21#wechat_redirect">小米-NLP算法工程师面试题</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483884&idx=1&sn=e1f4d13589606786f2d2467e11b4e2dc&scene=21#wechat_redirect">好未来-NLP算法工程师面经</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483862&idx=1&sn=0dc0ee080532d397b2b00bdd20c86260&scene=21#wechat_redirect">百度大模型算法工程师面经</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483853&idx=2&sn=f717767538329ce17325de72aa58ba1b&scene=21#wechat_redirect">昆仑天工大模型算法工程师</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483839&idx=1&sn=b66447f92f4dbfa8be7922f53aa8ba4b&scene=21#wechat_redirect">阿里大模型算法工程师一面</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483790&idx=1&sn=308fb18b66cc66b78f7e15822cdd6eff&scene=21#wechat_redirect">算法工程师面试常考手撕题</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483773&idx=1&sn=003c347fc05e1a3fa4328ac09dddb797&scene=21#wechat_redirect">搜狐大模型算法工程师</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483757&idx=1&sn=79394fd14e39948d1fc98aa09e031561&scene=21#wechat_redirect">字节大模型算法实习生</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483750&idx=1&sn=18d9c270e8d58a32dc4792fbc5f8f6e8&scene=21#wechat_redirect">理想汽车大模型算法实习生</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483745&idx=1&sn=ee37c895b25bf2a1f8387edf1d687e30&scene=21#wechat_redirect">百度大模型算法实习生面试题</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483731&idx=1&sn=08cb4b390e80f3ca4a1e0fa2dd5a3020&scene=21#wechat_redirect">腾讯大模型算法实习生面试题</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483723&idx=1&sn=baa9b82a7ac4f12e936ff8b58dcf8977&scene=21#wechat_redirect">阿里大模型算法工程师一面</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483713&idx=1&sn=c90af03630f92999eed214d5dc9f06a3&scene=21#wechat_redirect">某大厂大模型算法工程师面试题</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483697&idx=1&sn=82e8cbb46aa2a0a656ae6f76ed225b03&scene=21#wechat_redirect">说说百度大模型算法工程师二面经历</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247483686&idx=1&sn=79b3d0eb8a034cf7fe8746cd5e362899&scene=21#wechat_redirect">阿里大模型算法工程师面试小结</a></li></ul><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a><strong>致谢</strong></h2><ul><li>LLMs 千面郎君 更新版 <a href="https://mp.weixin.qq.com/s/C6NdO_Ebj3DQx2AVAAgQRQ">https://mp.weixin.qq.com/s/C6NdO_Ebj3DQx2AVAAgQRQ</a></li><li>LLMs九层妖塔 <a href="https://mp.weixin.qq.com/s/Eh0tY1zx2FqXQqIGa2dIBA">https://mp.weixin.qq.com/s/Eh0tY1zx2FqXQqIGa2dIBA</a></li><li>NLP 面无不过 <a href="https://github.com/km1994/NLP-Interview-Notes">https://github.com/km1994/NLP-Interview-Notes</a></li></ul><blockquote><p>来自: <a href="https://mp.weixin.qq.com/s/TAFvUlqdyqP-W6C10F1Hzw">算法工程师面试常考手撕题（更新）</a></p></blockquote>]]></content>
    
    
    <summary type="html">淘天技术的面试记录分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>华为暑期实习面试分享</title>
    <link href="http://example.com/2024/04/27/%E5%8D%8E%E4%B8%BA%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/"/>
    <id>http://example.com/2024/04/27/%E5%8D%8E%E4%B8%BA%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%88%86%E4%BA%AB/</id>
    <published>2024-04-27T23:28:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<p>这个稿子是我面试完回忆用语音转文字记录的，然后我稍微整理了一下，然后就是这个样子。</p><h2 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h2><p>面试首先面试让我自我介绍然后简单介绍一下学校以及参加的项目。简单介绍完，然后就开始问题项目的情况。我说做的项目有一些。主要是研究方向是医学影像，然后前面做的视觉项目，然后我还有一个论文，然后论文做的啥。然后我说我参加了一些竞赛，本科参加的数学建模和数学竞赛。然后问我在项目中遇到了什么问题，然后怎么解决的然后我举了个在实际项目中解决问题的方法。</p><p>然后我说我个人比较喜欢学习新技术，各方面都了解了一些。其他简单问了一点，具体忘了。然后面试就让写个题，这个题就是类似131切割回文字符串，用最小的切割次数是分割字符串，每一个部分都是回文字符串。然后我煎熬了半天写不出来，只能跟面试官说这是一个动态规划问题。然后问我刷了多少题，我说我最近两个月才开始刷，才是力扣刷了100道。然后我说我数据结构算法是大二学的，现在才刚学，不是特别清楚。</p><p>机器学习相对更清楚一点，然后就开始问我图像处理算法。然后问我的边缘检测用什么算子，然后我忘了是叫什么名字，然后简单画了一个图，但是这个9×9的这个表格中间填啥我给忘了。然后又问我在项目中是用到了机器学习算法吗？然后我说一般图像预处理用机器学习算法，实际做一般用深度学习算法。然后往往都是用一些腐蚀膨胀等这些操作滤波。然后。面试官问，对于边缘检测比较模糊和清晰的分别用什么算子，然后面试官最后说让等待通知下一次面试。</p><h2 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h2><p>然后没几十分钟就到主管面试，主管面试问的非常多而且杂，我简单记一下吧。首先。是让自我介绍一下，介绍完。然后说我知识面相对比较广，面试官问，我就是新知识从哪儿学习，首先关注的公众号，我说就是一些特别前沿的技术的公众号，然后具体问我都是哪些公众号，然后我列举了几个，然后其他还有什么渠道，然后我说在哔哩哔哩上关注了一些博主，然后让我列举，然后我列举了几个，然后问我具体哪个博主印象深刻，然后我举了个哔哩哔哩上的zomi酱，然后说他做ai系统，全栈从底层硬件生态到大语言模型训练以及加速等等有很多很多，感觉学到了很多。然后除此之外还有哪些方面，我说我和朋友一起，他们也比较关注前沿技术，所以平时也会进行一些分享交流。</p><p>然后问我的项目在做的过程中有没有遇到什么问题和困难然后如何解决的，然后我就某一个问题，然后进行了稍微详细的回答，然后我说问题主要体现在技术方面和与人打交道方面，然后我说技术方面如何如何解决的，然后他又问，在与人打交道的过程中如何如何解决的，当和甲方遇到争议的时候，不配合的时候如何如何解决的，这个就是比较套路的准备的问题，然后这块说了一些。</p><p>然后问项目是自己做还是和其他人一起？然后这几个项目都是自己负责的，然后问写作这块的问题，我说只有一个项目，是我带了一个研一的，主要在做，然后问如何分配工作，然后我具体说我是做算法设计，然后给他说具体应该如何如何做，让他自己去查具体的内容，然后他来处理数据。然后问有没有参与别人的项目，说主要这块比较大的项目不是很多。</p><p>然后问在团队协作中有没有什么特别坚持的地方？当与其他人出现分歧的时候，如何解决。</p><p>然后还有后面问到了对华为了解多少，对于公司的狼性文化了解多少，我说。我说年轻就应该奋斗，华为狼性文化是给奋斗者的，其他有些公司向华为学习，但是只只学习内卷，不给钱就不好。然后面试官讲了好多好多关于华为的企业文化的奋斗者什么什么之类的内容，如何为公司创造效益。</p><p>然后问如何看待压力，然后有没有遇到什么有压力的情况，然后我说之前换方向的时候感觉压力很大，然后面试官问面对压力如何解决的，然后我说平时可以锻炼，然后周末找朋友一起玩儿，然后前面遇到压力的困难的时候我也会积极向其他人学习，去找清楚压力的来源，然后解决他。</p><p>然后我有没有参加什么社团，然后我说我本科的时候加入了一个技术社团，然后在这个过程中和很多人进行技术交流学习，然后面试官问，我在这个过程中学到了什么，然后我说一分半是学到了技术，另外一方面是交到了很多好朋友。然后面试官我参加活动的频率，我说我开始闲的时候参加的多，后面我们也举办了一些活动，再后面大三忙起来了就不怎么参加了。</p><p>然后是问我如何看待aigc对人类以及对公司的影响，我说一方面提升生产力，使得低端产业更多的被替代，类似一场工业革命，然后面试官说对公司是什么样的，然后重点说了下对于公司的影响，这块我感觉我说的不太好，这个挺宏观的，对公司的没怎么考虑过。</p><p>最后是反问问我有没有什么问题向他问，我说在学校学的东西，和实际实习过程中，是不是有一些差距？主要体现在什么地方？然后我有什么需要准备的没有？然后他讲了一些讲了不少东西，然后说在学校做的东西很多时候相对偏长远更偏理论一些，但是企业相对来说更重效益，既要看重长远地也要看中短期短期利益，如果一段时间内都没有的话，可能这个项目就要被砍掉。然后我还问了问他们的平时具体业务是做什么，然后他就是说也开发这块，包括很多技术，这些前沿算法，把他们做成软件落地。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总之一面技术面问的时间相对比较短，也不是特别深，然后就半小时，后面半小时我都在写那道算法题，然后也没写出来。然后主管面面的时间还挺长的，然后问了好多。我现在暂时只能记得着这么多，其他应该还有一些问题。面试官说感觉还行，但是我不知道他是不是跟其他人也是这样说的，毕竟华为的池子还是挺大的，好多人都在泡池子。</p><p>6.7日oc</p><p>后续还是得好好刷题，做题这块还是差点意思。</p>]]></content>
    
    
    <summary type="html">华为一面二面的面试记录分享。</summary>
    
    
    
    <category term="面试记录" scheme="http://example.com/categories/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="面试记录" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客配置Github Actions和仓库分支存储实现自动化编译部署</title>
    <link href="http://example.com/2024/03/03/Hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEGithub%20Actions%E5%92%8C%E4%BB%93%E5%BA%93%E5%88%86%E6%94%AF%E5%AD%98%E5%82%A8%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2/"/>
    <id>http://example.com/2024/03/03/Hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEGithub%20Actions%E5%92%8C%E4%BB%93%E5%BA%93%E5%88%86%E6%94%AF%E5%AD%98%E5%82%A8%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91%E9%83%A8%E7%BD%B2/</id>
    <published>2024-03-03T02:19:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo博客配置Github-Actions和仓库分支存储实现自动化编译部署"><a href="#Hexo博客配置Github-Actions和仓库分支存储实现自动化编译部署" class="headerlink" title="Hexo博客配置Github Actions和仓库分支存储实现自动化编译部署"></a>Hexo博客配置Github Actions和仓库分支存储实现自动化编译部署</h1><p>这次的这个自动化其实是解决了一个大的问题，之前不带自动化的处理太麻烦了，而且加上之前也没配置好图床，导致每次写博客不仅要准备很久的材料，还要一张一张的上传照片获取链接，然后源文件建了一个github仓库，然后一个仓库只用作github pages，这样的话，每次写博客都要手动编译，使用<code>hexo g</code>然后<code>hexo d</code>，部署，最后使用另外一个仓库提交更改，然后commit，push到github pages仓库，这样很麻烦，所以我就想到了使用github actions自动部署博客，然后使用仓库分支存储图片等资源。（虽然不怎么写博客）<br>相信以后写文章方便之后一定能经常写（大概）。<br>这块我参考了之前浪潮的一次技术讲座，不过那个稍微麻烦了一些，我这里就简化了一下，做了个升级版。</p><h2 id="1-把源文件和github-pages的文件分开"><a href="#1-把源文件和github-pages的文件分开" class="headerlink" title="1.把源文件和github pages的文件分开"></a>1.把源文件和github pages的文件分开</h2><p>source文件和在github上编译好的github pages还是要区分开，这里选择的还是原来github pages的仓库，clone到本地后</p><p>随后建立静态界面的分支，同一个Bash窗口，键入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> 你的Github用户名.github.io <span class="comment"># 进入博客仓库文件夹</span></span><br><span class="line">git branch html <span class="comment"># 新建静态页面分支，存放生成的博客页面</span></span><br></pre></td></tr></table></figure><h2 id="2-main分支文件修改"><a href="#2-main分支文件修改" class="headerlink" title="2. main分支文件修改"></a>2. main分支文件修改</h2><p>首先将你的仓库文件夹清空。</p><p>注：所有清空操作建议在Git Bash窗口中进行，键入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -f * -r <span class="comment"># 强制递归清空仓库文件夹</span></span><br></pre></td></tr></table></figure><p>这样不会将.git&#x2F;文件夹中的仓库记录(这里此文件夹作为隐藏文件没有显示)删除，否则后续Git无法定位，也就无法继续操作<br>随后将之前博客的文件夹中的所有文件复制到这个仓库文件夹中，注意不要复制.git&#x2F;文件夹，因为这是仓库记录，复制后Git无法定位，也就无法继续操作；另外node_modules&#x2F;文件夹也不需要复制，因为这是node.js的依赖包，不需要上传到仓库中，不然可能会报错。</p><h2 id="3-推送main分支更改"><a href="#3-推送main分支更改" class="headerlink" title="3. 推送main分支更改"></a>3. 推送main分支更改</h2><p>回到仓库文件夹下的Git Bash窗口，输入:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add . <span class="comment"># 添加所有文件</span></span><br><span class="line">git commit -m <span class="string">&quot;update branch main&quot;</span> <span class="comment"># 提交更改</span></span><br><span class="line">git push <span class="comment"># 将修改推送到远程仓库</span></span><br></pre></td></tr></table></figure><h2 id="4-配置GiHub-Actions工作流文件"><a href="#4-配置GiHub-Actions工作流文件" class="headerlink" title="4. 配置GiHub Actions工作流文件"></a>4. 配置GiHub Actions工作流文件</h2><p>在仓库文件夹.github&#x2F;下新建一个目录workflows&#x2F;(注意有两层目录)，在里面新建一个hexo_build_deploy.yml文件，内容如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">Build</span> <span class="string">&amp;</span> <span class="string">Deploy</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line"><span class="comment"># 触发事件</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="comment"># 排除分支</span></span><br><span class="line">    <span class="attr">branches-ignore:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;html&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 工作流</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">branch</span> <span class="string">main</span></span><br><span class="line">          <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">          <span class="attr">with:</span></span><br><span class="line">            <span class="attr">ref:</span> <span class="string">main</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 工具安装</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Use</span> <span class="string">Node.js</span></span><br><span class="line">          <span class="attr">uses:</span> <span class="string">actions/setup-node@v3</span></span><br><span class="line">          <span class="attr">with:</span></span><br><span class="line">            <span class="attr">node-version:</span> <span class="string">&#x27;20&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Install</span> <span class="string">dependencies</span></span><br><span class="line">          <span class="attr">run:</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span></span><br><span class="line">          <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 部署</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span></span><br><span class="line">          <span class="attr">uses:</span> <span class="string">JamesIves/github-pages-deploy-action@v4.5.0</span></span><br><span class="line">          <span class="attr">with:</span></span><br><span class="line">            <span class="attr">branch:</span> <span class="string">html</span></span><br><span class="line">            <span class="attr">folder:</span> <span class="string">public</span></span><br></pre></td></tr></table></figure><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240303020755.png" alt="20240303020755"></p><h2 id="5-修改GitHub仓库设置"><a href="#5-修改GitHub仓库设置" class="headerlink" title="5.修改GitHub仓库设置"></a>5.修改GitHub仓库设置</h2><p>先在博客仓库Settings的Pages中将Branch设置为html<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240303020837.png" alt="20240303020837"></p><p>然后将Actions下的General中的Workflow permissons设置为Read and write permissions<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240303020935.png" alt="20240303020935"></p><h2 id="6-推送更改"><a href="#6-推送更改" class="headerlink" title="6. 推送更改"></a>6. 推送更改</h2><p>然后将更改推送到远程仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;update&quot;</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><h2 id="7-等待部署完成"><a href="#7-等待部署完成" class="headerlink" title="7. 等待部署完成"></a>7. 等待部署完成</h2><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240303021139.png" alt="20240303021139"><br>没有报错的话就完成了<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/20240303021234.png" alt="20240303021234"><br>https最好也打开。</p><h2 id="一些其他问题"><a href="#一些其他问题" class="headerlink" title="一些其他问题"></a>一些其他问题</h2><p>我在配置过程中碰到了一些其他问题，腾讯云配置DNS我开始弄的有点问题，就重新配的DNS Pod 。</p><p>除此之外，在配置的过程中node_modules是不需要的，我开始这里加了，后面报错，去掉就好了。</p><p>另外一个问题是我把源文件复制过去之后，最后部署完成之后网页是空白，最后发现是theme文件下的butterfly主题文件夹是空的，我把这个文件夹删了，然后重新clone了一遍butterfly主题，再部署就好了。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.xdu-inspur.club/blog/site/%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/get_a_blog.html#_7">https://www.xdu-inspur.club/blog/site/%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/get_a_blog.html#_7</a></p>]]></content>
    
    
    <summary type="html">本文介绍如何使用Github Actions自动部署博客，并且使用仓库分支存储图片等资源。</summary>
    
    
    
    <category term="博客配置" scheme="http://example.com/categories/%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="博客配置" scheme="http://example.com/tags/%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>服务器常见技术问题与技巧</title>
    <link href="http://example.com/2023/09/03/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B8%B8%E8%A7%81%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98%E4%B8%8E%E6%8A%80%E5%B7%A7/"/>
    <id>http://example.com/2023/09/03/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B8%B8%E8%A7%81%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98%E4%B8%8E%E6%8A%80%E5%B7%A7/</id>
    <published>2023-09-03T12:29:37.000Z</published>
    <updated>2024-10-10T14:15:36.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目前碰到的一些技术问题与知识"><a href="#目前碰到的一些技术问题与知识" class="headerlink" title="目前碰到的一些技术问题与知识"></a>目前碰到的一些技术问题与知识</h1><h2 id="ubuntu安装驱动时候出现的问题"><a href="#ubuntu安装驱动时候出现的问题" class="headerlink" title="ubuntu安装驱动时候出现的问题"></a>ubuntu安装驱动时候出现的问题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo -e “blacklist nouveau\noptions nouveau modeset=0” &gt; /etc/modprobe.d/disable-nouveau.conf</span><br><span class="line">update-initramfs -u</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改启动参数:在GRUB菜单中选择Ubuntu启动选项，按下”e”键以编辑启动参数。尝试在命令行中添加”nomodeset”参数，然后按下F10键启动。</p><h2 id="ubuntu安装cuda驱动时候出现报错"><a href="#ubuntu安装cuda驱动时候出现报错" class="headerlink" title="ubuntu安装cuda驱动时候出现报错"></a>ubuntu安装cuda驱动时候出现报错</h2><ul><li>安装显卡驱动需要关闭图形界面，在命令行完成安装（提前下好安装包）<br>关闭图形界面</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl isolate multi-user.target</span><br></pre></td></tr></table></figure><p>开机可能出现黑屏，按<code>ctrl + shift + F2</code>即可进入命令行窗口，登录即可，随后安装驱动，安装完成之后，执行下边命令开机默认进入图形用户界面。  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl isolate graphical.target</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/xiaoyuxin1/article/details/124526430">给Ubuntu安装驱动（nvidia）保姆级教程（方法一）_X.等雨停的博客-CSDN博客</a></p><h2 id="ubuntu拨号上网以及各种方式上网设置"><a href="#ubuntu拨号上网以及各种方式上网设置" class="headerlink" title="ubuntu拨号上网以及各种方式上网设置"></a>ubuntu拨号上网以及各种方式上网设置</h2><p>命令行输入即可进入图形管理界面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nm-connection-editor</span><br></pre></td></tr></table></figure><h2 id="ubuntu22-04-向日葵远程无法连接成功"><a href="#ubuntu22-04-向日葵远程无法连接成功" class="headerlink" title="ubuntu22.04 向日葵远程无法连接成功"></a>ubuntu22.04 向日葵远程无法连接成功</h2><p>需要切换桌面模式</p><h2 id="ubuntu机械盘挂载"><a href="#ubuntu机械盘挂载" class="headerlink" title="ubuntu机械盘挂载"></a>ubuntu机械盘挂载</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">创建挂载路径</span></span><br><span class="line">sudo mkdir /data</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">格式化硬盘</span></span><br><span class="line">sudo mkfs -t ext4 /dev/sda</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">挂载硬盘</span></span><br><span class="line">sudo mount -t ext4 /dev/sda /data</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">开机自动挂载</span></span><br><span class="line">sudo vim /etc/fstab</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在最后一行加入</span></span><br><span class="line">/dev/sda /data ext4  defaults 0 0</span><br></pre></td></tr></table></figure><h2 id="使用yaml创建虚拟环境"><a href="#使用yaml创建虚拟环境" class="headerlink" title="使用yaml创建虚拟环境"></a>使用yaml创建虚拟环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">transformer</span></span><br><span class="line"><span class="attr">channels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pytorch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">conda-forge</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">defaults</span></span><br><span class="line"><span class="attr">dependencies:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_libgcc_mutex=0.1=main</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_openmp_mutex=4.5=1_gnu</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">astor=0.8.1=py39h06a4308_0</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">autograd=1.3=pyhd3eb1b0_1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">pip:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">argparse==1.4.0</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">einops==0.3.2</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">nystrom-attention==0.0.11</span></span><br><span class="line"><span class="attr">prefix:</span> <span class="string">/home/ps/anaconda3/envs/transformer</span></span><br></pre></td></tr></table></figure><p>最后的<code>prefix</code>指定环境位置</p><h2 id="使用-gitkeep文件保存空文件夹"><a href="#使用-gitkeep文件保存空文件夹" class="headerlink" title="使用.gitkeep文件保存空文件夹"></a>使用<code>.gitkeep</code>文件保存空文件夹</h2><h2 id="创建一个新的普通用户"><a href="#创建一个新的普通用户" class="headerlink" title="创建一个新的普通用户"></a>创建一个新的普通用户</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -m ai -s /bin/bash</span><br><span class="line">sudo passwd ai</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sudo adduser ai sudo</span></span><br><span class="line">su ai</span><br></pre></td></tr></table></figure><ul><li>创建了可以登录的ai用户并使用&#x2F;bin&#x2F;bash作为shell。</li><li>设置密码。</li><li>为ai用户增加管理员权限。</li><li>切换登录用户为ai。</li></ul><h2 id="为普通用户添加sudo权限"><a href="#为普通用户添加sudo权限" class="headerlink" title="为普通用户添加sudo权限"></a>为普通用户添加sudo权限</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">为用户username添加sudo权限</span></span><br><span class="line">sudo usermod -a -G sudo username</span><br><span class="line"><span class="meta prompt_"> </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">去除用户username的sudo权限</span></span><br><span class="line">sudo usermod -G usergroup username</span><br></pre></td></tr></table></figure><p>给用户授权</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groups ai</span><br><span class="line">usermod -aG sudo meow</span><br><span class="line">visudo</span><br></pre></td></tr></table></figure><p>删除用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo deluser --remove-home ai</span><br></pre></td></tr></table></figure><p>删除用户目录</p><p>查看所有用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep bash /etc/passwd</span><br></pre></td></tr></table></figure><h2 id="linux新建用户也使用原来conda环境"><a href="#linux新建用户也使用原来conda环境" class="headerlink" title="linux新建用户也使用原来conda环境"></a>linux新建用户也使用原来conda环境</h2><p>a用户下安装anaconda，默认地址不变，b用户直接不可用，在b用户登录的终端编辑b用户的<code>.bashrc</code>文件，在文档最后一行加入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/home/&lt;Username&gt;/anaconda3/bin</span><br></pre></td></tr></table></figure><p>保存退出，并<code>source .bashrc</code>，然后<code>conda init</code>就可以了</p><p>高级配置参考<a href="https://zhuanlan.zhihu.com/p/570747928?utm_id=0">Anaconda 多用户共享安装（Ubuntu） - 知乎 (zhihu.com)</a></p><h2 id="给用户授予docker使用权限"><a href="#给用户授予docker使用权限" class="headerlink" title="给用户授予docker使用权限"></a>给用户授予docker使用权限</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo gpasswd -a $USER docker </span><br><span class="line">newgrp docker</span><br></pre></td></tr></table></figure><p><code>$USER</code>可以更换为其他，不换就是默认的。第二步有可能需要输密码，然后输入会发现错误，其实是没有设置密码，需要先设置密码，然后这样更新组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod a+rw /var/run/docker.sock</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure><h2 id="neofetch"><a href="#neofetch" class="headerlink" title="neofetch"></a>neofetch</h2><p>方便的查看系统的工具</p><h2 id="卸载图形界面"><a href="#卸载图形界面" class="headerlink" title="卸载图形界面"></a>卸载图形界面</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove gnome-shell</span><br><span class="line">sudo apt-get remove gnome </span><br><span class="line">sudo apt-get autoremove</span><br><span class="line">sudo apt-get purge gnome</span><br><span class="line">sudo apt-get autoclean</span><br><span class="line">sudo apt-get clean</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><h2 id="ssh免密登录"><a href="#ssh免密登录" class="headerlink" title="ssh免密登录"></a>ssh免密登录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@服务器IP</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lscpu</span><br></pre></td></tr></table></figure><h2 id="cudnn-source-problem"><a href="#cudnn-source-problem" class="headerlink" title="cudnn source problem"></a>cudnn source problem</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W: GPG error: file:/var/cudnn-local-repo-ubuntu2004-8.6.0.163  InRelease: The following signatures couldn&#x27;t be verified because the public key is not available: NO_PUBKEY 323547C4B0FE0A41</span><br><span class="line">E: The repository &#x27;file:/var/cudnn-local-repo-ubuntu2004-8.6.0.163  InRelease&#x27; is not signed.</span><br><span class="line">N: Updating from such a repository can&#x27;t be done securely, and is therefore disabled by default.</span><br><span class="line">N: See apt-secure(8) manpage for repository creation and user configuration details.</span><br></pre></td></tr></table></figure><p>then input <code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 323547C4B0FE0A41</code> to solve the problem.</p><p>But it doesn’t work. You can follow the steps below to solve the problem.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /var/cudnn-local-repo-ubuntu2004-8.4.1.50/*.gpg /usr/share/keyrings/</span><br></pre></td></tr></table></figure><p>复制到&#x2F;usr&#x2F;share&#x2F;keyrings即可。</p><h2 id="两个盘软raid命令"><a href="#两个盘软raid命令" class="headerlink" title="两个盘软raid命令"></a>两个盘软raid命令</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mdadm</span><br><span class="line">#格式化两块硬盘</span><br><span class="line">#sudo mkfs.ext4 -F /dev/sdb</span><br><span class="line">#sudo mkfs.ext4 -F /dev/sdc</span><br><span class="line">#mdadm管理 raid0</span><br><span class="line">sudo mdadm --create --verbose /dev/md0 --level=0 --raid-devices=2 /dev/sd&#123;b,c&#125;</span><br><span class="line">#格式化并挂载</span><br><span class="line">sudo mkfs.ext4 -F /dev/md0 sudo mkdir -p /home/md0 sudo mount /dev/md0 /home/md0</span><br></pre></td></tr></table></figure><h2 id="每隔时间杀程序"><a href="#每隔时间杀程序" class="headerlink" title="每隔时间杀程序"></a>每隔时间杀程序</h2><p>kill_python.py</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ true ];do</span><br><span class="line">sleep 5</span><br><span class="line">ps -ef |grep -w  python |grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>然后再讲其添加到crontab中，执行crontab -e，添加如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/30 * * * * /usr/bin/sh /aa/bb/cleanFfmpegProcess.sh</span><br></pre></td></tr></table></figure><p>:wq即可。</p><h2 id="关闭服务器自动休眠"><a href="#关闭服务器自动休眠" class="headerlink" title="关闭服务器自动休眠"></a>关闭服务器自动休眠</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target</span><br></pre></td></tr></table></figure><p>查看系统休眠状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status sleep.target</span><br></pre></td></tr></table></figure><h2 id="设置github的ssh连上github"><a href="#设置github的ssh连上github" class="headerlink" title="设置github的ssh连上github"></a>设置github的ssh连上github</h2><p>生成ssh key</p><p>读取公钥</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>复制粘贴到github。github配置ssh key的地方在<br><a href="https://github.com/settings/keys">https://github.com/settings/keys</a></p><p>测试ssh key是否配置成功，在linux开发机上输入<br>$ ssh -T <a href="mailto:git@github.com">git@github.com</a></p><p>如果出现Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access 。这就表示已成功连上github</p><p>3、配置git的用户名和邮箱</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name &quot;your name&quot;</span><br><span class="line">$ git config --global user.email &quot;your_email@youremail.com&quot;</span><br></pre></td></tr></table></figure><p>以后可以通过git config –global -l来查看全局设置，git config –global -e来编辑</p><h2 id="更新包导致cuda和驱动版本不对应，NVIDIA-SMI-has-failed-because-it-couldn‘t-communicate-with-the-NVIDIA-driver"><a href="#更新包导致cuda和驱动版本不对应，NVIDIA-SMI-has-failed-because-it-couldn‘t-communicate-with-the-NVIDIA-driver" class="headerlink" title="更新包导致cuda和驱动版本不对应，NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver"></a>更新包导致cuda和驱动版本不对应，NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver</h2><p>终端nvidia-smi出现这样，<strong>是内核版本更新的问题，导致新版本内核和原来显卡驱动不匹配</strong></p><p>查看已安装内核</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg --get-selections |grep linux-image</span><br></pre></td></tr></table></figure><p>查看正在使用的内核</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -a</span><br></pre></td></tr></table></figure><p>利用命令 <code>ll /usr/src/</code> 可查看下面有一个nvidia-470.82.00文件夹，版本号因电脑而异。</p><p>只需执行两条命令就好：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dkms</span><br><span class="line">sudo dkms install -m nvidia -v 470.82.00</span><br></pre></td></tr></table></figure><p>（470.82.00表示的是驱动版本号）</p><h3 id="禁止内核自动更新"><a href="#禁止内核自动更新" class="headerlink" title="禁止内核自动更新"></a>禁止内核自动更新</h3><p>1）命令行关闭系统自动更新，使用命令打开文件并编辑</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/apt/apt.conf.d/<span class="number">10</span>periodic</span><br></pre></td></tr></table></figure><p>将双引号中的“1”全部置“0”即可，修改后保存。</p><p>ubuntu默认启动了自动更新内核，为了避免出现重启系统后遇到错误进入不到系统中去，我们可以进一步关闭内核更新，使用当前内核。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-mark hold linux-image-generic linux-headers-generic </span><br><span class="line">linux-image-generic set on hold.</span><br><span class="line">linux-headers-generic set on hold.</span><br></pre></td></tr></table></figure><p>如果要重启启动内核更新：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-mark unhold linux-image-generic linux-headers-generic</span><br></pre></td></tr></table></figure><h2 id="文件格式问题"><a href="#文件格式问题" class="headerlink" title="文件格式问题"></a>文件格式问题</h2><p>qsub:script is written in DOS&#x2F;Windows text format</p><p>dos格式文件传输到unix系统时，会在每行的结尾多一个^M（&#x2F;r），当然也有可能看不到。但是在vim的时候，会在下面显示此文件的格式，比如 “dos.txt” [dos] 120L, 2532C 字样,表示是一个[dos]格式文件，如果是MAC系统的，会显示[MAC]。因为文件格式的原因有时会导致我们的unix程序，或者shell程序出现错误，那么需要把这些dos文件格式转换成unix格式，方法是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim dos.txt</span><br><span class="line">:set fileformat=unix</span><br><span class="line">:w</span><br></pre></td></tr></table></figure><h2 id="centos7安装驱动"><a href="#centos7安装驱动" class="headerlink" title="centos7安装驱动"></a>centos7安装驱动</h2><p><a href="https://www.cnblogs.com/2012blog/p/9431432.html">https://www.cnblogs.com/2012blog/p/9431432.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="彻底卸载wsl"><a href="#彻底卸载wsl" class="headerlink" title="彻底卸载wsl"></a>彻底卸载wsl</h2><p>在powerShell上<br>先看还有哪些子系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --list --all</span><br></pre></td></tr></table></figure><p>注销子系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --unregister Ubuntu</span><br></pre></td></tr></table></figure><h2 id="只读文件系统"><a href="#只读文件系统" class="headerlink" title="只读文件系统"></a>只读文件系统</h2><p>在linux硬盘挂载时候，部分情况磁盘变成ro只读而不是rw<br>一般df查看挂载设备</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">umount -i 挂载路径</span><br></pre></td></tr></table></figure><p>rm -rf 目录&#x2F;文件</p><p>在部分情况下，先使用windows系统，后使用linux，可能有系统挂载问题，即使chmod 777 也没用，出现错误：</p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/image2.png" alt="image"><br>需要重新挂载</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">umount /dev/sda2</span><br></pre></td></tr></table></figure><p>报错target busy<br>杀死使用该目录的进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fuser  -<span class="built_in">mv</span> -k /dev/sda2</span><br></pre></td></tr></table></figure><p> 再次卸载，卸载成功后重新挂载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/sda2 /data</span><br></pre></td></tr></table></figure><p>报错解决：<br>报错1：“The disk contains an unclean file system (0, 0). Metadata kept in Windows cache, refused to mount. Falling back to read-only mount because the NTFS partition is in an unsafe state. Please resume and shutdown Windows fully (no hibernation or fast restarting.)”。说明是NTFS分区格式错误<br>解决方法：ntfsfix修复，需要安装工具：<br>sudo apt-get install ntfs-3g<br>安装完成后进行ntfsfix修复：<br>sudo ntfsfix &#x2F;dev&#x2F;sda2<br>提示修复成功。                        </p><p><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/image3.png" alt="image3"></p><p>报错2：“没有那个文件或目录”。说明&#x2F;data目录不存在，需要创建。<br>解决方法：media根目录下创建新目录：<br>mkdir &#x2F;media&#x2F;jngk&#x2F;data<br>然后重新挂载，即可挂载成功。<br>mount &#x2F;dev&#x2F;sda2 &#x2F;media&#x2F;jngk&#x2F;data<br>现在该目录就不是只读文件系统了，在该目录下右键，新建文件夹选项也不再是灰色不可选状态了。</p><h1 id="无法将“XXX”项识别为-cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次"><a href="#无法将“XXX”项识别为-cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次" class="headerlink" title="无法将“XXX”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次"></a>无法将“XXX”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次</h1><p>进入PowerShell 模式<br>Get-ExecutionPolicy -List 查看当前所有作用域<br>1</p><p>上图显示就最后一个作用域有权限，其他作用域都没有权限，那么我们就需要去给它设置权限</p><p>设置权限<br>Set-ExecutionPolicy RemoteSigned -Scope &lt; scopeName &gt;,设置当前用户作用域具备权限，具体设置格</p><p>Set-ExecutionPolicy RemoteSigned -Scope CurrentUser<br>1</p><p>按照上面的格式，执行需要加权限的作用域，然后再去尝试之前的方法，发现就不会报错提示了。</p><h1 id="挖矿病毒处理"><a href="#挖矿病毒处理" class="headerlink" title="挖矿病毒处理"></a>挖矿病毒处理</h1><p>找到病毒文件：</p><p>方法一：由于入侵者对程序的名称做了伪装，无法直接通过其进行查找。因此使用PID入手，查找其在&#x2F;proc里的文件，进而发现了关键路径</p><p>将所有病毒文件展示出来，可以发现关键词miner</p><p>对里面的文件作进一步的查看，可以看到ETH（以太坊）、POOL、WALLET等关键词，实锤中了挖矿病毒</p><p>此外，查看run文件，可以发现这个病毒文件确实做了伪装</p><p>方法二：考虑到入侵者可能会用某些方式将病毒程序作一定的隐藏，这里通过查看用户的计划任务来定位病毒文件，因为病毒文件可能被kill掉而入侵者不会每次都自己手动启动的，肯定会设置自动启动。查看计划任务的命令为crontab -l<br>为了查看病毒文件在服务器上是否有多份，可以使用如下命令一次性查看所有用户的计划任务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> u <span class="keyword">in</span> $(<span class="built_in">cat</span> /etc/passwd | <span class="built_in">cut</span> -d<span class="string">&quot;:&quot;</span> -f1)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$u</span>&gt;&gt;temp.txt</span><br><span class="line">    crontab -l -u <span class="variable">$u</span> &gt;&gt; temp.txt</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cat</span> temp.txt</span><br><span class="line"><span class="built_in">rm</span> temp.txt</span><br></pre></td></tr></table></figure><p>方法三：尝试定位病毒文件中的特有关键词，比如“miner”<br>updatedb<br>locate miner</p><h1 id="由于locate命令不能查找-x2F-dev-x2F-shm之类的路径，以防万一可以使用find命令，不过会很慢"><a href="#由于locate命令不能查找-x2F-dev-x2F-shm之类的路径，以防万一可以使用find命令，不过会很慢" class="headerlink" title="由于locate命令不能查找&#x2F;dev&#x2F;shm之类的路径，以防万一可以使用find命令，不过会很慢"></a>由于locate命令不能查找&#x2F;dev&#x2F;shm之类的路径，以防万一可以使用find命令，不过会很慢</h1><h1 id="find-x2F-name-miner"><a href="#find-x2F-name-miner" class="headerlink" title="find &#x2F; -name miner"></a>find &#x2F; -name miner</h1><p>应对方法：</p><p>删除整个病毒文件夹（python），kill相关的PID，并删除相关定时任务（使用命令crontab -e，或者如果只有一个定时任务的话可以用命令crontab -r）</p><p>更改所有用户的密码，并设置一定的密码复杂度（可以使用cracklib）</p><p>移除所有除了管理员以外用户的sudo权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">admin=<span class="string">&quot;root,sudo,%sudo&quot;</span> <span class="comment"># 填入管理员账号（前三个不能删）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">cat</span> /etc/sudoers|grep  <span class="string">&quot;ALL=(ALL:ALL) ALL&quot;</span>|<span class="built_in">cut</span> -f 1|<span class="built_in">cut</span> -f 1 -d <span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="subst">$(echo $admin|grep $i)</span>&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*** deluser <span class="variable">$i</span> sudo&quot;</span></span><br><span class="line">        deluser <span class="variable">$i</span> sudo</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(getent group sudo|<span class="built_in">cut</span> -f 4 -d :|<span class="built_in">tr</span> -s <span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="subst">$(echo $admin|grep $i)</span>&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*** deluser <span class="variable">$i</span> sudo&quot;</span></span><br><span class="line">        deluser <span class="variable">$i</span> sudo</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>删除现有的所有密钥&amp;授权</p><p>updatedb</p><h1 id="删除公钥-密钥"><a href="#删除公钥-密钥" class="headerlink" title="删除公钥+密钥"></a>删除公钥+密钥</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> pub <span class="keyword">in</span> $(locate .pub|grep .pub$)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    u=$(ll <span class="variable">$pub</span>|awk <span class="string">&#x27;&#123;printf $3&#125;&#x27;</span>)</span><br><span class="line">    <span class="comment"># 根据UID判断所属用户是否为普通用户</span></span><br><span class="line">    <span class="keyword">if</span> [ 999 -lt $(<span class="built_in">id</span> -u <span class="variable">$u</span>) ]</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            pri=$(<span class="built_in">echo</span> <span class="variable">$&#123;pub%????&#125;</span>)</span><br><span class="line">            <span class="built_in">rm</span> <span class="variable">$pub</span></span><br><span class="line">            <span class="built_in">rm</span> <span class="variable">$pri</span></span><br><span class="line">            <span class="built_in">echo</span> del  <span class="variable">$u</span> <span class="variable">$pub</span> <span class="variable">$pri</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> save <span class="variable">$u</span> <span class="variable">$pub</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment"># 删除knowN_hosts</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(locate known_hosts|grep known_hosts$)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">rm</span> <span class="variable">$i</span></span><br><span class="line">    <span class="built_in">echo</span> del <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment"># 删除authorized_keys</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(locate authorized_keys|grep authorized_keys$)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">rm</span> <span class="variable">$i</span></span><br><span class="line">    <span class="built_in">echo</span> del <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h1 id="查看剩余"><a href="#查看剩余" class="headerlink" title="查看剩余"></a>查看剩余</h1><p>updatedb<br>locate .pub|grep .pub$<br>locate known_hosts<br>locate authorized_keys</p><p>可能会遇到某个文件用root权限也删除不了</p><p>如果使用命令lsattr发现该文件的隐藏属性中存在除了e以外的属性，则用命令chattr来移除这些属性</p><p>设置远程连接只能使用密钥，不能使用密码</p><p>禁止使用root账号进行远程连接</p><p>使用终端安全杀毒软件、内网安全监控产品、漏洞扫描设备等专业工具</p><h2 id="查杀病毒"><a href="#查杀病毒" class="headerlink" title="查杀病毒"></a>查杀病毒</h2><p>1、crontab -e 发现一条自启动任务，且为“python”程序，但不确定是否为挖矿程序，暂时保留<br>2、根据挖矿程序PID，进入&#x2F;proc&#x2F;PID目录下检查<br>进入&#x2F;tmp&#x2F;…&#x2F;Python目录<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/image4.png" alt="image4"><br>通过sudo vim config.ini检查config.ini文件，发现正是挖矿配置文件，从而自启动任务为挖矿任务<br><img src="https://gcore.jsdelivr.net/gh/CoderJackZhu/bloggallery/img/image5.png" alt="image5"><br>4、kill挖矿进程，删除挖矿文件及自启动任务，同时关闭挖矿病毒使用的端口<br>5、使用杀毒软件全盘查杀</p><h2 id="在cuda版本过高的情况nvidia官方bug，会出现内存泄露no-such-progress"><a href="#在cuda版本过高的情况nvidia官方bug，会出现内存泄露no-such-progress" class="headerlink" title="在cuda版本过高的情况nvidia官方bug，会出现内存泄露no such progress"></a>在cuda版本过高的情况nvidia官方bug，会出现内存泄露no such progress</h2><p>使用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install --upgrade nvidia-ml-py</span><br></pre></td></tr></table></figure><h2 id="fsl-安装"><a href="#fsl-安装" class="headerlink" title="fsl 安装"></a>fsl 安装</h2><p>使用本地安装包<br>解压缩到要安装的文件夹。推荐和我一样解压缩到&#x2F;usr&#x2F;local目录下<br>有可能会出现权限不足的问题无法解压缩，可以在usr目录中打开终端，输入<br>sudo chmod -R 777 local<br>输入密码后，打开权限<br>配置环境变量.bashrc</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export FSLDIR=/usr/local/fsl</span><br><span class="line">export PATH=$PATH:$FSLDIR/bin</span><br><span class="line">source $FSLDIR/etc/fslconf/fsl.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="卸载cuda"><a href="#卸载cuda" class="headerlink" title="卸载cuda"></a>卸载cuda</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/cuda-xx.x/bin/</span><br><span class="line">sudo ./cuda-uninstaller</span><br><span class="line">sudo rm -rf /usr/local/cuda-xx.x</span><br></pre></td></tr></table></figure><h1 id="微星tpm"><a href="#微星tpm" class="headerlink" title="微星tpm"></a>微星tpm</h1><p>微星gl63有TPM的，下面教你如何操作：<br>首先进bios<br>然后按右边的CTRL+SHIFT+左边的ALT+F2开启超级模式。<br>之后在高级选项中找到PCH-FW Configuration<br>找到PTT Configuratio<br>把dTPM改为PTT（如果已经是PTT可以跳过此步）。<br>之后在高级选项中找到Trusted Computing<br>找到Security Device Support，将Disable改为Enable<br>按F10保存。</p>]]></content>
    
    
    <summary type="html">本文记录了一些常见的技术问题与技巧</summary>
    
    
    
    <category term="技术问题" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98/"/>
    
    
    <category term="技术问题" scheme="http://example.com/tags/%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
</feed>
